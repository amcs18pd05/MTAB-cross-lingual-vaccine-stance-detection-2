{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67dbdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in /Users/bharathia/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages (0.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1906904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bharathia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from pprint import pprint\n",
    "import preprocessor as p\n",
    "from functions import *\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torchmetrics import F1Score\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertPreTrainedModel, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word.context_word_embs as nawcwe\n",
    "import nlpaug.augmenter.word.word_embs as nawwe\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "import nlpaug.augmenter.word.spelling as naws\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "# seed_val = 42\n",
    "\n",
    "# random.seed(seed_val)\n",
    "# np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29512c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e900ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d20aa3",
   "metadata": {},
   "source": [
    "### UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470323c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "    # Remvoing Retweet symbols\n",
    "    z = lambda x: re.compile('RT @').sub('@', x, count=1).strip()\n",
    "    text = z(text)\n",
    "    text = p.clean(text)\n",
    "    #Don't remove the hashtag entirely, just remove the # symbol but keep the keyword\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.replace(':', \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "MAX_LEN = 128 # max sequences length\n",
    "batch_size = 32\n",
    "\n",
    "labels_encoding = {\n",
    "    \"negative\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"positive\": 2\n",
    "}\n",
    "\n",
    "def preprocessing(df):\n",
    "    sentences = df.processed_tweet.values\n",
    "    labels = np.array([labels_encoding[l] for l in df.stance.values])\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
    "    \n",
    "    encoded_sentences = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,\n",
    "                            add_special_tokens = True,\n",
    "                            truncation=True,\n",
    "                            max_length = MAX_LEN\n",
    "                    )\n",
    "        \n",
    "        encoded_sentences.append(encoded_sent)\n",
    "    encoded_sentences = pad_sequences(encoded_sentences, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                            value=0, truncating=\"post\", padding=\"post\")\n",
    "    return encoded_sentences, labels\n",
    "\n",
    "def attention_masks(encoded_sentences):\n",
    "    # attention masks, 0 for padding, 1 for actual token\n",
    "    attention_masks = []\n",
    "    for sent in encoded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks\n",
    "\n",
    "def train_test_split(df, frac=0.1):\n",
    "    \n",
    "    # get random sample \n",
    "    test = df.sample(frac=frac, axis=0, random_state = 42)\n",
    "\n",
    "    # get everything but the test sample\n",
    "    train = df.drop(index=test.index)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe009a60",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/71174306/expected-in-usr-lib-libc-1-dylib-installing-tensorflow-on-m1-macbook-pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605da93",
   "metadata": {},
   "source": [
    "### Combining the English datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666dec5a",
   "metadata": {},
   "source": [
    "### 1. VADet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37b4a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372900215620001795</td>\n",
       "      <td>1/1Today, I visited CPG Hospital to take the C...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372148534506573826</td>\n",
       "      <td>@notjustamummy2 @LozzaFox Well there is a viru...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1373396702116376579</td>\n",
       "      <td>@paulreiddublin @HSELive Mr Reid , may I ask i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1373276419527208966</td>\n",
       "      <td>@is_salsu @NphcdaNG 1. 2nd March 2021\\n\\n2. Th...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372964450437840906</td>\n",
       "      <td>@drsanjaygupta @ChrisCuomo @DaveedDiggs @Lin_M...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1372900215620001795  1/1Today, I visited CPG Hospital to take the C...   \n",
       "1  1372148534506573826  @notjustamummy2 @LozzaFox Well there is a viru...   \n",
       "2  1373396702116376579  @paulreiddublin @HSELive Mr Reid , may I ask i...   \n",
       "3  1373276419527208966  @is_salsu @NphcdaNG 1. 2nd March 2021\\n\\n2. Th...   \n",
       "4  1372964450437840906  @drsanjaygupta @ChrisCuomo @DaveedDiggs @Lin_M...   \n",
       "\n",
       "     stance  \n",
       "0  positive  \n",
       "1  negative  \n",
       "2  negative  \n",
       "3  positive  \n",
       "4  positive  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vad_hydrated_tweets = [json.loads(line) for line in open('hydrated_tweets/vad_ids.json', 'r')]\n",
    "vad_hydrated_tweets = pd.read_csv('hydrator-tweets/vad_ids.csv')\n",
    "vad_class_labels = pd.read_csv('datasets/VADet/annotated_twids.csv')\n",
    "vad_ids = []\n",
    "vad_tweets = []\n",
    "\n",
    "for index, element in vad_hydrated_tweets.iterrows():\n",
    "    vad_ids.append(element['id'])\n",
    "    vad_tweets.append(element['text'])\n",
    "    \n",
    "vad_data = pd.DataFrame(list(zip(vad_ids, vad_tweets)), columns = ['id', 'tweet'])\n",
    "vad_data = vad_data.merge(vad_class_labels[['ID', 'stance']], how = 'inner', left_on = 'id', right_on = 'ID')[['id', 'tweet', 'stance']]\n",
    "vad_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec06f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2046"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vad_hydrated_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c29318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    1367\n",
       "negative     517\n",
       "neutral      162\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad_data['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "413dc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data.to_csv('experimental_datasets/vad_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f35c580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2046 entries, 0 to 2045\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      2046 non-null   int64 \n",
      " 1   tweet   2046 non-null   object\n",
      " 2   stance  2046 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 63.9+ KB\n"
     ]
    }
   ],
   "source": [
    "vad_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8c8e5",
   "metadata": {},
   "source": [
    "### 2. Vaccine Stance dataset (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0545d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1301859457274535936</td>\n",
       "      <td>หรือช้อปออนไลน์ได้ที่~\\n\\nShopee: https://t.co...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1303711210622255104</td>\n",
       "      <td>@_rafaellugh HAHAHAHSHA PWDE MAN MAG ZUMBA SA ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1303057753171202048</td>\n",
       "      <td>@djlavoie @chrislhayes big pharma says they al...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1301867614302547968</td>\n",
       "      <td>\"Whether intentional or not, the response to #...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1306393902899920896</td>\n",
       "      <td>@GovAbbott don't you dare mandate the coronavi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1301859457274535936  หรือช้อปออนไลน์ได้ที่~\\n\\nShopee: https://t.co...   \n",
       "1  1303711210622255104  @_rafaellugh HAHAHAHSHA PWDE MAN MAG ZUMBA SA ...   \n",
       "2  1303057753171202048  @djlavoie @chrislhayes big pharma says they al...   \n",
       "3  1301867614302547968  \"Whether intentional or not, the response to #...   \n",
       "4  1306393902899920896  @GovAbbott don't you dare mandate the coronavi...   \n",
       "\n",
       "     stance  \n",
       "0  negative  \n",
       "1   neutral  \n",
       "2  negative  \n",
       "3  negative  \n",
       "4  negative  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vac_stance_hydrated_tweets = [json.loads(line) for line in open('hydrated_tweets/vac_stance_ids.json', 'r')]\n",
    "vac_stance_hydrated_tweets = pd.read_csv('hydrator-tweets/vac_stance_ids.csv')\n",
    "vac_stance_class_labels = pd.read_csv('datasets/VaccineStance/Data.csv')\n",
    "vac_stance_ids = []\n",
    "vac_stance_tweets = []\n",
    "label_mapping = {1:'positive', 2:'negative', 3:'neutral'}\n",
    "\n",
    "for index, element in vac_stance_hydrated_tweets.iterrows():\n",
    "    vac_stance_ids.append(element['id'])\n",
    "    vac_stance_tweets.append(element['text'])\n",
    "    \n",
    "vac_stance_data = pd.DataFrame(list(zip(vac_stance_ids, vac_stance_tweets)), columns = ['id', 'tweet'])\n",
    "vac_stance_data = vac_stance_data.merge(vac_stance_class_labels[['ID', 'Code']], how = 'inner', left_on = 'id', right_on = 'ID')[['id', 'tweet', 'Code']]\n",
    "vac_stance_data['stance'] = vac_stance_data['Code'].replace(label_mapping)\n",
    "vac_stance_data.drop(columns=['Code'], inplace = True)\n",
    "vac_stance_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4fa6525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vac_stance_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a16ec48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    30\n",
       "neutral     13\n",
       "negative    12\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vac_stance_data['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aecfd3",
   "metadata": {},
   "source": [
    "### 3. Opinions regarding COVID-19 vaccine in the first month (Nov 9th 2020 - Dec 8th 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03572e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1325774333504073728</td>\n",
       "      <td>Great job by the #Pfizer #vaccine scientists w...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1325997637640876032</td>\n",
       "      <td>Bother is Profs.of different sections who tell...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1325892752467488770</td>\n",
       "      <td>They say the vaccine will be tested on animals...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325895792054427648</td>\n",
       "      <td>I don’t care who tf you voted for. If this goe...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1329215336344809472</td>\n",
       "      <td>This is the vaccine that's so unstable that it...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              tweet  \\\n",
       "0  1325774333504073728  Great job by the #Pfizer #vaccine scientists w...   \n",
       "1  1325997637640876032  Bother is Profs.of different sections who tell...   \n",
       "2  1325892752467488770  They say the vaccine will be tested on animals...   \n",
       "3  1325895792054427648  I don’t care who tf you voted for. If this goe...   \n",
       "4  1329215336344809472  This is the vaccine that's so unstable that it...   \n",
       "\n",
       "     stance  \n",
       "0  positive  \n",
       "1  negative  \n",
       "2  negative  \n",
       "3  positive  \n",
       "4  positive  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vac_opinions_hydrated_tweets = [json.loads(line) for line in open('hydrated_tweets/vac_opinions_first_month_ids.json', 'r')]\n",
    "vac_opinions_hydrated_tweets = pd.read_csv('hydrator-tweets/vac_opinions_first_month_ids.csv')\n",
    "vac_opinions_class_labels = pd.read_csv('datasets/FirstMonthStance/vaccination_stance_first_month.csv')\n",
    "vac_opinions_ids = []\n",
    "vac_opinions_tweets = []\n",
    "label_mapping = {2.0:'positive', 0.0:'negative', 1.0:'neutral'}\n",
    "\n",
    "for index, element in vac_opinions_hydrated_tweets.iterrows():\n",
    "    vac_opinions_ids.append(element['id'])\n",
    "    vac_opinions_tweets.append(element['text'])\n",
    "    \n",
    "vac_opinions_data = pd.DataFrame(list(zip(vac_opinions_ids, vac_opinions_tweets)), columns = ['id', 'tweet'])\n",
    "vac_opinions_data = vac_opinions_data.merge(vac_opinions_class_labels[['id', 'category']], how = 'inner', left_on = 'id', right_on = 'id')[['id', 'tweet', 'category']]\n",
    "vac_opinions_data['stance'] = vac_opinions_data['category'].replace(label_mapping)\n",
    "vac_opinions_data.drop(columns=['category'], inplace = True)\n",
    "vac_opinions_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05dc26fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     901\n",
       "positive    879\n",
       "negative    612\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vac_opinions_data['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b261ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2392 entries, 0 to 2391\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      2392 non-null   int64 \n",
      " 1   tweet   2392 non-null   object\n",
      " 2   stance  2392 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 74.8+ KB\n"
     ]
    }
   ],
   "source": [
    "vac_opinions_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce0af40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([vad_data, vac_stance_data, vac_opinions_data], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2cd1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['processed_tweet'] = combined_df['tweet'].apply(process_tweet)\n",
    "combined_df = combined_df[combined_df['processed_tweet'].notna()]\n",
    "combined_df.to_csv('processed_datasets/combined_english_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f456f",
   "metadata": {},
   "source": [
    "### AUGMENTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1598cbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>stance</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1372900215620001795</td>\n",
       "      <td>1/1Today, I visited CPG Hospital to take the C...</td>\n",
       "      <td>positive</td>\n",
       "      <td>/1Today, I visited CPG Hospital to take the Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1372148534506573826</td>\n",
       "      <td>@notjustamummy2 @LozzaFox Well there is a viru...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Well there is a virus but all of this is nothi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1373396702116376579</td>\n",
       "      <td>@paulreiddublin @HSELive Mr Reid , may I ask i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Mr Reid , may I ask if you have received a vac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1373276419527208966</td>\n",
       "      <td>@is_salsu @NphcdaNG 1. 2nd March 2021\\n\\n2. Th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>. nd March . The Head of the EMA, Emer Cooke, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1372964450437840906</td>\n",
       "      <td>@drsanjaygupta @ChrisCuomo @DaveedDiggs @Lin_M...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I got my AstraZeneca shot this week and feel g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   id  \\\n",
       "0           0  1372900215620001795   \n",
       "1           1  1372148534506573826   \n",
       "2           2  1373396702116376579   \n",
       "3           3  1373276419527208966   \n",
       "4           4  1372964450437840906   \n",
       "\n",
       "                                               tweet    stance  \\\n",
       "0  1/1Today, I visited CPG Hospital to take the C...  positive   \n",
       "1  @notjustamummy2 @LozzaFox Well there is a viru...  negative   \n",
       "2  @paulreiddublin @HSELive Mr Reid , may I ask i...  negative   \n",
       "3  @is_salsu @NphcdaNG 1. 2nd March 2021\\n\\n2. Th...  positive   \n",
       "4  @drsanjaygupta @ChrisCuomo @DaveedDiggs @Lin_M...  positive   \n",
       "\n",
       "                                     processed_tweet  \n",
       "0  /1Today, I visited CPG Hospital to take the Co...  \n",
       "1  Well there is a virus but all of this is nothi...  \n",
       "2  Mr Reid , may I ask if you have received a vac...  \n",
       "3  . nd March . The Head of the EMA, Emer Cooke, ...  \n",
       "4  I got my AstraZeneca shot this week and feel g...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.read_csv('processed_datasets/combined_english_tweets.csv')\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b33f591a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4493"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889153d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_data = combined_df[['processed_tweet', 'stance']]\n",
    "train_data, test_data = train_test_split(final_training_data, 0.1)\n",
    "train_data = train_data.dropna().reset_index(drop=True)\n",
    "test_data = test_data.dropna().reset_index(drop=True)\n",
    "train_data = train_data.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ed1c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'negative' 'negative' ... 'neutral' 'neutral' 'neutral']\n",
      "positive    2031\n",
      "negative    2031\n",
      "neutral     2031\n",
      "Name: stance, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'stance'\n",
    "minority_classes = ['negative', 'neutral']\n",
    "\n",
    "majority_df = train_data[train_data[target_variable].isin(minority_classes)==False]\n",
    "minority_df = train_data[train_data[target_variable].isin(minority_classes)]\n",
    "\n",
    "le = LabelEncoder()\n",
    "minority_df[target_variable] = le.fit_transform(minority_df[target_variable])\n",
    "#print(minority_df.head())\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy = {0:2031, 1:2031}, random_state = 42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(minority_df.drop(target_variable, axis=1), minority_df[target_variable])\n",
    "y_resampled = le.inverse_transform(y_resampled)\n",
    "\n",
    "print(y_resampled)\n",
    "\n",
    "resampled_df = pd.DataFrame({'processed_tweet':X_resampled['processed_tweet'], 'stance': y_resampled})\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns=minority_df.drop(target_variable, axis=1).columns)\n",
    "# y_resampled_df = pd.DataFrame(y_resampled, columns=[target_variable])\n",
    "train_data = pd.concat([majority_df, resampled_df])\n",
    "\n",
    "# Print value counts of target variable to check if upsampling worked\n",
    "print(train_data[target_variable].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5687313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.543430\n",
       "neutral     0.240535\n",
       "negative    0.216036\n",
       "Name: stance, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['stance'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fd3607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/czk31wg97tvdq3v6ky8dnt5r0000gn/T/ipykernel_6491/3614916683.py:27: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  augmented_training_data = train_data.append(pd.DataFrame(list(zip(augmented_sentences, augmented_labels)), columns = ['processed_tweet', 'stance']), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# sample = combined_df.iloc[7]['processed_tweet']\n",
    "# sample_original = combined_df.iloc[7]['tweet']\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "        model_path='distilbert-base-uncased')\n",
    "# aug = naw.SynonymAug(aug_src='wordnet')\n",
    "# generated_sentences = aug.augment(sample)\n",
    "# print(sample_original)\n",
    "# print(sample)\n",
    "# print(generated_sentences)\n",
    "\n",
    "original_sentences = []\n",
    "\n",
    "augmented_sentences = []\n",
    "augmented_labels = []\n",
    "#final_training_data = final_training_data.dropna().reset_index(drop=True)\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    if row['stance'] == 'negative' or row['stance'] == 'neutral':\n",
    "        generated_sentences = aug.augment(row['processed_tweet'])\n",
    "        original_sentences.append(row['processed_tweet'])\n",
    "        for sentence in generated_sentences:\n",
    "            augmented_sentences.append(sentence)\n",
    "            augmented_labels.append(row['stance'])\n",
    "            \n",
    "original_augmented_comparison_df = pd.DataFrame(list(zip(original_sentences, augmented_sentences)), columns = ['original', 'augmented'])\n",
    "original_augmented_comparison_df.to_csv('experimental_datasets/CWEA_augments_inspect.csv')\n",
    "augmented_training_data = train_data.append(pd.DataFrame(list(zip(augmented_sentences, augmented_labels)), columns = ['processed_tweet', 'stance']), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a24035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "Augment positive\n",
    "Augment negative alone\n",
    "Check precision, recall per class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56db48c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    2031\n",
       "negative    1044\n",
       "neutral      967\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e86f9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('processed_datasets/final_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df33cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4493\n"
     ]
    }
   ],
   "source": [
    "print(len(final_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b713530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2088\n",
       "positive    2031\n",
       "neutral      967\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_training_data['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67fafc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_training_data.to_csv('processed_datasets/final_augmented_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eff54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_training_data = pd.read_csv('processed_datasets/final_augmented_training_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c2689",
   "metadata": {},
   "source": [
    "### Loading Non-english datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e8e9f",
   "metadata": {},
   "source": [
    "### 1. VaccinEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc2d4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccin_eu_french = pd.read_csv('./datasets/VaccinEU/labeled_tweets_french.csv', sep = '\\t', header = 0, names = ['tweet', 'stance'])\n",
    "vaccin_eu_german = pd.read_csv('./datasets/VaccinEU/labeled_tweets_german.csv', sep = '\\t', header = 0, names = ['tweet', 'stance'])\n",
    "vaccin_eu_italian = pd.read_csv('./datasets/VaccinEU/labeled_tweets_italian.csv', sep = '\\t', header = 0, names = ['tweet', 'stance'])\n",
    "\n",
    "label_mapping = {1:'positive', 2:'negative', 3:'neutral'}\n",
    "\n",
    "vaccin_eu_french = vaccin_eu_french[vaccin_eu_french['stance'] != 4]\n",
    "vaccin_eu_german = vaccin_eu_german[vaccin_eu_german['stance'] != 4]\n",
    "vaccin_eu_italian = vaccin_eu_italian[vaccin_eu_italian['stance'] != 4]\n",
    "\n",
    "vaccin_eu_french['stance'] = vaccin_eu_french['stance'].replace(label_mapping)\n",
    "vaccin_eu_german['stance'] = vaccin_eu_german['stance'].replace(label_mapping)\n",
    "vaccin_eu_italian['stance'] = vaccin_eu_italian['stance'].replace(label_mapping)\n",
    "\n",
    "vaccin_eu_french['processed_tweet'] = vaccin_eu_french['tweet'].apply(process_tweet)\n",
    "vaccin_eu_german['processed_tweet'] = vaccin_eu_german['tweet'].apply(process_tweet)\n",
    "vaccin_eu_italian['processed_tweet'] = vaccin_eu_italian['tweet'].apply(process_tweet)\n",
    "\n",
    "vaccin_eu_french.to_csv('processed_datasets/VaccinEU/french_tweets.csv')\n",
    "vaccin_eu_german.to_csv('processed_datasets/VaccinEU/german_tweets.csv')\n",
    "vaccin_eu_italian.to_csv('processed_datasets/VaccinEU/italian_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8c7a8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    419\n",
       "neutral     279\n",
       "negative    135\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccin_eu_french['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b423acde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    547\n",
       "neutral     169\n",
       "negative    108\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccin_eu_german['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5ef4dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     458\n",
       "positive    314\n",
       "negative    151\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccin_eu_italian['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e3fd854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Info pratique  Je viens de bouger ma deuxime injection du juillet pour le juillet sur Doctolib. Profitez-en. jemevaccine'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccin_eu_french.iloc[0]['processed_tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfc2b5",
   "metadata": {},
   "source": [
    "### Finetuning MBERT on the English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8449fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 22:06:25.874309: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-20 22:06:25.874979: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "    print(f'Found GPU at: {device_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2018041",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.has_mps:    \n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print('using the CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f5144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a7747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmented_df = pd.read_csv('processed_datasets/final_augmented_training_data.csv')\n",
    "# print(train_data['stance'].value_counts(normalize=True))\n",
    "# print(test_data['stance'].value_counts(normalize=True))\n",
    "# print(train_data[~train_data['processed_tweet'].notna()])\n",
    "\n",
    "train_encoded_sentences, train_labels = preprocessing(train_data)\n",
    "train_attention_masks = attention_masks(train_encoded_sentences)\n",
    "\n",
    "test_encoded_sentences, test_labels = preprocessing(test_data)\n",
    "test_attention_masks = attention_masks(test_encoded_sentences)\n",
    "\n",
    "train_inputs = torch.tensor(train_encoded_sentences)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(test_encoded_sentences)\n",
    "validation_labels = torch.tensor(test_labels)\n",
    "validation_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "# data loader for training\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# data loader for validation\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10618bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    0.340242\n",
       "positive    0.339198\n",
       "neutral     0.320561\n",
       "Name: stance, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df['stance'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c43dd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b19de72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed_val)\n",
    "#torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    num_labels = 3,   \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = True, \n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 3e-5, \n",
    "                  eps = 1e-8, \n",
    "                  weight_decay = 0.01\n",
    "                )\n",
    "\n",
    "epochs = 4\n",
    "lambd = 0.9\n",
    "temperature = 0.3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # 10% * datasetSize/batchSize\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1197671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(preds, labels):\n",
    "    p = np.argmax(preds, axis=1).flatten()\n",
    "    l = labels.flatten()\n",
    "    return np.sum(p==l)/len(l)\n",
    "\n",
    "def compute_f1(preds, labels):\n",
    "    p = np.argmax(preds, axis=1).tolist()\n",
    "    l = labels.tolist()\n",
    "    f1_macro = f1_score(l, p, average='macro')\n",
    "    f1_per_class = f1_score(l, p, average=None)\n",
    "    return f1_macro, f1_per_class\n",
    "\n",
    "def compute_precision_recall(preds, labels):\n",
    "    p = np.argmax(preds, axis=1).tolist()\n",
    "    l = labels.tolist()\n",
    "    precision_per_class = precision_score(l, p, average=None)\n",
    "    recall_per_class = recall_score(l, p, average=None)\n",
    "    return precision_per_class, recall_per_class\n",
    "\n",
    "def compute_contrastive_loss(temp, embedding, label):\n",
    "    \"\"\"calculate the contrastive loss\n",
    "    \"\"\"\n",
    "    # cosine similarity between embeddings\n",
    "    cosine_sim = cosine_similarity(embedding, embedding)\n",
    "    # remove diagonal elements from matrix\n",
    "    dis = cosine_sim[~np.eye(cosine_sim.shape[0], dtype=bool)].reshape(cosine_sim.shape[0], -1)\n",
    "    # apply temprature to elements\n",
    "    dis = dis / temp\n",
    "    cosine_sim = cosine_sim / temp\n",
    "    # apply exp to elements\n",
    "    dis = np.exp(dis)\n",
    "    cosine_sim = np.exp(cosine_sim)\n",
    "\n",
    "    # calculate row sum\n",
    "    row_sum = []\n",
    "    for i in range(len(embedding)):\n",
    "        row_sum.append(sum(dis[i]))\n",
    "    # calculate outer sum\n",
    "    contrastive_loss = 0\n",
    "    for i in range(len(embedding)):\n",
    "        n_i = label.tolist().count(label[i]) - 1\n",
    "        inner_sum = 0\n",
    "        # calculate inner sum\n",
    "        for j in range(len(embedding)):\n",
    "            if label[i] == label[j] and i != j:\n",
    "                inner_sum = inner_sum + np.log(cosine_sim[i][j] / row_sum[i])\n",
    "        if n_i != 0:\n",
    "            contrastive_loss += (inner_sum / (-n_i))\n",
    "        else:\n",
    "            contrastive_loss += 0\n",
    "    return contrastive_loss\n",
    "    \n",
    "\n",
    "def run_train(epochs):\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        print('======== Epoch {:} / {:} ========'.format(e + 1, epochs))\n",
    "        start_train_time = time.time()\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            if step%10 == 0:\n",
    "                elapsed = time.time()-start_train_time\n",
    "                print(f'{step}/{len(train_dataloader)} --> Time elapsed {elapsed}')\n",
    "\n",
    "            # input_data, input_masks, input_labels = batch\n",
    "            input_data = batch[0].to(device)\n",
    "            input_masks = batch[1].to(device)\n",
    "            input_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # forward propagation\n",
    "            out = model(input_data,\n",
    "                        token_type_ids = None, \n",
    "                        attention_mask = input_masks,\n",
    "                        labels = input_labels)\n",
    "            \n",
    "            cross_loss = out[0]\n",
    "            hidden_states = out[2]\n",
    "            last_hidden_state = hidden_states[-1][:, 0, :]\n",
    "            contrastive_loss = compute_contrastive_loss(temperature, last_hidden_state.cpu().detach().numpy(), input_labels)\n",
    "            new_loss = (lambd * contrastive_loss) + (1-lambd)*(cross_loss)\n",
    "            total_loss = total_loss + new_loss.item()\n",
    "\n",
    "            # backward propagation\n",
    "            new_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss = total_loss/len(train_dataloader)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Training took {time.time()-start_train_time}\")\n",
    "\n",
    "        # Validation\n",
    "        start_validation_time = time.time()\n",
    "        model.eval()\n",
    "        eval_loss, eval_acc = 0,0\n",
    "        \n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            eval_data, eval_masks, eval_labels = batch\n",
    "            with torch.no_grad():\n",
    "                out = model(eval_data,\n",
    "                            token_type_ids = None, \n",
    "                            attention_mask=eval_masks)\n",
    "            logits = out[0]\n",
    "\n",
    "            #  Uncomment for GPU execution\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            eval_labels = eval_labels.to('cpu').numpy()\n",
    "            batch_acc = compute_accuracy(logits, eval_labels)\n",
    "            batch_f1_macro, batch_f1_per_class = compute_f1(logits, eval_labels)\n",
    "            batch_precision, batch_recall = compute_precision_recall(logits, eval_labels)\n",
    "\n",
    "            # Uncomment for CPU execution\n",
    "            # batch_acc = compute_accuracy(logits.numpy(), eval_labels.numpy())\n",
    "#             eval_f1 += batch_f1_macro\n",
    "            \n",
    "#             eval_acc += batch_acc\n",
    "            \n",
    "        print(f\"Accuracy: {batch_acc}, Time elapsed: {time.time()-start_validation_time}\")\n",
    "        print(f\"F1 score (Macro): {batch_f1_macro}\")\n",
    "        print(f\"F1 score (Per class): {batch_f1_per_class}\")\n",
    "        print(f\"Precision score (Per class): {batch_precision}\")\n",
    "        print(f\"Recall score (Per class): {batch_recall}\")\n",
    "        \n",
    "    \n",
    "    torch.save(model.state_dict(), 'model/model_params.pth')\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b54689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 4 ========\n",
      "0/127 --> Time elapsed 0.004640817642211914\n",
      "10/127 --> Time elapsed 54.2857871055603\n",
      "20/127 --> Time elapsed 104.97831702232361\n",
      "30/127 --> Time elapsed 152.73749208450317\n",
      "40/127 --> Time elapsed 200.0746431350708\n",
      "50/127 --> Time elapsed 247.9450421333313\n",
      "60/127 --> Time elapsed 293.7600841522217\n",
      "70/127 --> Time elapsed 341.88068199157715\n",
      "80/127 --> Time elapsed 389.26674604415894\n",
      "90/127 --> Time elapsed 436.22749304771423\n",
      "100/127 --> Time elapsed 484.44323110580444\n",
      "110/127 --> Time elapsed 532.0291759967804\n",
      "120/127 --> Time elapsed 580.200798034668\n",
      "Training took 610.9949510097504\n",
      "Accuracy: 0.7060133630289532, Time elapsed: 16.912190914154053\n",
      "F1 score (Macro): 0.66513706606238\n",
      "F1 score (Per class): [0.56179775 0.65714286 0.77647059]\n",
      "Precision score (Per class): [0.61728395 0.67647059 0.7443609 ]\n",
      "Recall score (Per class): [0.51546392 0.63888889 0.81147541]\n",
      "======== Epoch 2 / 4 ========\n",
      "0/127 --> Time elapsed 0.006830692291259766\n",
      "10/127 --> Time elapsed 49.36410880088806\n",
      "20/127 --> Time elapsed 96.47195172309875\n",
      "30/127 --> Time elapsed 144.1337788105011\n",
      "40/127 --> Time elapsed 191.56403994560242\n",
      "50/127 --> Time elapsed 238.96025490760803\n",
      "60/127 --> Time elapsed 286.5180838108063\n",
      "70/127 --> Time elapsed 333.5581419467926\n",
      "80/127 --> Time elapsed 386.16527581214905\n",
      "90/127 --> Time elapsed 434.31063866615295\n",
      "100/127 --> Time elapsed 483.21133375167847\n",
      "110/127 --> Time elapsed 530.939285993576\n",
      "120/127 --> Time elapsed 582.0060248374939\n",
      "Training took 611.6433207988739\n",
      "Accuracy: 0.7483296213808464, Time elapsed: 18.610371828079224\n",
      "F1 score (Macro): 0.726596367716953\n",
      "F1 score (Per class): [0.68539326 0.69354839 0.80084746]\n",
      "Precision score (Per class): [0.75308642 0.61428571 0.82894737]\n",
      "Recall score (Per class): [0.62886598 0.7962963  0.77459016]\n",
      "======== Epoch 3 / 4 ========\n",
      "0/127 --> Time elapsed 0.0040378570556640625\n",
      "10/127 --> Time elapsed 57.511919021606445\n",
      "20/127 --> Time elapsed 108.1769769191742\n",
      "30/127 --> Time elapsed 157.70968174934387\n",
      "40/127 --> Time elapsed 206.20580983161926\n",
      "50/127 --> Time elapsed 256.621356010437\n",
      "60/127 --> Time elapsed 306.55811882019043\n",
      "70/127 --> Time elapsed 356.4792540073395\n",
      "80/127 --> Time elapsed 403.8694579601288\n",
      "90/127 --> Time elapsed 454.30623292922974\n",
      "100/127 --> Time elapsed 505.7969300746918\n",
      "110/127 --> Time elapsed 556.4310698509216\n",
      "120/127 --> Time elapsed 605.7437801361084\n",
      "Training took 636.9857199192047\n",
      "Accuracy: 0.6636971046770601, Time elapsed: 18.727325916290283\n",
      "F1 score (Macro): 0.6411222850466315\n",
      "F1 score (Per class): [0.5988024  0.59285714 0.73170732]\n",
      "Precision score (Per class): [0.71428571 0.48255814 0.79710145]\n",
      "Recall score (Per class): [0.51546392 0.76851852 0.67622951]\n",
      "======== Epoch 4 / 4 ========\n",
      "0/127 --> Time elapsed 0.003921985626220703\n",
      "10/127 --> Time elapsed 87.66769886016846\n",
      "20/127 --> Time elapsed 182.4347279071808\n",
      "30/127 --> Time elapsed 272.5808539390564\n",
      "40/127 --> Time elapsed 374.1040060520172\n",
      "50/127 --> Time elapsed 472.71168518066406\n",
      "60/127 --> Time elapsed 589.4426019191742\n",
      "70/127 --> Time elapsed 700.5040061473846\n",
      "80/127 --> Time elapsed 804.7891299724579\n",
      "90/127 --> Time elapsed 906.3094580173492\n",
      "100/127 --> Time elapsed 1001.3923959732056\n",
      "110/127 --> Time elapsed 1106.8176860809326\n",
      "120/127 --> Time elapsed 1200.197163105011\n",
      "Training took 1262.1133041381836\n",
      "Accuracy: 0.7371937639198218, Time elapsed: 15.41939401626587\n",
      "F1 score (Macro): 0.7110683523093614\n",
      "F1 score (Per class): [0.67403315 0.65833333 0.80083857]\n",
      "Precision score (Per class): [0.72619048 0.59848485 0.81974249]\n",
      "Recall score (Per class): [0.62886598 0.73148148 0.78278689]\n"
     ]
    }
   ],
   "source": [
    "losses = run_train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efa2eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(df_test):\n",
    "\n",
    "    test_encoded_sentences, test_labels = preprocessing(df_test)\n",
    "    test_attention_masks = attention_masks(test_encoded_sentences)\n",
    "\n",
    "    test_inputs = torch.tensor(test_encoded_sentences)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=len(test_data))\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    num_labels = 3,   \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False,)\n",
    "    model.to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load('model/model_params.pth'))\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_acc = 0,0\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        eval_data, eval_masks, eval_labels = batch\n",
    "        with torch.no_grad():\n",
    "            out = model(eval_data,\n",
    "                        token_type_ids = None,\n",
    "                        attention_mask=eval_masks)\n",
    "        logits = out[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        eval_labels = eval_labels.to('cpu').numpy()\n",
    "        batch_acc = compute_accuracy(logits, eval_labels)\n",
    "        f1_macro, f1_classwise = compute_f1(logits, eval_labels)\n",
    "        precision_test, recall_test = compute_precision_recall(logits, eval_labels)\n",
    "        #eval_acc += batch_acc\n",
    "    print(f\"Accuracy: {batch_acc}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro}\")\n",
    "    print(f\"F1 Score (Classwise): {f1_classwise}\")\n",
    "    print(f\"Precision score (Per class): {precision_test}\")\n",
    "    print(f\"Recall score (Per class): {recall_test}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cef3a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.17 GB, other allocations: 9.35 GB, max allowed: 18.13 GB). Tried to allocate 1.22 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvaccin_eu_french\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mrun_test\u001b[0;34m(df_test)\u001b[0m\n\u001b[1;32m     27\u001b[0m eval_data, eval_masks, eval_labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 29\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m logits \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:549\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:450\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    449\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 450\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tf-conda-hf-torch/lib/python3.9/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 9.17 GB, other allocations: 9.35 GB, max allowed: 18.13 GB). Tried to allocate 1.22 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "run_test(vaccin_eu_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc1ea582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5752427184466019\n",
      "F1 Score (Macro): 0.5109529812368666\n",
      "F1 Score (Classwise): [0.33125    0.51073986 0.69086909]\n",
      "Precision score (Per class): [0.25       0.428      0.86740331]\n",
      "Recall score (Per class): [0.49074074 0.63313609 0.57404022]\n"
     ]
    }
   ],
   "source": [
    "run_test(vaccin_eu_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9b7955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49295774647887325\n",
      "F1 Score (Macro): 0.4707186596868637\n",
      "F1 Score (Classwise): [0.35698925 0.60201511 0.45315162]\n",
      "Precision score (Per class): [0.26433121 0.71130952 0.48717949]\n",
      "Recall score (Per class): [0.54966887 0.52183406 0.42356688]\n"
     ]
    }
   ],
   "source": [
    "run_test(vaccin_eu_italian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f4b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
