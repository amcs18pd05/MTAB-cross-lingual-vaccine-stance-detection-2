{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf1e1bc",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e99eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bharathia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "from array import array\n",
    "import preprocessor as p\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from functions import *\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import F1Score\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertPreTrainedModel, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "import warnings\n",
    "\n",
    "nltk.download('wordnet')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# seed_val = 42\n",
    "# torch.manual_seed(42)\n",
    "# random.seed(seed_val)\n",
    "# np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bec1fbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0.dev20220922'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e90ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "seed_everything(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/84288\n",
    "# https://github.com/pytorch/pytorch/issues/7068\n",
    "# https://github.com/facebookresearch/maskrcnn-benchmark/issues/376\n",
    "# https://discuss.pytorch.org/t/nondeterminism-even-when-setting-all-seeds-0-workers-and-cudnn-deterministic/26080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e90dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3380, -0.2398,  0.3864], device='mps:0')\n",
      "tensor([-0.3380, -0.2398,  0.3864], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(999)\n",
    "print (torch.randn(3, device='mps'))\n",
    "torch.manual_seed(999)\n",
    "print (torch.randn(3, device='mps'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced0b2b",
   "metadata": {},
   "source": [
    "### Loading the combined english training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec0c97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data = pd.read_csv('processed_datasets/combined_english_tweets.csv')\n",
    "english_training_data = english_data[['processed_tweet', 'stance']]\n",
    "english_train_data, english_test_data = train_test_split(english_training_data, 0.1)\n",
    "english_train_data = english_train_data.dropna().reset_index(drop=True)\n",
    "english_test_data = english_test_data.dropna().reset_index(drop=True)\n",
    "english_train_data = english_train_data.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18db3c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    2031\n",
       "negative    1044\n",
       "neutral      967\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_data['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7657d20",
   "metadata": {},
   "source": [
    "### Loading translation augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db0d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_training_data = pd.read_csv('processed_datasets/backtranslated_train_data.csv')\n",
    "english_train_data, english_test_data = train_test_split(english_training_data, 0.1)\n",
    "english_train_data = english_train_data.dropna().reset_index(drop=True)\n",
    "english_test_data = english_test_data.dropna().reset_index(drop=True)\n",
    "english_train_data = english_train_data.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ace06a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>1181</td>\n",
       "      <td>This article by the great gets right into who ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7820</th>\n",
       "      <td>8717</td>\n",
       "      <td>LUTTE CONTRE LA TYRANNY!!! Le vaccin contre le...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13614</th>\n",
       "      <td>15131</td>\n",
       "      <td>I ricercatori con l'Università del Texas e Pfi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>320</td>\n",
       "      <td>Relieved to report that mum has just had her s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9292</th>\n",
       "      <td>10356</td>\n",
       "      <td>Ich hatte gerade die erste Dosis meines Pfizer...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                    processed_tweet    stance\n",
       "1071         1181  This article by the great gets right into who ...  negative\n",
       "7820         8717  LUTTE CONTRE LA TYRANNY!!! Le vaccin contre le...  negative\n",
       "13614       15131  I ricercatori con l'Università del Texas e Pfi...  positive\n",
       "291           320  Relieved to report that mum has just had her s...  positive\n",
       "9292        10356  Ich hatte gerade die erste Dosis meines Pfizer...  positive"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dde51c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>Under the leadership of , we have developed a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Current(ish; new data always pouring in) effic...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>Just think before you refuse to have the new C...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>Champagne and questions greet first data showi...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203</th>\n",
       "      <td>Canada made $1 billion upfront payments to Pfi...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        processed_tweet    stance\n",
       "4003  Under the leadership of , we have developed a ...  positive\n",
       "149   Current(ish; new data always pouring in) effic...  positive\n",
       "2025  Just think before you refuse to have the new C...  positive\n",
       "2505  Champagne and questions greet first data showi...   neutral\n",
       "3203  Canada made $1 billion upfront payments to Pfi...   neutral"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38309888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    8208\n",
       "negative    4078\n",
       "neutral     3884\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_data['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2118bdf6",
   "metadata": {},
   "source": [
    "### Data resampling - Oversampling minority classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fab9c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'neutral' 'negative' ... 'neutral' 'neutral' 'neutral']\n",
      "positive    2031\n",
      "neutral     2031\n",
      "negative    2031\n",
      "Name: stance, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'stance'\n",
    "minority_classes = ['negative', 'neutral']\n",
    "\n",
    "majority_df = english_train_data[english_train_data[target_variable].isin(minority_classes)==False]\n",
    "minority_df = english_train_data[english_train_data[target_variable].isin(minority_classes)]\n",
    "\n",
    "le = LabelEncoder()\n",
    "minority_df[target_variable] = le.fit_transform(minority_df[target_variable])\n",
    "#print(minority_df.head())\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy = {0:2031, 1:2031}, random_state = 42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(minority_df.drop(target_variable, axis=1), minority_df[target_variable])\n",
    "y_resampled = le.inverse_transform(y_resampled)\n",
    "\n",
    "print(y_resampled)\n",
    "\n",
    "resampled_df = pd.DataFrame({'processed_tweet':X_resampled['processed_tweet'], 'stance': y_resampled})\n",
    "# X_resampled_df = pd.DataFrame(X_resampled, columns=minority_df.drop(target_variable, axis=1).columns)\n",
    "# y_resampled_df = pd.DataFrame(y_resampled, columns=[target_variable])\n",
    "english_train_data = pd.concat([majority_df, resampled_df])\n",
    "\n",
    "# Print value counts of target variable to check if upsampling worked\n",
    "print(english_train_data[target_variable].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2550b2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>Under the leadership of , we have developed a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Current(ish; new data always pouring in) effic...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>Just think before you refuse to have the new C...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>Just had my vaccine this morning and it was As...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>Unlike most drugs, vaccines are not static. Wi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        processed_tweet    stance\n",
       "4003  Under the leadership of , we have developed a ...  positive\n",
       "149   Current(ish; new data always pouring in) effic...  positive\n",
       "2025  Just think before you refuse to have the new C...  positive\n",
       "952   Just had my vaccine this morning and it was As...  positive\n",
       "1041  Unlike most drugs, vaccines are not static. Wi...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48161af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [processed_tweet, stance]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_data[english_train_data['stance'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deebaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_balanced = english_train_data.groupby('stance')\n",
    "english_train_balanced = english_train_balanced.apply(lambda x: x.sample(english_train_balanced.size().min()).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbe9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_translations = pickle.load(open('processed_datasets/french_translations.pkl', 'rb'))\n",
    "german_translations = pickle.load(open('processed_datasets/german_translations.pkl', 'rb'))\n",
    "italian_translations = pickle.load(open('processed_datasets/italian_translations.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb76e7",
   "metadata": {},
   "source": [
    "### Loading the unlabeled data for adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce14e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_pro_ids = []\n",
    "french_anti_ids = []\n",
    "french_nlbl_ids = []\n",
    "\n",
    "german_pro_ids = []\n",
    "german_anti_ids = []\n",
    "german_nlbl_ids = []\n",
    "\n",
    "italian_pro_ids = []\n",
    "italian_anti_ids = []\n",
    "italian_nlbl_ids = []\n",
    "\n",
    "\n",
    "\n",
    "with open('datasets/VaccinEU/french/french_anti_ids.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    french_anti_ids = lines[0:1500]\n",
    "    \n",
    "with open('datasets/VaccinEU/french/french_pro_ids.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    french_pro_ids = lines[0:1500]\n",
    "    \n",
    "with open('datasets/VaccinEU/french/french_ids.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    french_nlbl_ids = random.sample(lines, 4000)\n",
    "\n",
    "french_ids = french_pro_ids + french_anti_ids + french_nlbl_ids\n",
    "\n",
    "with open('datasets/VaccinEU/german/german_anti_ids.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    german_anti_ids = lines[0:1000]\n",
    "    \n",
    "with open('datasets/VaccinEU/german/german_pro_ids.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    german_pro_ids = lines[0:1000]\n",
    "    \n",
    "with open('datasets/VaccinEU/german/german_ids.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    german_nlbl_ids = random.sample(lines, 4000)\n",
    "\n",
    "german_ids = german_pro_ids + german_anti_ids + german_nlbl_ids\n",
    "\n",
    "with open('datasets/VaccinEU/italian/italian_anti_ids.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    italian_anti_ids = lines[0:1000]\n",
    "    \n",
    "with open('datasets/VaccinEU/italian/italian_pro_ids.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    italian_pro_ids = lines[0:1000]\n",
    "    \n",
    "with open('datasets/VaccinEU/italian/italian_ids.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "    italian_nlbl_ids = random.sample(lines, 4000)\n",
    "\n",
    "italian_ids = italian_pro_ids + italian_anti_ids + italian_nlbl_ids\n",
    "\n",
    "df_french = pd.DataFrame(french_ids)\n",
    "df_french.to_csv('processed_datasets/vaccinEU/french_adv_ids.csv', header=False, index=None)\n",
    "\n",
    "df_german = pd.DataFrame(german_ids)\n",
    "df_german.to_csv('processed_datasets/vaccinEU/german_adv_ids.csv', header=False, index=None)\n",
    "\n",
    "df_italian = pd.DataFrame(italian_ids)\n",
    "df_italian.to_csv('processed_datasets/vaccinEU/italian_adv_ids.csv', header=False, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35187023",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_adv_tweets = pd.read_csv('processed_datasets/vaccinEU/french_adv_ids_hydrated.csv')['text']\n",
    "german_adv_tweets = pd.read_csv('processed_datasets/vaccinEU/german_adv_ids_hydrated.csv')['text']\n",
    "italian_adv_tweets = pd.read_csv('processed_datasets/vaccinEU/italian_adv_ids_hydrated.csv')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16985427",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_adv_tweets = french_adv_tweets.apply(process_tweet).tolist()\n",
    "german_adv_tweets = german_adv_tweets.apply(process_tweet).tolist()\n",
    "italian_adv_tweets = italian_adv_tweets.apply(process_tweet).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0382b1",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca686ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 12:20:01.212948: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-27 12:20:01.213489: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "    print(f'Found GPU at: {device_name}')\n",
    "    \n",
    "if torch.has_mps:    \n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print('using the CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61277c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b31ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128 # max sequences length\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b132d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_translated_data(translated_data: List[str]) -> List[int]:\n",
    "    sentences = np.array(translated_data)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
    "    \n",
    "    encoded_sentences = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,\n",
    "                            add_special_tokens = True,\n",
    "                            truncation=True,\n",
    "                            max_length = MAX_LEN\n",
    "                    )\n",
    "        \n",
    "        encoded_sentences.append(encoded_sent)\n",
    "    encoded_sentences = pad_sequences(encoded_sentences, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                            value=0, truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    return encoded_sentences\n",
    "\n",
    "def _init_fn():\n",
    "     np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb8dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_sentences, train_labels = preprocessing(english_train_data)\n",
    "train_attention_masks = attention_masks(train_encoded_sentences)\n",
    "\n",
    "test_encoded_sentences, test_labels = preprocessing(english_test_data)\n",
    "test_attention_masks = attention_masks(test_encoded_sentences)\n",
    "\n",
    "train_inputs = torch.tensor(train_encoded_sentences)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(test_encoded_sentences)\n",
    "validation_labels = torch.tensor(test_labels)\n",
    "validation_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "# data loader for training\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "#train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "# data loader for validation\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "#validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_data, batch_size=len(validation_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a939a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader for translated text\n",
    "translated_data = italian_adv_tweets\n",
    "train_translated_sentences = preprocessing_translated_data(translated_data)\n",
    "train_translated_attention_masks = attention_masks(train_translated_sentences)\n",
    "\n",
    "train_translated_inputs = torch.tensor(train_translated_sentences)\n",
    "train_translated_masks = torch.tensor(train_translated_attention_masks)\n",
    "\n",
    "train_translated_data = TensorDataset(train_translated_inputs, train_translated_masks)\n",
    "#train_translated_sampler = SequentialSampler(train_translated_data)\n",
    "train_translated_dataloader = DataLoader(\n",
    "    train_translated_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee03c989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5131"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(italian_adv_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102ac20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccin_eu_french = pd.read_csv('processed_datasets/VaccinEU/french_tweets.csv')\n",
    "vaccin_eu_german = pd.read_csv('processed_datasets/VaccinEU/german_tweets.csv')\n",
    "vaccin_eu_italian = pd.read_csv('processed_datasets/VaccinEU/italian_tweets.csv')\n",
    "\n",
    "french_encoded_sentences, french_labels = preprocessing(vaccin_eu_french)\n",
    "french_attention_masks = attention_masks(french_encoded_sentences)\n",
    "\n",
    "french_inputs = torch.tensor(french_encoded_sentences)\n",
    "french_labels = torch.tensor(french_labels)\n",
    "french_masks = torch.tensor(french_attention_masks)\n",
    "\n",
    "french_data = TensorDataset(french_inputs, french_masks, french_labels)\n",
    "french_sampler = SequentialSampler(french_data)\n",
    "french_dataloader = DataLoader(french_data, sampler=french_sampler, batch_size=len(french_data))\n",
    "\n",
    "\n",
    "german_encoded_sentences, german_labels = preprocessing(vaccin_eu_german)\n",
    "german_attention_masks = attention_masks(german_encoded_sentences)\n",
    "\n",
    "german_inputs = torch.tensor(german_encoded_sentences)\n",
    "german_labels = torch.tensor(german_labels)\n",
    "german_masks = torch.tensor(german_attention_masks)\n",
    "\n",
    "german_data = TensorDataset(german_inputs, german_masks, german_labels)\n",
    "german_sampler = SequentialSampler(german_data)\n",
    "german_dataloader = DataLoader(german_data, sampler=german_sampler, batch_size=len(german_data))\n",
    "\n",
    "\n",
    "italian_encoded_sentences, italian_labels = preprocessing(vaccin_eu_italian)\n",
    "italian_attention_masks = attention_masks(italian_encoded_sentences)\n",
    "\n",
    "italian_inputs = torch.tensor(italian_encoded_sentences)\n",
    "italian_labels = torch.tensor(italian_labels)\n",
    "italian_masks = torch.tensor(italian_attention_masks)\n",
    "\n",
    "italian_data = TensorDataset(italian_inputs, italian_masks, italian_labels)\n",
    "italian_sampler = SequentialSampler(italian_data)\n",
    "italian_dataloader = DataLoader(italian_data, sampler=italian_sampler, batch_size=len(italian_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4617f22",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/ludovicocuoghi/detecting-bullying-tweets-pytorch-lstm-bert#PyTorch-LSTM-modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541338f",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dda61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 3\n",
    "hidden_size = 768\n",
    "intermediate_size = 800\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.encoder = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        outputs = self.encoder(x, attention_mask=mask)\n",
    "        feat = outputs[0][:, 0, :]\n",
    "#         feat = outputs[0]\n",
    "        return feat\n",
    "    \n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size, num_classes, dropout=0.1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=768, hidden_size=lstm_hidden_size, dropout = dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class BiLSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size, num_classes, dropout=0.1):\n",
    "        super(BiLSTMAttentionClassifier, self).__init__()\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=768, hidden_size=lstm_hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(2*lstm_hidden_size, num_classes)\n",
    "\n",
    "        self.attention = nn.Linear(2*lstm_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_output, _ = self.bilstm(x)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        attention_scores = self.attention(lstm_output).squeeze(-1)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(-1)\n",
    "        attention_output = torch.sum(lstm_output * attention_weights, dim=1)\n",
    "\n",
    "        # Apply dropout and pass through the fully connected layer\n",
    "        output = self.dropout(attention_output)\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        out = self.classifier(x)\n",
    "        return out\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "            \n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator model for source language\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(intermediate_size, intermediate_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(intermediate_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        out = self.layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f369b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "batch_size = 32\n",
    "pre_epochs = 3\n",
    "adapt_epochs = 2\n",
    "pre_log_step = 5\n",
    "adapt_log_step = 5\n",
    "clip_value = 0.01\n",
    "c_learning_rate = 5e-5\n",
    "d_learning_rate = 1e-5\n",
    "temperature = 20\n",
    "lambd = 0.9\n",
    "contrastive_temp = 0.3\n",
    "\n",
    "src_encoder = BertEncoder()\n",
    "src_classifier = BertClassifier()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "\n",
    "src_encoder = src_encoder.to(device)\n",
    "src_classifier = src_classifier.to(device)\n",
    "lstm_classifier = LSTMClassifier(lstm_hidden_size=128, num_classes=3)\n",
    "bi_lstm_classifier = BiLSTMAttentionClassifier(lstm_hidden_size = 128, num_classes=3)\n",
    "discriminator = discriminator.to(device)\n",
    "gbc = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7f8b7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ecb1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "    p = np.argmax(preds, axis=1).flatten()\n",
    "    l = labels.flatten()\n",
    "    #p = preds.flatten()\n",
    "#     print(\"True labels:\", l)\n",
    "#     print(\"Predictions:\", p)\n",
    "    return np.sum(p==l)/len(l)\n",
    "\n",
    "def compute_f1(preds, labels):\n",
    "    p = np.argmax(preds, axis=1).tolist()\n",
    "    #p = preds.tolist()\n",
    "    l = labels.tolist()\n",
    "    f1_macro = f1_score(l, p, average='macro')\n",
    "    f1_per_class = f1_score(l, p, average=None)\n",
    "    return f1_macro, f1_per_class\n",
    "\n",
    "def compute_precision_recall(preds, labels):\n",
    "    p = np.argmax(preds, axis=1).tolist()\n",
    "    #p = preds.tolist()\n",
    "    l = labels.tolist()\n",
    "    precision_per_class = precision_score(l, p, average=None)\n",
    "    recall_per_class = recall_score(l, p, average=None)\n",
    "    return precision_per_class, recall_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cceff488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_errors(encoder, classifier, data_loader):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    # init loss and accuracy\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    # set loss function\n",
    "    #criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([0.85,0.75,0.5]).to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # evaluate network\n",
    "    for (inputs, mask, labels) in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = encoder(inputs, mask)\n",
    "            preds = classifier(feat)\n",
    "        loss += criterion(preds, labels).item()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.to('cpu').numpy()\n",
    "        \n",
    "    preds = np.argmax(preds, axis=1).flatten()\n",
    "    preds = preds.tolist()\n",
    "    labels = labels.flatten()\n",
    "    labels = labels.tolist()\n",
    "    \n",
    "    incorrect_indices = [i for i in range(len(labels)) if labels[i] != preds[i]]\n",
    "    \n",
    "    incorrect_preds = [preds[i] for i in incorrect_indices]\n",
    "    \n",
    "    correct_labels = [labels[i] for i in incorrect_indices]\n",
    "    \n",
    "    return incorrect_indices, correct_labels, incorrect_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc1db250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation(encoder, classifier, data_loader):\n",
    "    \"\"\"Evaluation for target language encoder by source classifier on target language dataset\"\"\"\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    # set loss function\n",
    "    #criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([0.85,0.75,0.5]).to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # evaluate network\n",
    "    for (inputs, mask, labels) in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = encoder(inputs, mask)\n",
    "            preds = classifier(feat)\n",
    "#             feat = feat.detach().cpu().numpy()\n",
    "        #preds = gbc_classifier.predict_proba(feat)\n",
    "        loss += criterion(preds, labels).item()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        #preds = gbc_classifier.predict(preds)\n",
    "        labels = labels.to('cpu').numpy()\n",
    "        \n",
    "        batch_acc = compute_accuracy(preds, labels)\n",
    "        batch_f1_macro, batch_f1_per_class = compute_f1(preds, labels)\n",
    "        batch_precision, batch_recall = compute_precision_recall(preds, labels)\n",
    "\n",
    "#         pred_cls = preds.data.max(1)[1]\n",
    "#         acc += pred_cls.eq(labels.data).cpu().sum().item()\n",
    "    print(\"Validation loss: \", loss)\n",
    "    print(f\"Accuracy: {batch_acc}\")\n",
    "    print(f\"F1 score (Macro): {batch_f1_macro}\")\n",
    "    print(f\"F1 score (Per class): {batch_f1_per_class}\")\n",
    "    print(f\"Precision score (Per class): {batch_precision}\")\n",
    "    print(f\"Recall score (Per class): {batch_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b97efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrastive_loss(temp, embedding, label):\n",
    "    \"\"\"calculate the contrastive loss\n",
    "    \"\"\"\n",
    "    # cosine similarity between embeddings\n",
    "    cosine_sim = cosine_similarity(embedding, embedding)\n",
    "    # remove diagonal elements from matrix\n",
    "    dis = cosine_sim[~np.eye(cosine_sim.shape[0], dtype=bool)].reshape(cosine_sim.shape[0], -1)\n",
    "    # apply temprature to elements\n",
    "    dis = dis / temp\n",
    "    cosine_sim = cosine_sim / temp\n",
    "    # apply exp to elements\n",
    "    dis = np.exp(dis)\n",
    "    cosine_sim = np.exp(cosine_sim)\n",
    "\n",
    "    # calculate row sum\n",
    "    row_sum = []\n",
    "    for i in range(len(embedding)):\n",
    "        row_sum.append(sum(dis[i]))\n",
    "    # calculate outer sum\n",
    "    contrastive_loss = 0\n",
    "    for i in range(len(embedding)):\n",
    "        n_i = label.tolist().count(label[i]) - 1\n",
    "        inner_sum = 0\n",
    "        # calculate inner sum\n",
    "        for j in range(len(embedding)):\n",
    "            if label[i] == label[j] and i != j:\n",
    "                inner_sum = inner_sum + np.log(cosine_sim[i][j] / row_sum[i])\n",
    "        if n_i != 0:\n",
    "            contrastive_loss += (inner_sum / (-n_i))\n",
    "        else:\n",
    "            contrastive_loss += 0\n",
    "    return contrastive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d7776de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "598ec81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(src_enc, src_class):\n",
    "    tgt_enc = BertEncoder().to(device)\n",
    "    tgt_enc.load_state_dict(src_enc.state_dict())\n",
    "    max_grad_norm = 1.0\n",
    "    \n",
    "    print(\"French Test: \\n\")\n",
    "    evaluate_validation(tgt_enc, src_class, french_dataloader)\n",
    "    print(\"German Test:\")\n",
    "    evaluate_validation(tgt_enc, src_class, german_dataloader)\n",
    "    print(\"Italian Test: \\n\")\n",
    "    evaluate_validation(tgt_enc, src_class, italian_dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f70bda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(data_loader):\n",
    "    \"\"\"Train classifier for source language.\"\"\"\n",
    "    \n",
    "    global src_encoder\n",
    "#     global bi_lstm_classifier\n",
    "    global src_classifier\n",
    "\n",
    "    # setup criterion and optimizer\n",
    "#     optimizer = torch.optim.AdamW(list(src_encoder.parameters()) + list(bi_lstm_classifier.parameters()),\n",
    "#                            lr=c_learning_rate)\n",
    "#     optimizer = torch.optim.AdamW(list(encoder.parameters()),lr=c_learning_rate)\n",
    "    optimizer = torch.optim.AdamW(list(src_encoder.parameters()) + list(src_classifier.parameters()),\n",
    "                              lr=c_learning_rate)\n",
    "    class_weights = [1.39,1.29,0.66]\n",
    "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "    CELoss = nn.CrossEntropyLoss(weight = class_weights)\n",
    "#     CELoss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # set train state for Dropout and BN layers\n",
    "    src_encoder.train()\n",
    "#     bi_lstm_classifier.train()\n",
    "    src_classifier.train()\n",
    "\n",
    "    for epoch in range(pre_epochs):\n",
    "        print(f\"Epoch: {epoch}/{pre_epochs}\")\n",
    "        for step, (inputs, mask, labels) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            mask = mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero gradients for optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute loss for discriminator\n",
    "            feat = src_encoder(inputs, mask)\n",
    "            preds = src_classifier(feat)\n",
    "            CE_loss = CELoss(preds, labels)\n",
    "#             contrastive_loss = compute_contrastive_loss(contrastive_temp, \n",
    "#                                                         feat.cpu().detach().numpy(), labels)\n",
    "#             cls_loss = (lambd * contrastive_loss) + (1-lambd)*(CE_loss)\n",
    "            cls_loss = CE_loss\n",
    "            \n",
    "\n",
    "            # optimize source classifier\n",
    "            cls_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print step info\n",
    "            if (step) % pre_log_step == 0:\n",
    "                print(\"Epoch [%.2d/%.2d] Step [%.3d/%.3d]: cls_loss=%.4f\"\n",
    "                      % (epoch,\n",
    "                         pre_epochs,\n",
    "                         step,\n",
    "                         len(data_loader),\n",
    "                         cls_loss.item()))\n",
    "        print(f'At the end of Epoch: {epoch}')\n",
    "            \n",
    "        \n",
    "        evaluate_validation(src_encoder, src_classifier, validation_dataloader)\n",
    "        evaluate_test(src_encoder, src_classifier)\n",
    "        torch.save(src_encoder.state_dict(), f\"model/pretrain/src_encoder_12_11_bt_class_weights_epoch_{epoch}.pth\")\n",
    "        torch.save(src_classifier.state_dict(), f\"model/pretrain/src_classifier_12_11_bt_class_weights_epoch_{epoch}.pth\")\n",
    "\n",
    "    return src_encoder, src_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CE only model only for 7 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a49d1a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10\n",
      "Epoch [00/10] Step [000/506]: cls_loss=1.1000\n",
      "Epoch [00/10] Step [005/506]: cls_loss=1.1221\n",
      "Epoch [00/10] Step [010/506]: cls_loss=1.1192\n",
      "Epoch [00/10] Step [015/506]: cls_loss=1.0528\n",
      "Epoch [00/10] Step [020/506]: cls_loss=1.0721\n",
      "Epoch [00/10] Step [025/506]: cls_loss=1.0746\n",
      "Epoch [00/10] Step [030/506]: cls_loss=1.0581\n",
      "Epoch [00/10] Step [035/506]: cls_loss=0.9930\n",
      "Epoch [00/10] Step [040/506]: cls_loss=0.9892\n",
      "Epoch [00/10] Step [045/506]: cls_loss=0.7411\n",
      "Epoch [00/10] Step [050/506]: cls_loss=1.2604\n",
      "Epoch [00/10] Step [055/506]: cls_loss=0.7803\n",
      "Epoch [00/10] Step [060/506]: cls_loss=0.9303\n",
      "Epoch [00/10] Step [065/506]: cls_loss=0.6414\n",
      "Epoch [00/10] Step [070/506]: cls_loss=0.7542\n",
      "Epoch [00/10] Step [075/506]: cls_loss=1.0886\n",
      "Epoch [00/10] Step [080/506]: cls_loss=1.0352\n",
      "Epoch [00/10] Step [085/506]: cls_loss=0.5419\n",
      "Epoch [00/10] Step [090/506]: cls_loss=0.9129\n",
      "Epoch [00/10] Step [095/506]: cls_loss=0.8157\n",
      "Epoch [00/10] Step [100/506]: cls_loss=0.6255\n",
      "Epoch [00/10] Step [105/506]: cls_loss=0.7481\n",
      "Epoch [00/10] Step [110/506]: cls_loss=0.5959\n",
      "Epoch [00/10] Step [115/506]: cls_loss=0.6585\n",
      "Epoch [00/10] Step [120/506]: cls_loss=0.8524\n",
      "Epoch [00/10] Step [125/506]: cls_loss=0.9356\n",
      "Epoch [00/10] Step [130/506]: cls_loss=0.8663\n",
      "Epoch [00/10] Step [135/506]: cls_loss=0.8088\n",
      "Epoch [00/10] Step [140/506]: cls_loss=0.7859\n",
      "Epoch [00/10] Step [145/506]: cls_loss=1.1896\n",
      "Epoch [00/10] Step [150/506]: cls_loss=0.8447\n",
      "Epoch [00/10] Step [155/506]: cls_loss=0.6236\n",
      "Epoch [00/10] Step [160/506]: cls_loss=0.7258\n",
      "Epoch [00/10] Step [165/506]: cls_loss=0.6507\n",
      "Epoch [00/10] Step [170/506]: cls_loss=1.1180\n",
      "Epoch [00/10] Step [175/506]: cls_loss=0.7492\n",
      "Epoch [00/10] Step [180/506]: cls_loss=0.9193\n",
      "Epoch [00/10] Step [185/506]: cls_loss=0.6855\n",
      "Epoch [00/10] Step [190/506]: cls_loss=0.6385\n",
      "Epoch [00/10] Step [195/506]: cls_loss=0.7106\n",
      "Epoch [00/10] Step [200/506]: cls_loss=0.6752\n",
      "Epoch [00/10] Step [205/506]: cls_loss=0.5248\n",
      "Epoch [00/10] Step [210/506]: cls_loss=0.5786\n",
      "Epoch [00/10] Step [215/506]: cls_loss=0.7369\n",
      "Epoch [00/10] Step [220/506]: cls_loss=0.8375\n",
      "Epoch [00/10] Step [225/506]: cls_loss=0.8055\n",
      "Epoch [00/10] Step [230/506]: cls_loss=0.8261\n",
      "Epoch [00/10] Step [235/506]: cls_loss=0.8499\n",
      "Epoch [00/10] Step [240/506]: cls_loss=0.7581\n",
      "Epoch [00/10] Step [245/506]: cls_loss=0.6193\n",
      "Epoch [00/10] Step [250/506]: cls_loss=0.8134\n",
      "Epoch [00/10] Step [255/506]: cls_loss=0.8184\n",
      "Epoch [00/10] Step [260/506]: cls_loss=0.5953\n",
      "Epoch [00/10] Step [265/506]: cls_loss=0.7293\n",
      "Epoch [00/10] Step [270/506]: cls_loss=0.5616\n",
      "Epoch [00/10] Step [275/506]: cls_loss=0.8081\n",
      "Epoch [00/10] Step [280/506]: cls_loss=0.4642\n",
      "Epoch [00/10] Step [285/506]: cls_loss=1.0127\n",
      "Epoch [00/10] Step [290/506]: cls_loss=0.7141\n",
      "Epoch [00/10] Step [295/506]: cls_loss=0.6794\n",
      "Epoch [00/10] Step [300/506]: cls_loss=0.7929\n",
      "Epoch [00/10] Step [305/506]: cls_loss=0.6060\n",
      "Epoch [00/10] Step [310/506]: cls_loss=0.9810\n",
      "Epoch [00/10] Step [315/506]: cls_loss=0.5956\n",
      "Epoch [00/10] Step [320/506]: cls_loss=0.4964\n",
      "Epoch [00/10] Step [325/506]: cls_loss=0.9875\n",
      "Epoch [00/10] Step [330/506]: cls_loss=0.4950\n",
      "Epoch [00/10] Step [335/506]: cls_loss=0.5232\n",
      "Epoch [00/10] Step [340/506]: cls_loss=0.5136\n",
      "Epoch [00/10] Step [345/506]: cls_loss=0.9108\n",
      "Epoch [00/10] Step [350/506]: cls_loss=0.6030\n",
      "Epoch [00/10] Step [355/506]: cls_loss=0.8265\n",
      "Epoch [00/10] Step [360/506]: cls_loss=0.5779\n",
      "Epoch [00/10] Step [365/506]: cls_loss=0.8817\n",
      "Epoch [00/10] Step [370/506]: cls_loss=0.3463\n",
      "Epoch [00/10] Step [375/506]: cls_loss=0.4265\n",
      "Epoch [00/10] Step [380/506]: cls_loss=0.8170\n",
      "Epoch [00/10] Step [385/506]: cls_loss=0.3416\n",
      "Epoch [00/10] Step [390/506]: cls_loss=0.7711\n",
      "Epoch [00/10] Step [395/506]: cls_loss=0.5514\n",
      "Epoch [00/10] Step [400/506]: cls_loss=0.5458\n",
      "Epoch [00/10] Step [405/506]: cls_loss=0.3508\n",
      "Epoch [00/10] Step [410/506]: cls_loss=0.5950\n",
      "Epoch [00/10] Step [415/506]: cls_loss=0.6625\n",
      "Epoch [00/10] Step [420/506]: cls_loss=0.6814\n",
      "Epoch [00/10] Step [425/506]: cls_loss=0.5485\n",
      "Epoch [00/10] Step [430/506]: cls_loss=0.5983\n",
      "Epoch [00/10] Step [435/506]: cls_loss=0.6599\n",
      "Epoch [00/10] Step [440/506]: cls_loss=0.3869\n",
      "Epoch [00/10] Step [445/506]: cls_loss=0.8766\n",
      "Epoch [00/10] Step [450/506]: cls_loss=0.5131\n",
      "Epoch [00/10] Step [455/506]: cls_loss=0.5290\n",
      "Epoch [00/10] Step [460/506]: cls_loss=0.7200\n",
      "Epoch [00/10] Step [465/506]: cls_loss=0.5056\n",
      "Epoch [00/10] Step [470/506]: cls_loss=0.5970\n",
      "Epoch [00/10] Step [475/506]: cls_loss=0.8000\n",
      "Epoch [00/10] Step [480/506]: cls_loss=0.5799\n",
      "Epoch [00/10] Step [485/506]: cls_loss=0.5302\n",
      "Epoch [00/10] Step [490/506]: cls_loss=0.4593\n",
      "Epoch [00/10] Step [495/506]: cls_loss=0.5399\n",
      "Epoch [00/10] Step [500/506]: cls_loss=0.5688\n",
      "Epoch [00/10] Step [505/506]: cls_loss=0.3554\n",
      "At the end of Epoch: 0\n",
      "Validation loss:  0.5586512088775635\n",
      "Accuracy: 0.7633630289532294\n",
      "F1 score (Macro): 0.7448446354846702\n",
      "F1 score (Per class): [0.68848758 0.7295174  0.81652893]\n",
      "Precision score (Per class): [0.7625     0.68565401 0.80368764]\n",
      "Recall score (Per class): [0.62757202 0.7793765  0.82978723]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  0.9932263493537903\n",
      "Accuracy: 0.5594237695078031\n",
      "F1 score (Macro): 0.4954965034965035\n",
      "F1 score (Per class): [0.32867133 0.476      0.68181818]\n",
      "Precision score (Per class): [0.31125828 0.53846154 0.65075922]\n",
      "Recall score (Per class): [0.34814815 0.4265233  0.71599045]\n",
      "German Test:\n",
      "Validation loss:  0.8138483166694641\n",
      "Accuracy: 0.6347087378640777\n",
      "F1 score (Macro): 0.5428684621296759\n",
      "F1 score (Per class): [0.34814815 0.52770449 0.75275275]\n",
      "Precision score (Per class): [0.29012346 0.47619048 0.83185841]\n",
      "Recall score (Per class): [0.43518519 0.59171598 0.68738574]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.12908136844635\n",
      "Accuracy: 0.46045503791982667\n",
      "F1 score (Macro): 0.44014364657914856\n",
      "F1 score (Per class): [0.34173669 0.49859155 0.4801027 ]\n",
      "Precision score (Per class): [0.2961165  0.70238095 0.40215054]\n",
      "Recall score (Per class): [0.40397351 0.38646288 0.5955414 ]\n",
      "Epoch: 1/10\n",
      "Epoch [01/10] Step [000/506]: cls_loss=0.6154\n",
      "Epoch [01/10] Step [005/506]: cls_loss=0.6014\n",
      "Epoch [01/10] Step [010/506]: cls_loss=0.5067\n",
      "Epoch [01/10] Step [015/506]: cls_loss=0.6375\n",
      "Epoch [01/10] Step [020/506]: cls_loss=0.4673\n",
      "Epoch [01/10] Step [025/506]: cls_loss=0.6630\n",
      "Epoch [01/10] Step [030/506]: cls_loss=0.4415\n",
      "Epoch [01/10] Step [035/506]: cls_loss=0.6904\n",
      "Epoch [01/10] Step [040/506]: cls_loss=0.3890\n",
      "Epoch [01/10] Step [045/506]: cls_loss=0.2434\n",
      "Epoch [01/10] Step [050/506]: cls_loss=0.5162\n",
      "Epoch [01/10] Step [055/506]: cls_loss=0.4281\n",
      "Epoch [01/10] Step [060/506]: cls_loss=0.5003\n",
      "Epoch [01/10] Step [065/506]: cls_loss=0.1398\n",
      "Epoch [01/10] Step [070/506]: cls_loss=0.3873\n",
      "Epoch [01/10] Step [075/506]: cls_loss=0.4329\n",
      "Epoch [01/10] Step [080/506]: cls_loss=0.5409\n",
      "Epoch [01/10] Step [085/506]: cls_loss=0.2274\n",
      "Epoch [01/10] Step [090/506]: cls_loss=0.2629\n",
      "Epoch [01/10] Step [095/506]: cls_loss=0.5083\n",
      "Epoch [01/10] Step [100/506]: cls_loss=0.4082\n",
      "Epoch [01/10] Step [105/506]: cls_loss=0.3524\n",
      "Epoch [01/10] Step [110/506]: cls_loss=0.3595\n",
      "Epoch [01/10] Step [115/506]: cls_loss=0.3336\n",
      "Epoch [01/10] Step [120/506]: cls_loss=0.5950\n",
      "Epoch [01/10] Step [125/506]: cls_loss=0.4725\n",
      "Epoch [01/10] Step [130/506]: cls_loss=0.6179\n",
      "Epoch [01/10] Step [135/506]: cls_loss=0.5803\n",
      "Epoch [01/10] Step [140/506]: cls_loss=0.3083\n",
      "Epoch [01/10] Step [145/506]: cls_loss=0.6595\n",
      "Epoch [01/10] Step [150/506]: cls_loss=0.3592\n",
      "Epoch [01/10] Step [155/506]: cls_loss=0.4722\n",
      "Epoch [01/10] Step [160/506]: cls_loss=0.3304\n",
      "Epoch [01/10] Step [165/506]: cls_loss=0.2754\n",
      "Epoch [01/10] Step [170/506]: cls_loss=0.5594\n",
      "Epoch [01/10] Step [175/506]: cls_loss=0.2820\n",
      "Epoch [01/10] Step [180/506]: cls_loss=0.5464\n",
      "Epoch [01/10] Step [185/506]: cls_loss=0.3178\n",
      "Epoch [01/10] Step [190/506]: cls_loss=0.6771\n",
      "Epoch [01/10] Step [195/506]: cls_loss=0.4771\n",
      "Epoch [01/10] Step [200/506]: cls_loss=0.3046\n",
      "Epoch [01/10] Step [205/506]: cls_loss=0.1979\n",
      "Epoch [01/10] Step [210/506]: cls_loss=0.2122\n",
      "Epoch [01/10] Step [215/506]: cls_loss=0.1907\n",
      "Epoch [01/10] Step [220/506]: cls_loss=0.4529\n",
      "Epoch [01/10] Step [225/506]: cls_loss=0.6971\n",
      "Epoch [01/10] Step [230/506]: cls_loss=0.5078\n",
      "Epoch [01/10] Step [235/506]: cls_loss=0.2587\n",
      "Epoch [01/10] Step [240/506]: cls_loss=0.3530\n",
      "Epoch [01/10] Step [245/506]: cls_loss=0.2608\n",
      "Epoch [01/10] Step [250/506]: cls_loss=0.3174\n",
      "Epoch [01/10] Step [255/506]: cls_loss=0.4593\n",
      "Epoch [01/10] Step [260/506]: cls_loss=0.3759\n",
      "Epoch [01/10] Step [265/506]: cls_loss=0.4655\n",
      "Epoch [01/10] Step [270/506]: cls_loss=0.5146\n",
      "Epoch [01/10] Step [275/506]: cls_loss=0.4804\n",
      "Epoch [01/10] Step [280/506]: cls_loss=0.2047\n",
      "Epoch [01/10] Step [285/506]: cls_loss=0.4930\n",
      "Epoch [01/10] Step [290/506]: cls_loss=0.3899\n",
      "Epoch [01/10] Step [295/506]: cls_loss=0.4208\n",
      "Epoch [01/10] Step [300/506]: cls_loss=0.5547\n",
      "Epoch [01/10] Step [305/506]: cls_loss=0.2222\n",
      "Epoch [01/10] Step [310/506]: cls_loss=0.5292\n",
      "Epoch [01/10] Step [315/506]: cls_loss=0.2439\n",
      "Epoch [01/10] Step [320/506]: cls_loss=0.3492\n",
      "Epoch [01/10] Step [325/506]: cls_loss=0.4139\n",
      "Epoch [01/10] Step [330/506]: cls_loss=0.2331\n",
      "Epoch [01/10] Step [335/506]: cls_loss=0.1868\n",
      "Epoch [01/10] Step [340/506]: cls_loss=0.3016\n",
      "Epoch [01/10] Step [345/506]: cls_loss=0.2387\n",
      "Epoch [01/10] Step [350/506]: cls_loss=0.4241\n",
      "Epoch [01/10] Step [355/506]: cls_loss=0.3960\n",
      "Epoch [01/10] Step [360/506]: cls_loss=0.1361\n",
      "Epoch [01/10] Step [365/506]: cls_loss=0.4471\n",
      "Epoch [01/10] Step [370/506]: cls_loss=0.1637\n",
      "Epoch [01/10] Step [375/506]: cls_loss=0.1939\n",
      "Epoch [01/10] Step [380/506]: cls_loss=0.4190\n",
      "Epoch [01/10] Step [385/506]: cls_loss=0.1808\n",
      "Epoch [01/10] Step [390/506]: cls_loss=0.3500\n",
      "Epoch [01/10] Step [395/506]: cls_loss=0.1122\n",
      "Epoch [01/10] Step [400/506]: cls_loss=0.2258\n",
      "Epoch [01/10] Step [405/506]: cls_loss=0.0542\n",
      "Epoch [01/10] Step [410/506]: cls_loss=0.2666\n",
      "Epoch [01/10] Step [415/506]: cls_loss=0.2752\n",
      "Epoch [01/10] Step [420/506]: cls_loss=0.5031\n",
      "Epoch [01/10] Step [425/506]: cls_loss=0.2945\n",
      "Epoch [01/10] Step [430/506]: cls_loss=0.2919\n",
      "Epoch [01/10] Step [435/506]: cls_loss=0.3951\n",
      "Epoch [01/10] Step [440/506]: cls_loss=0.1652\n",
      "Epoch [01/10] Step [445/506]: cls_loss=0.5170\n",
      "Epoch [01/10] Step [450/506]: cls_loss=0.1357\n",
      "Epoch [01/10] Step [455/506]: cls_loss=0.2096\n",
      "Epoch [01/10] Step [460/506]: cls_loss=0.2509\n",
      "Epoch [01/10] Step [465/506]: cls_loss=0.1915\n",
      "Epoch [01/10] Step [470/506]: cls_loss=0.1688\n",
      "Epoch [01/10] Step [475/506]: cls_loss=0.3719\n",
      "Epoch [01/10] Step [480/506]: cls_loss=0.4979\n",
      "Epoch [01/10] Step [485/506]: cls_loss=0.2016\n",
      "Epoch [01/10] Step [490/506]: cls_loss=0.3710\n",
      "Epoch [01/10] Step [495/506]: cls_loss=0.2701\n",
      "Epoch [01/10] Step [500/506]: cls_loss=0.2143\n",
      "Epoch [01/10] Step [505/506]: cls_loss=0.0058\n",
      "At the end of Epoch: 1\n",
      "Validation loss:  0.427896648645401\n",
      "Accuracy: 0.8340757238307349\n",
      "F1 score (Macro): 0.8256042181849468\n",
      "F1 score (Per class): [0.82681564 0.78833693 0.86166008]\n",
      "Precision score (Per class): [0.90464548 0.71709234 0.8690205 ]\n",
      "Recall score (Per class): [0.76131687 0.87529976 0.85442329]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  1.4250339269638062\n",
      "Accuracy: 0.5414165666266506\n",
      "F1 score (Macro): 0.4892522921876184\n",
      "F1 score (Per class): [0.3375     0.46987952 0.66037736]\n",
      "Precision score (Per class): [0.29189189 0.53424658 0.65268065]\n",
      "Recall score (Per class): [0.4        0.41935484 0.66825776]\n",
      "German Test:\n",
      "Validation loss:  1.1408612728118896\n",
      "Accuracy: 0.5764563106796117\n",
      "F1 score (Macro): 0.5068439074549844\n",
      "F1 score (Per class): [0.34615385 0.4822335  0.69214437]\n",
      "Precision score (Per class): [0.26470588 0.42222222 0.82531646]\n",
      "Recall score (Per class): [0.5        0.56213018 0.59597806]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.5772686004638672\n",
      "Accuracy: 0.46912242686890576\n",
      "F1 score (Macro): 0.4519779113938438\n",
      "F1 score (Per class): [0.36461126 0.49860724 0.49271523]\n",
      "Precision score (Per class): [0.30630631 0.68846154 0.42176871]\n",
      "Recall score (Per class): [0.45033113 0.39082969 0.59235669]\n",
      "Epoch: 2/10\n",
      "Epoch [02/10] Step [000/506]: cls_loss=0.3034\n",
      "Epoch [02/10] Step [005/506]: cls_loss=0.3659\n",
      "Epoch [02/10] Step [010/506]: cls_loss=0.1066\n",
      "Epoch [02/10] Step [015/506]: cls_loss=0.2169\n",
      "Epoch [02/10] Step [020/506]: cls_loss=0.1780\n",
      "Epoch [02/10] Step [025/506]: cls_loss=0.1513\n",
      "Epoch [02/10] Step [030/506]: cls_loss=0.0446\n",
      "Epoch [02/10] Step [035/506]: cls_loss=0.3895\n",
      "Epoch [02/10] Step [040/506]: cls_loss=0.1106\n",
      "Epoch [02/10] Step [045/506]: cls_loss=0.0366\n",
      "Epoch [02/10] Step [050/506]: cls_loss=0.3135\n",
      "Epoch [02/10] Step [055/506]: cls_loss=0.2148\n",
      "Epoch [02/10] Step [060/506]: cls_loss=0.1371\n",
      "Epoch [02/10] Step [065/506]: cls_loss=0.1486\n",
      "Epoch [02/10] Step [070/506]: cls_loss=0.1106\n",
      "Epoch [02/10] Step [075/506]: cls_loss=0.1511\n",
      "Epoch [02/10] Step [080/506]: cls_loss=0.4773\n",
      "Epoch [02/10] Step [085/506]: cls_loss=0.0533\n",
      "Epoch [02/10] Step [090/506]: cls_loss=0.1955\n",
      "Epoch [02/10] Step [095/506]: cls_loss=0.1945\n",
      "Epoch [02/10] Step [100/506]: cls_loss=0.1597\n",
      "Epoch [02/10] Step [105/506]: cls_loss=0.2393\n",
      "Epoch [02/10] Step [110/506]: cls_loss=0.2045\n",
      "Epoch [02/10] Step [115/506]: cls_loss=0.1307\n",
      "Epoch [02/10] Step [120/506]: cls_loss=0.1327\n",
      "Epoch [02/10] Step [125/506]: cls_loss=0.1416\n",
      "Epoch [02/10] Step [130/506]: cls_loss=0.0924\n",
      "Epoch [02/10] Step [135/506]: cls_loss=0.3561\n",
      "Epoch [02/10] Step [140/506]: cls_loss=0.1194\n",
      "Epoch [02/10] Step [145/506]: cls_loss=0.9546\n",
      "Epoch [02/10] Step [150/506]: cls_loss=0.1150\n",
      "Epoch [02/10] Step [155/506]: cls_loss=0.1373\n",
      "Epoch [02/10] Step [160/506]: cls_loss=0.2633\n",
      "Epoch [02/10] Step [165/506]: cls_loss=0.0285\n",
      "Epoch [02/10] Step [170/506]: cls_loss=0.3370\n",
      "Epoch [02/10] Step [175/506]: cls_loss=0.0268\n",
      "Epoch [02/10] Step [180/506]: cls_loss=0.3805\n",
      "Epoch [02/10] Step [185/506]: cls_loss=0.0919\n",
      "Epoch [02/10] Step [190/506]: cls_loss=0.0805\n",
      "Epoch [02/10] Step [195/506]: cls_loss=0.1433\n",
      "Epoch [02/10] Step [200/506]: cls_loss=0.1324\n",
      "Epoch [02/10] Step [205/506]: cls_loss=0.0532\n",
      "Epoch [02/10] Step [210/506]: cls_loss=0.1691\n",
      "Epoch [02/10] Step [215/506]: cls_loss=0.0417\n",
      "Epoch [02/10] Step [220/506]: cls_loss=0.1157\n",
      "Epoch [02/10] Step [225/506]: cls_loss=0.0927\n",
      "Epoch [02/10] Step [230/506]: cls_loss=0.3340\n",
      "Epoch [02/10] Step [235/506]: cls_loss=0.0942\n",
      "Epoch [02/10] Step [240/506]: cls_loss=0.2419\n",
      "Epoch [02/10] Step [245/506]: cls_loss=0.0946\n",
      "Epoch [02/10] Step [250/506]: cls_loss=0.1198\n",
      "Epoch [02/10] Step [255/506]: cls_loss=0.4406\n",
      "Epoch [02/10] Step [260/506]: cls_loss=0.1636\n",
      "Epoch [02/10] Step [265/506]: cls_loss=0.1279\n",
      "Epoch [02/10] Step [270/506]: cls_loss=0.0205\n",
      "Epoch [02/10] Step [275/506]: cls_loss=0.1802\n",
      "Epoch [02/10] Step [280/506]: cls_loss=0.1630\n",
      "Epoch [02/10] Step [285/506]: cls_loss=0.2353\n",
      "Epoch [02/10] Step [290/506]: cls_loss=0.2840\n",
      "Epoch [02/10] Step [295/506]: cls_loss=0.1132\n",
      "Epoch [02/10] Step [300/506]: cls_loss=0.0571\n",
      "Epoch [02/10] Step [305/506]: cls_loss=0.0597\n",
      "Epoch [02/10] Step [310/506]: cls_loss=0.0286\n",
      "Epoch [02/10] Step [315/506]: cls_loss=0.0437\n",
      "Epoch [02/10] Step [320/506]: cls_loss=0.0156\n",
      "Epoch [02/10] Step [325/506]: cls_loss=0.0230\n",
      "Epoch [02/10] Step [330/506]: cls_loss=0.0829\n",
      "Epoch [02/10] Step [335/506]: cls_loss=0.0244\n",
      "Epoch [02/10] Step [340/506]: cls_loss=0.0730\n",
      "Epoch [02/10] Step [345/506]: cls_loss=0.2305\n",
      "Epoch [02/10] Step [350/506]: cls_loss=0.1456\n",
      "Epoch [02/10] Step [355/506]: cls_loss=0.0848\n",
      "Epoch [02/10] Step [360/506]: cls_loss=0.1849\n",
      "Epoch [02/10] Step [365/506]: cls_loss=0.2411\n",
      "Epoch [02/10] Step [370/506]: cls_loss=0.0389\n",
      "Epoch [02/10] Step [375/506]: cls_loss=0.0209\n",
      "Epoch [02/10] Step [380/506]: cls_loss=0.1269\n",
      "Epoch [02/10] Step [385/506]: cls_loss=0.2069\n",
      "Epoch [02/10] Step [390/506]: cls_loss=0.0915\n",
      "Epoch [02/10] Step [395/506]: cls_loss=0.1087\n",
      "Epoch [02/10] Step [400/506]: cls_loss=0.0908\n",
      "Epoch [02/10] Step [405/506]: cls_loss=0.0647\n",
      "Epoch [02/10] Step [410/506]: cls_loss=0.1442\n",
      "Epoch [02/10] Step [415/506]: cls_loss=0.3936\n",
      "Epoch [02/10] Step [420/506]: cls_loss=0.2492\n",
      "Epoch [02/10] Step [425/506]: cls_loss=0.0205\n",
      "Epoch [02/10] Step [430/506]: cls_loss=0.1623\n",
      "Epoch [02/10] Step [435/506]: cls_loss=0.2292\n",
      "Epoch [02/10] Step [440/506]: cls_loss=0.0341\n",
      "Epoch [02/10] Step [445/506]: cls_loss=0.0974\n",
      "Epoch [02/10] Step [450/506]: cls_loss=0.1829\n",
      "Epoch [02/10] Step [455/506]: cls_loss=0.1822\n",
      "Epoch [02/10] Step [460/506]: cls_loss=0.0396\n",
      "Epoch [02/10] Step [465/506]: cls_loss=0.1672\n",
      "Epoch [02/10] Step [470/506]: cls_loss=0.1442\n",
      "Epoch [02/10] Step [475/506]: cls_loss=0.0680\n",
      "Epoch [02/10] Step [480/506]: cls_loss=0.2321\n",
      "Epoch [02/10] Step [485/506]: cls_loss=0.1148\n",
      "Epoch [02/10] Step [490/506]: cls_loss=0.2686\n",
      "Epoch [02/10] Step [495/506]: cls_loss=0.0464\n",
      "Epoch [02/10] Step [500/506]: cls_loss=0.1569\n",
      "Epoch [02/10] Step [505/506]: cls_loss=0.0032\n",
      "At the end of Epoch: 2\n",
      "Validation loss:  0.5917465090751648\n",
      "Accuracy: 0.8051224944320713\n",
      "F1 score (Macro): 0.7890849232187941\n",
      "F1 score (Per class): [0.7364532  0.78443743 0.84636413]\n",
      "Precision score (Per class): [0.91717791 0.69850187 0.82692308]\n",
      "Recall score (Per class): [0.61522634 0.89448441 0.86674132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  1.6949741840362549\n",
      "Accuracy: 0.5150060024009604\n",
      "F1 score (Macro): 0.47015485637882115\n",
      "F1 score (Per class): [0.2829582  0.52881356 0.59869281]\n",
      "Precision score (Per class): [0.25       0.50160772 0.66184971]\n",
      "Recall score (Per class): [0.32592593 0.55913978 0.54653938]\n",
      "German Test:\n",
      "Validation loss:  1.2251203060150146\n",
      "Accuracy: 0.5691747572815534\n",
      "F1 score (Macro): 0.5021985918868412\n",
      "F1 score (Per class): [0.35664336 0.47482014 0.67513228]\n",
      "Precision score (Per class): [0.28651685 0.39919355 0.80150754]\n",
      "Recall score (Per class): [0.47222222 0.58579882 0.58318099]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.7107231616973877\n",
      "Accuracy: 0.5102925243770314\n",
      "F1 score (Macro): 0.47691657592180836\n",
      "F1 score (Per class): [0.37790698 0.61751152 0.43533123]\n",
      "Precision score (Per class): [0.33678756 0.65365854 0.43125   ]\n",
      "Recall score (Per class): [0.43046358 0.58515284 0.43949045]\n",
      "Epoch: 3/10\n",
      "Epoch [03/10] Step [000/506]: cls_loss=0.1806\n",
      "Epoch [03/10] Step [005/506]: cls_loss=0.2070\n",
      "Epoch [03/10] Step [010/506]: cls_loss=0.1248\n",
      "Epoch [03/10] Step [015/506]: cls_loss=0.2275\n",
      "Epoch [03/10] Step [020/506]: cls_loss=0.1726\n",
      "Epoch [03/10] Step [025/506]: cls_loss=0.1958\n",
      "Epoch [03/10] Step [030/506]: cls_loss=0.0950\n",
      "Epoch [03/10] Step [035/506]: cls_loss=0.2109\n",
      "Epoch [03/10] Step [040/506]: cls_loss=0.0337\n",
      "Epoch [03/10] Step [045/506]: cls_loss=0.1710\n",
      "Epoch [03/10] Step [050/506]: cls_loss=0.0572\n",
      "Epoch [03/10] Step [055/506]: cls_loss=0.0542\n",
      "Epoch [03/10] Step [060/506]: cls_loss=0.1012\n",
      "Epoch [03/10] Step [065/506]: cls_loss=0.0181\n",
      "Epoch [03/10] Step [070/506]: cls_loss=0.0179\n",
      "Epoch [03/10] Step [075/506]: cls_loss=0.0418\n",
      "Epoch [03/10] Step [080/506]: cls_loss=0.3937\n",
      "Epoch [03/10] Step [085/506]: cls_loss=0.3753\n",
      "Epoch [03/10] Step [090/506]: cls_loss=0.1280\n",
      "Epoch [03/10] Step [095/506]: cls_loss=0.1366\n",
      "Epoch [03/10] Step [100/506]: cls_loss=0.0892\n",
      "Epoch [03/10] Step [105/506]: cls_loss=0.0217\n",
      "Epoch [03/10] Step [110/506]: cls_loss=0.1603\n",
      "Epoch [03/10] Step [115/506]: cls_loss=0.0308\n",
      "Epoch [03/10] Step [120/506]: cls_loss=0.0452\n",
      "Epoch [03/10] Step [125/506]: cls_loss=0.0298\n",
      "Epoch [03/10] Step [130/506]: cls_loss=0.0422\n",
      "Epoch [03/10] Step [135/506]: cls_loss=0.0900\n",
      "Epoch [03/10] Step [140/506]: cls_loss=0.0321\n",
      "Epoch [03/10] Step [145/506]: cls_loss=0.1181\n",
      "Epoch [03/10] Step [150/506]: cls_loss=0.0295\n",
      "Epoch [03/10] Step [155/506]: cls_loss=0.0937\n",
      "Epoch [03/10] Step [160/506]: cls_loss=0.0108\n",
      "Epoch [03/10] Step [165/506]: cls_loss=0.0742\n",
      "Epoch [03/10] Step [170/506]: cls_loss=0.2299\n",
      "Epoch [03/10] Step [175/506]: cls_loss=0.1784\n",
      "Epoch [03/10] Step [180/506]: cls_loss=0.1082\n",
      "Epoch [03/10] Step [185/506]: cls_loss=0.0590\n",
      "Epoch [03/10] Step [190/506]: cls_loss=0.0673\n",
      "Epoch [03/10] Step [195/506]: cls_loss=0.1399\n",
      "Epoch [03/10] Step [200/506]: cls_loss=0.0188\n",
      "Epoch [03/10] Step [205/506]: cls_loss=0.0470\n",
      "Epoch [03/10] Step [210/506]: cls_loss=0.0841\n",
      "Epoch [03/10] Step [215/506]: cls_loss=0.0563\n",
      "Epoch [03/10] Step [220/506]: cls_loss=0.0772\n",
      "Epoch [03/10] Step [225/506]: cls_loss=0.0441\n",
      "Epoch [03/10] Step [230/506]: cls_loss=0.1407\n",
      "Epoch [03/10] Step [235/506]: cls_loss=0.0213\n",
      "Epoch [03/10] Step [240/506]: cls_loss=0.0373\n",
      "Epoch [03/10] Step [245/506]: cls_loss=0.0383\n",
      "Epoch [03/10] Step [250/506]: cls_loss=0.1037\n",
      "Epoch [03/10] Step [255/506]: cls_loss=0.0155\n",
      "Epoch [03/10] Step [260/506]: cls_loss=0.2309\n",
      "Epoch [03/10] Step [265/506]: cls_loss=0.0292\n",
      "Epoch [03/10] Step [270/506]: cls_loss=0.2325\n",
      "Epoch [03/10] Step [275/506]: cls_loss=0.2413\n",
      "Epoch [03/10] Step [280/506]: cls_loss=0.0688\n",
      "Epoch [03/10] Step [285/506]: cls_loss=0.0766\n",
      "Epoch [03/10] Step [290/506]: cls_loss=0.0668\n",
      "Epoch [03/10] Step [295/506]: cls_loss=0.0456\n",
      "Epoch [03/10] Step [300/506]: cls_loss=0.1152\n",
      "Epoch [03/10] Step [305/506]: cls_loss=0.0525\n",
      "Epoch [03/10] Step [310/506]: cls_loss=0.0951\n",
      "Epoch [03/10] Step [315/506]: cls_loss=0.0291\n",
      "Epoch [03/10] Step [320/506]: cls_loss=0.0206\n",
      "Epoch [03/10] Step [325/506]: cls_loss=0.0281\n",
      "Epoch [03/10] Step [330/506]: cls_loss=0.0132\n",
      "Epoch [03/10] Step [335/506]: cls_loss=0.1530\n",
      "Epoch [03/10] Step [340/506]: cls_loss=0.0135\n",
      "Epoch [03/10] Step [345/506]: cls_loss=0.0758\n",
      "Epoch [03/10] Step [350/506]: cls_loss=0.0238\n",
      "Epoch [03/10] Step [355/506]: cls_loss=0.1377\n",
      "Epoch [03/10] Step [360/506]: cls_loss=0.0260\n",
      "Epoch [03/10] Step [365/506]: cls_loss=0.0702\n",
      "Epoch [03/10] Step [370/506]: cls_loss=0.0101\n",
      "Epoch [03/10] Step [375/506]: cls_loss=0.0737\n",
      "Epoch [03/10] Step [380/506]: cls_loss=0.0496\n",
      "Epoch [03/10] Step [385/506]: cls_loss=0.0608\n",
      "Epoch [03/10] Step [390/506]: cls_loss=0.0173\n",
      "Epoch [03/10] Step [395/506]: cls_loss=0.0658\n",
      "Epoch [03/10] Step [400/506]: cls_loss=0.0157\n",
      "Epoch [03/10] Step [405/506]: cls_loss=0.0489\n",
      "Epoch [03/10] Step [410/506]: cls_loss=0.0052\n",
      "Epoch [03/10] Step [415/506]: cls_loss=0.0900\n",
      "Epoch [03/10] Step [420/506]: cls_loss=0.1944\n",
      "Epoch [03/10] Step [425/506]: cls_loss=0.0138\n",
      "Epoch [03/10] Step [430/506]: cls_loss=0.0769\n",
      "Epoch [03/10] Step [435/506]: cls_loss=0.0506\n",
      "Epoch [03/10] Step [440/506]: cls_loss=0.1228\n",
      "Epoch [03/10] Step [445/506]: cls_loss=0.0601\n",
      "Epoch [03/10] Step [450/506]: cls_loss=0.0543\n",
      "Epoch [03/10] Step [455/506]: cls_loss=0.0463\n",
      "Epoch [03/10] Step [460/506]: cls_loss=0.1175\n",
      "Epoch [03/10] Step [465/506]: cls_loss=0.2971\n",
      "Epoch [03/10] Step [470/506]: cls_loss=0.2094\n",
      "Epoch [03/10] Step [475/506]: cls_loss=0.3744\n",
      "Epoch [03/10] Step [480/506]: cls_loss=0.0227\n",
      "Epoch [03/10] Step [485/506]: cls_loss=0.0493\n",
      "Epoch [03/10] Step [490/506]: cls_loss=0.1150\n",
      "Epoch [03/10] Step [495/506]: cls_loss=0.0583\n",
      "Epoch [03/10] Step [500/506]: cls_loss=0.1631\n",
      "Epoch [03/10] Step [505/506]: cls_loss=0.0010\n",
      "At the end of Epoch: 3\n",
      "Validation loss:  0.4223026633262634\n",
      "Accuracy: 0.8758351893095768\n",
      "F1 score (Macro): 0.8657425800141842\n",
      "F1 score (Per class): [0.83737024 0.85885167 0.90100582]\n",
      "Precision score (Per class): [0.95275591 0.85680191 0.85441767]\n",
      "Recall score (Per class): [0.74691358 0.86091127 0.95296753]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  1.7718008756637573\n",
      "Accuracy: 0.5366146458583433\n",
      "F1 score (Macro): 0.4821961622791135\n",
      "F1 score (Per class): [0.30872483 0.49710983 0.64075383]\n",
      "Precision score (Per class): [0.28220859 0.5375     0.63255814]\n",
      "Recall score (Per class): [0.34074074 0.46236559 0.64916468]\n",
      "German Test:\n",
      "Validation loss:  1.2511718273162842\n",
      "Accuracy: 0.6104368932038835\n",
      "F1 score (Macro): 0.5161901151742578\n",
      "F1 score (Per class): [0.31944444 0.4957265  0.73339941]\n",
      "Precision score (Per class): [0.25555556 0.47802198 0.8008658 ]\n",
      "Recall score (Per class): [0.42592593 0.5147929  0.67641682]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  2.0755879878997803\n",
      "Accuracy: 0.48537378114842905\n",
      "F1 score (Macro): 0.4548150075702762\n",
      "F1 score (Per class): [0.33116883 0.52400549 0.5092707 ]\n",
      "Precision score (Per class): [0.32484076 0.70479705 0.41616162]\n",
      "Recall score (Per class): [0.33774834 0.41703057 0.65605096]\n",
      "Epoch: 4/10\n",
      "Epoch [04/10] Step [000/506]: cls_loss=0.1482\n",
      "Epoch [04/10] Step [005/506]: cls_loss=0.0396\n",
      "Epoch [04/10] Step [010/506]: cls_loss=0.0390\n",
      "Epoch [04/10] Step [015/506]: cls_loss=0.0377\n",
      "Epoch [04/10] Step [020/506]: cls_loss=0.0080\n",
      "Epoch [04/10] Step [025/506]: cls_loss=0.0137\n",
      "Epoch [04/10] Step [030/506]: cls_loss=0.1613\n",
      "Epoch [04/10] Step [035/506]: cls_loss=0.0316\n",
      "Epoch [04/10] Step [040/506]: cls_loss=0.0442\n",
      "Epoch [04/10] Step [045/506]: cls_loss=0.0105\n",
      "Epoch [04/10] Step [050/506]: cls_loss=0.0106\n",
      "Epoch [04/10] Step [055/506]: cls_loss=0.0054\n",
      "Epoch [04/10] Step [060/506]: cls_loss=0.0054\n",
      "Epoch [04/10] Step [065/506]: cls_loss=0.0046\n",
      "Epoch [04/10] Step [070/506]: cls_loss=0.0074\n",
      "Epoch [04/10] Step [075/506]: cls_loss=0.0223\n",
      "Epoch [04/10] Step [080/506]: cls_loss=0.0231\n",
      "Epoch [04/10] Step [085/506]: cls_loss=0.0049\n",
      "Epoch [04/10] Step [090/506]: cls_loss=0.1328\n",
      "Epoch [04/10] Step [095/506]: cls_loss=0.0260\n",
      "Epoch [04/10] Step [100/506]: cls_loss=0.1176\n",
      "Epoch [04/10] Step [105/506]: cls_loss=0.0872\n",
      "Epoch [04/10] Step [110/506]: cls_loss=0.0542\n",
      "Epoch [04/10] Step [115/506]: cls_loss=0.0386\n",
      "Epoch [04/10] Step [120/506]: cls_loss=0.1134\n",
      "Epoch [04/10] Step [125/506]: cls_loss=0.0390\n",
      "Epoch [04/10] Step [130/506]: cls_loss=0.0294\n",
      "Epoch [04/10] Step [135/506]: cls_loss=0.0312\n",
      "Epoch [04/10] Step [140/506]: cls_loss=0.1968\n",
      "Epoch [04/10] Step [145/506]: cls_loss=0.0871\n",
      "Epoch [04/10] Step [150/506]: cls_loss=0.0275\n",
      "Epoch [04/10] Step [155/506]: cls_loss=0.0760\n",
      "Epoch [04/10] Step [160/506]: cls_loss=0.0881\n",
      "Epoch [04/10] Step [165/506]: cls_loss=0.0246\n",
      "Epoch [04/10] Step [170/506]: cls_loss=0.0327\n",
      "Epoch [04/10] Step [175/506]: cls_loss=0.0306\n",
      "Epoch [04/10] Step [180/506]: cls_loss=0.0238\n",
      "Epoch [04/10] Step [185/506]: cls_loss=0.0079\n",
      "Epoch [04/10] Step [190/506]: cls_loss=0.0503\n",
      "Epoch [04/10] Step [195/506]: cls_loss=0.0425\n",
      "Epoch [04/10] Step [200/506]: cls_loss=0.2271\n",
      "Epoch [04/10] Step [205/506]: cls_loss=0.0574\n",
      "Epoch [04/10] Step [210/506]: cls_loss=0.0593\n",
      "Epoch [04/10] Step [215/506]: cls_loss=0.0551\n",
      "Epoch [04/10] Step [220/506]: cls_loss=0.1148\n",
      "Epoch [04/10] Step [225/506]: cls_loss=0.1560\n",
      "Epoch [04/10] Step [230/506]: cls_loss=0.1244\n",
      "Epoch [04/10] Step [235/506]: cls_loss=0.1563\n",
      "Epoch [04/10] Step [240/506]: cls_loss=0.0878\n",
      "Epoch [04/10] Step [245/506]: cls_loss=0.0098\n",
      "Epoch [04/10] Step [250/506]: cls_loss=0.0683\n",
      "Epoch [04/10] Step [255/506]: cls_loss=0.0296\n",
      "Epoch [04/10] Step [260/506]: cls_loss=0.0166\n",
      "Epoch [04/10] Step [265/506]: cls_loss=0.0454\n",
      "Epoch [04/10] Step [270/506]: cls_loss=0.0272\n",
      "Epoch [04/10] Step [275/506]: cls_loss=0.0389\n",
      "Epoch [04/10] Step [280/506]: cls_loss=0.0012\n",
      "Epoch [04/10] Step [285/506]: cls_loss=0.0022\n",
      "Epoch [04/10] Step [290/506]: cls_loss=0.0027\n",
      "Epoch [04/10] Step [295/506]: cls_loss=0.0800\n",
      "Epoch [04/10] Step [300/506]: cls_loss=0.0098\n",
      "Epoch [04/10] Step [305/506]: cls_loss=0.0020\n",
      "Epoch [04/10] Step [310/506]: cls_loss=0.0631\n",
      "Epoch [04/10] Step [315/506]: cls_loss=0.0010\n",
      "Epoch [04/10] Step [320/506]: cls_loss=0.1968\n",
      "Epoch [04/10] Step [325/506]: cls_loss=0.0600\n",
      "Epoch [04/10] Step [330/506]: cls_loss=0.0142\n",
      "Epoch [04/10] Step [335/506]: cls_loss=0.0104\n",
      "Epoch [04/10] Step [340/506]: cls_loss=0.0127\n",
      "Epoch [04/10] Step [345/506]: cls_loss=0.2073\n",
      "Epoch [04/10] Step [350/506]: cls_loss=0.0154\n",
      "Epoch [04/10] Step [355/506]: cls_loss=0.2169\n",
      "Epoch [04/10] Step [360/506]: cls_loss=0.0188\n",
      "Epoch [04/10] Step [365/506]: cls_loss=0.1634\n",
      "Epoch [04/10] Step [370/506]: cls_loss=0.0484\n",
      "Epoch [04/10] Step [375/506]: cls_loss=0.0661\n",
      "Epoch [04/10] Step [380/506]: cls_loss=0.0382\n",
      "Epoch [04/10] Step [385/506]: cls_loss=0.0140\n",
      "Epoch [04/10] Step [390/506]: cls_loss=0.0857\n",
      "Epoch [04/10] Step [395/506]: cls_loss=0.0492\n",
      "Epoch [04/10] Step [400/506]: cls_loss=0.0053\n",
      "Epoch [04/10] Step [405/506]: cls_loss=0.0385\n",
      "Epoch [04/10] Step [410/506]: cls_loss=0.0248\n",
      "Epoch [04/10] Step [415/506]: cls_loss=0.0224\n",
      "Epoch [04/10] Step [420/506]: cls_loss=0.1072\n",
      "Epoch [04/10] Step [425/506]: cls_loss=0.0018\n",
      "Epoch [04/10] Step [430/506]: cls_loss=0.0038\n",
      "Epoch [04/10] Step [435/506]: cls_loss=0.1038\n",
      "Epoch [04/10] Step [440/506]: cls_loss=0.0535\n",
      "Epoch [04/10] Step [445/506]: cls_loss=0.0521\n",
      "Epoch [04/10] Step [450/506]: cls_loss=0.0168\n",
      "Epoch [04/10] Step [455/506]: cls_loss=0.1989\n",
      "Epoch [04/10] Step [460/506]: cls_loss=0.0164\n",
      "Epoch [04/10] Step [465/506]: cls_loss=0.0696\n",
      "Epoch [04/10] Step [470/506]: cls_loss=0.1464\n",
      "Epoch [04/10] Step [475/506]: cls_loss=0.0327\n",
      "Epoch [04/10] Step [480/506]: cls_loss=0.0372\n",
      "Epoch [04/10] Step [485/506]: cls_loss=0.0267\n",
      "Epoch [04/10] Step [490/506]: cls_loss=0.0158\n",
      "Epoch [04/10] Step [495/506]: cls_loss=0.0274\n",
      "Epoch [04/10] Step [500/506]: cls_loss=0.1674\n",
      "Epoch [04/10] Step [505/506]: cls_loss=0.0029\n",
      "At the end of Epoch: 4\n",
      "Validation loss:  0.4588228762149811\n",
      "Accuracy: 0.8402004454342984\n",
      "F1 score (Macro): 0.8316866044862681\n",
      "F1 score (Per class): [0.83256881 0.79022403 0.87226697]\n",
      "Precision score (Per class): [0.94041451 0.68672566 0.89704142]\n",
      "Recall score (Per class): [0.74691358 0.93045564 0.84882419]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  1.8419431447982788\n",
      "Accuracy: 0.5294117647058824\n",
      "F1 score (Macro): 0.4959863088299669\n",
      "F1 score (Per class): [0.33742331 0.56230032 0.58823529]\n",
      "Precision score (Per class): [0.28795812 0.50720461 0.71186441]\n",
      "Recall score (Per class): [0.40740741 0.63082437 0.50119332]\n",
      "German Test:\n",
      "Validation loss:  1.4090386629104614\n",
      "Accuracy: 0.5934466019417476\n",
      "F1 score (Macro): 0.5104546800975859\n",
      "F1 score (Per class): [0.33201581 0.49187935 0.70746888]\n",
      "Precision score (Per class): [0.28965517 0.40458015 0.8177458 ]\n",
      "Recall score (Per class): [0.38888889 0.62721893 0.62340037]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.7900627851486206\n",
      "Accuracy: 0.5222101841820151\n",
      "F1 score (Macro): 0.4761883359197682\n",
      "F1 score (Per class): [0.34650456 0.64088398 0.44117647]\n",
      "Precision score (Per class): [0.32022472 0.64876957 0.45302013]\n",
      "Recall score (Per class): [0.37748344 0.63318777 0.42993631]\n",
      "Epoch: 5/10\n",
      "Epoch [05/10] Step [000/506]: cls_loss=0.1399\n",
      "Epoch [05/10] Step [005/506]: cls_loss=0.0783\n",
      "Epoch [05/10] Step [010/506]: cls_loss=0.0473\n",
      "Epoch [05/10] Step [015/506]: cls_loss=0.0258\n",
      "Epoch [05/10] Step [020/506]: cls_loss=0.0068\n",
      "Epoch [05/10] Step [025/506]: cls_loss=0.0029\n",
      "Epoch [05/10] Step [030/506]: cls_loss=0.0102\n",
      "Epoch [05/10] Step [035/506]: cls_loss=0.0038\n",
      "Epoch [05/10] Step [040/506]: cls_loss=0.0082\n",
      "Epoch [05/10] Step [045/506]: cls_loss=0.0038\n",
      "Epoch [05/10] Step [050/506]: cls_loss=0.0015\n",
      "Epoch [05/10] Step [055/506]: cls_loss=0.0021\n",
      "Epoch [05/10] Step [060/506]: cls_loss=0.0131\n",
      "Epoch [05/10] Step [065/506]: cls_loss=0.0007\n",
      "Epoch [05/10] Step [070/506]: cls_loss=0.0045\n",
      "Epoch [05/10] Step [075/506]: cls_loss=0.0044\n",
      "Epoch [05/10] Step [080/506]: cls_loss=0.0130\n",
      "Epoch [05/10] Step [085/506]: cls_loss=0.0076\n",
      "Epoch [05/10] Step [090/506]: cls_loss=0.2095\n",
      "Epoch [05/10] Step [095/506]: cls_loss=0.0021\n",
      "Epoch [05/10] Step [100/506]: cls_loss=0.2440\n",
      "Epoch [05/10] Step [105/506]: cls_loss=0.0134\n",
      "Epoch [05/10] Step [110/506]: cls_loss=0.0770\n",
      "Epoch [05/10] Step [115/506]: cls_loss=0.0112\n",
      "Epoch [05/10] Step [120/506]: cls_loss=0.0103\n",
      "Epoch [05/10] Step [125/506]: cls_loss=0.0457\n",
      "Epoch [05/10] Step [130/506]: cls_loss=0.0025\n",
      "Epoch [05/10] Step [135/506]: cls_loss=0.0865\n",
      "Epoch [05/10] Step [140/506]: cls_loss=0.0339\n",
      "Epoch [05/10] Step [145/506]: cls_loss=0.0639\n",
      "Epoch [05/10] Step [150/506]: cls_loss=0.0051\n",
      "Epoch [05/10] Step [155/506]: cls_loss=0.0357\n",
      "Epoch [05/10] Step [160/506]: cls_loss=0.0284\n",
      "Epoch [05/10] Step [165/506]: cls_loss=0.0774\n",
      "Epoch [05/10] Step [170/506]: cls_loss=0.0215\n",
      "Epoch [05/10] Step [175/506]: cls_loss=0.0029\n",
      "Epoch [05/10] Step [180/506]: cls_loss=0.0215\n",
      "Epoch [05/10] Step [185/506]: cls_loss=0.0058\n",
      "Epoch [05/10] Step [190/506]: cls_loss=0.0020\n",
      "Epoch [05/10] Step [195/506]: cls_loss=0.0180\n",
      "Epoch [05/10] Step [200/506]: cls_loss=0.0610\n",
      "Epoch [05/10] Step [205/506]: cls_loss=0.0006\n",
      "Epoch [05/10] Step [210/506]: cls_loss=0.0008\n",
      "Epoch [05/10] Step [215/506]: cls_loss=0.0050\n",
      "Epoch [05/10] Step [220/506]: cls_loss=0.0205\n",
      "Epoch [05/10] Step [225/506]: cls_loss=0.0044\n",
      "Epoch [05/10] Step [230/506]: cls_loss=0.0159\n",
      "Epoch [05/10] Step [235/506]: cls_loss=0.0022\n",
      "Epoch [05/10] Step [240/506]: cls_loss=0.0436\n",
      "Epoch [05/10] Step [245/506]: cls_loss=0.0008\n",
      "Epoch [05/10] Step [250/506]: cls_loss=0.0026\n",
      "Epoch [05/10] Step [255/506]: cls_loss=0.1916\n",
      "Epoch [05/10] Step [260/506]: cls_loss=0.0133\n",
      "Epoch [05/10] Step [265/506]: cls_loss=0.0396\n",
      "Epoch [05/10] Step [270/506]: cls_loss=0.0009\n",
      "Epoch [05/10] Step [275/506]: cls_loss=0.0098\n",
      "Epoch [05/10] Step [280/506]: cls_loss=0.0622\n",
      "Epoch [05/10] Step [285/506]: cls_loss=0.0064\n",
      "Epoch [05/10] Step [290/506]: cls_loss=0.0775\n",
      "Epoch [05/10] Step [295/506]: cls_loss=0.0202\n",
      "Epoch [05/10] Step [300/506]: cls_loss=0.1734\n",
      "Epoch [05/10] Step [305/506]: cls_loss=0.0271\n",
      "Epoch [05/10] Step [310/506]: cls_loss=0.0384\n",
      "Epoch [05/10] Step [315/506]: cls_loss=0.0025\n",
      "Epoch [05/10] Step [320/506]: cls_loss=0.0083\n",
      "Epoch [05/10] Step [325/506]: cls_loss=0.0282\n",
      "Epoch [05/10] Step [330/506]: cls_loss=0.0033\n",
      "Epoch [05/10] Step [335/506]: cls_loss=0.1016\n",
      "Epoch [05/10] Step [340/506]: cls_loss=0.0039\n",
      "Epoch [05/10] Step [345/506]: cls_loss=0.0008\n",
      "Epoch [05/10] Step [350/506]: cls_loss=0.0432\n",
      "Epoch [05/10] Step [355/506]: cls_loss=0.2051\n",
      "Epoch [05/10] Step [360/506]: cls_loss=0.0392\n",
      "Epoch [05/10] Step [365/506]: cls_loss=0.0228\n",
      "Epoch [05/10] Step [370/506]: cls_loss=0.0003\n",
      "Epoch [05/10] Step [375/506]: cls_loss=0.0010\n",
      "Epoch [05/10] Step [380/506]: cls_loss=0.0522\n",
      "Epoch [05/10] Step [385/506]: cls_loss=0.0006\n",
      "Epoch [05/10] Step [390/506]: cls_loss=0.1213\n",
      "Epoch [05/10] Step [395/506]: cls_loss=0.0235\n",
      "Epoch [05/10] Step [400/506]: cls_loss=0.0394\n",
      "Epoch [05/10] Step [405/506]: cls_loss=0.0024\n",
      "Epoch [05/10] Step [410/506]: cls_loss=0.0119\n",
      "Epoch [05/10] Step [415/506]: cls_loss=0.0211\n",
      "Epoch [05/10] Step [420/506]: cls_loss=0.0990\n",
      "Epoch [05/10] Step [425/506]: cls_loss=0.0015\n",
      "Epoch [05/10] Step [430/506]: cls_loss=0.0059\n",
      "Epoch [05/10] Step [435/506]: cls_loss=0.0051\n",
      "Epoch [05/10] Step [440/506]: cls_loss=0.0017\n",
      "Epoch [05/10] Step [445/506]: cls_loss=0.0012\n",
      "Epoch [05/10] Step [450/506]: cls_loss=0.0259\n",
      "Epoch [05/10] Step [455/506]: cls_loss=0.0638\n",
      "Epoch [05/10] Step [460/506]: cls_loss=0.0441\n",
      "Epoch [05/10] Step [465/506]: cls_loss=0.0017\n",
      "Epoch [05/10] Step [470/506]: cls_loss=0.0076\n",
      "Epoch [05/10] Step [475/506]: cls_loss=0.1553\n",
      "Epoch [05/10] Step [480/506]: cls_loss=0.0866\n",
      "Epoch [05/10] Step [485/506]: cls_loss=0.0006\n",
      "Epoch [05/10] Step [490/506]: cls_loss=0.0609\n",
      "Epoch [05/10] Step [495/506]: cls_loss=0.0966\n",
      "Epoch [05/10] Step [500/506]: cls_loss=0.1484\n",
      "Epoch [05/10] Step [505/506]: cls_loss=0.0316\n",
      "At the end of Epoch: 5\n",
      "Validation loss:  0.5823697447776794\n",
      "Accuracy: 0.821826280623608\n",
      "F1 score (Macro): 0.8166952295879675\n",
      "F1 score (Per class): [0.84030011 0.75722543 0.85256015]\n",
      "Precision score (Per class): [0.87695749 0.63285024 0.94917582]\n",
      "Recall score (Per class): [0.80658436 0.94244604 0.77379619]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  2.495729446411133\n",
      "Accuracy: 0.48619447779111646\n",
      "F1 score (Macro): 0.4566990796878401\n",
      "F1 score (Per class): [0.3125     0.57746479 0.48013245]\n",
      "Precision score (Per class): [0.25345622 0.47563805 0.78378378]\n",
      "Recall score (Per class): [0.40740741 0.73476703 0.34606205]\n",
      "German Test:\n",
      "Validation loss:  2.2633652687072754\n",
      "Accuracy: 0.5024271844660194\n",
      "F1 score (Macro): 0.46075182949387755\n",
      "F1 score (Per class): [0.3081761  0.48627451 0.58780488]\n",
      "Precision score (Per class): [0.23333333 0.36363636 0.88278388]\n",
      "Recall score (Per class): [0.4537037  0.73372781 0.44058501]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  2.3215184211730957\n",
      "Accuracy: 0.5037919826652221\n",
      "F1 score (Macro): 0.4611306805642205\n",
      "F1 score (Per class): [0.40198511 0.63577586 0.34563107]\n",
      "Precision score (Per class): [0.32142857 0.62765957 0.44278607]\n",
      "Recall score (Per class): [0.53642384 0.6441048  0.28343949]\n",
      "Epoch: 6/10\n",
      "Epoch [06/10] Step [000/506]: cls_loss=0.0223\n",
      "Epoch [06/10] Step [005/506]: cls_loss=0.0788\n",
      "Epoch [06/10] Step [010/506]: cls_loss=0.0153\n",
      "Epoch [06/10] Step [015/506]: cls_loss=0.0132\n",
      "Epoch [06/10] Step [020/506]: cls_loss=0.0409\n",
      "Epoch [06/10] Step [025/506]: cls_loss=0.0986\n",
      "Epoch [06/10] Step [030/506]: cls_loss=0.0351\n",
      "Epoch [06/10] Step [035/506]: cls_loss=0.0060\n",
      "Epoch [06/10] Step [040/506]: cls_loss=0.0096\n",
      "Epoch [06/10] Step [045/506]: cls_loss=0.0097\n",
      "Epoch [06/10] Step [050/506]: cls_loss=0.0198\n",
      "Epoch [06/10] Step [055/506]: cls_loss=0.0059\n",
      "Epoch [06/10] Step [060/506]: cls_loss=0.0210\n",
      "Epoch [06/10] Step [065/506]: cls_loss=0.0041\n",
      "Epoch [06/10] Step [070/506]: cls_loss=0.0471\n",
      "Epoch [06/10] Step [075/506]: cls_loss=0.0312\n",
      "Epoch [06/10] Step [080/506]: cls_loss=0.0077\n",
      "Epoch [06/10] Step [085/506]: cls_loss=0.0864\n",
      "Epoch [06/10] Step [090/506]: cls_loss=0.0261\n",
      "Epoch [06/10] Step [095/506]: cls_loss=0.1579\n",
      "Epoch [06/10] Step [100/506]: cls_loss=0.2104\n",
      "Epoch [06/10] Step [105/506]: cls_loss=0.0134\n",
      "Epoch [06/10] Step [110/506]: cls_loss=0.0825\n",
      "Epoch [06/10] Step [115/506]: cls_loss=0.0015\n",
      "Epoch [06/10] Step [120/506]: cls_loss=0.0103\n",
      "Epoch [06/10] Step [125/506]: cls_loss=0.0309\n",
      "Epoch [06/10] Step [130/506]: cls_loss=0.0577\n",
      "Epoch [06/10] Step [135/506]: cls_loss=0.0295\n",
      "Epoch [06/10] Step [140/506]: cls_loss=0.0385\n",
      "Epoch [06/10] Step [145/506]: cls_loss=0.0985\n",
      "Epoch [06/10] Step [150/506]: cls_loss=0.0386\n",
      "Epoch [06/10] Step [155/506]: cls_loss=0.0732\n",
      "Epoch [06/10] Step [160/506]: cls_loss=0.1137\n",
      "Epoch [06/10] Step [165/506]: cls_loss=0.0019\n",
      "Epoch [06/10] Step [170/506]: cls_loss=0.0646\n",
      "Epoch [06/10] Step [175/506]: cls_loss=0.0247\n",
      "Epoch [06/10] Step [180/506]: cls_loss=0.0818\n",
      "Epoch [06/10] Step [185/506]: cls_loss=0.0130\n",
      "Epoch [06/10] Step [190/506]: cls_loss=0.0282\n",
      "Epoch [06/10] Step [195/506]: cls_loss=0.0008\n",
      "Epoch [06/10] Step [200/506]: cls_loss=0.1033\n",
      "Epoch [06/10] Step [205/506]: cls_loss=0.0068\n",
      "Epoch [06/10] Step [210/506]: cls_loss=0.0017\n",
      "Epoch [06/10] Step [215/506]: cls_loss=0.0021\n",
      "Epoch [06/10] Step [220/506]: cls_loss=0.0409\n",
      "Epoch [06/10] Step [225/506]: cls_loss=0.0327\n",
      "Epoch [06/10] Step [230/506]: cls_loss=0.0297\n",
      "Epoch [06/10] Step [235/506]: cls_loss=0.0877\n",
      "Epoch [06/10] Step [240/506]: cls_loss=0.0033\n",
      "Epoch [06/10] Step [245/506]: cls_loss=0.0088\n",
      "Epoch [06/10] Step [250/506]: cls_loss=0.0430\n",
      "Epoch [06/10] Step [255/506]: cls_loss=0.0042\n",
      "Epoch [06/10] Step [260/506]: cls_loss=0.0101\n",
      "Epoch [06/10] Step [265/506]: cls_loss=0.0837\n",
      "Epoch [06/10] Step [270/506]: cls_loss=0.0298\n",
      "Epoch [06/10] Step [275/506]: cls_loss=0.0014\n",
      "Epoch [06/10] Step [280/506]: cls_loss=0.0026\n",
      "Epoch [06/10] Step [285/506]: cls_loss=0.0659\n",
      "Epoch [06/10] Step [290/506]: cls_loss=0.0453\n",
      "Epoch [06/10] Step [295/506]: cls_loss=0.0081\n",
      "Epoch [06/10] Step [300/506]: cls_loss=0.0088\n",
      "Epoch [06/10] Step [305/506]: cls_loss=0.1990\n",
      "Epoch [06/10] Step [310/506]: cls_loss=0.0998\n",
      "Epoch [06/10] Step [315/506]: cls_loss=0.0127\n",
      "Epoch [06/10] Step [320/506]: cls_loss=0.0475\n",
      "Epoch [06/10] Step [325/506]: cls_loss=0.2679\n",
      "Epoch [06/10] Step [330/506]: cls_loss=0.0229\n",
      "Epoch [06/10] Step [335/506]: cls_loss=0.0182\n",
      "Epoch [06/10] Step [340/506]: cls_loss=0.0030\n",
      "Epoch [06/10] Step [345/506]: cls_loss=0.0905\n",
      "Epoch [06/10] Step [350/506]: cls_loss=0.0576\n",
      "Epoch [06/10] Step [355/506]: cls_loss=0.0547\n",
      "Epoch [06/10] Step [360/506]: cls_loss=0.0960\n",
      "Epoch [06/10] Step [365/506]: cls_loss=0.0807\n",
      "Epoch [06/10] Step [370/506]: cls_loss=0.0135\n",
      "Epoch [06/10] Step [375/506]: cls_loss=0.0092\n",
      "Epoch [06/10] Step [380/506]: cls_loss=0.0816\n",
      "Epoch [06/10] Step [385/506]: cls_loss=0.0055\n",
      "Epoch [06/10] Step [390/506]: cls_loss=0.1412\n",
      "Epoch [06/10] Step [395/506]: cls_loss=0.0018\n",
      "Epoch [06/10] Step [400/506]: cls_loss=0.0103\n",
      "Epoch [06/10] Step [405/506]: cls_loss=0.0381\n",
      "Epoch [06/10] Step [410/506]: cls_loss=0.0306\n",
      "Epoch [06/10] Step [415/506]: cls_loss=0.0512\n",
      "Epoch [06/10] Step [420/506]: cls_loss=0.0504\n",
      "Epoch [06/10] Step [425/506]: cls_loss=0.0090\n",
      "Epoch [06/10] Step [430/506]: cls_loss=0.0719\n",
      "Epoch [06/10] Step [435/506]: cls_loss=0.0334\n",
      "Epoch [06/10] Step [440/506]: cls_loss=0.0802\n",
      "Epoch [06/10] Step [445/506]: cls_loss=0.0594\n",
      "Epoch [06/10] Step [450/506]: cls_loss=0.0178\n",
      "Epoch [06/10] Step [455/506]: cls_loss=0.0309\n",
      "Epoch [06/10] Step [460/506]: cls_loss=0.4735\n",
      "Epoch [06/10] Step [465/506]: cls_loss=0.0014\n",
      "Epoch [06/10] Step [470/506]: cls_loss=0.0136\n",
      "Epoch [06/10] Step [475/506]: cls_loss=0.0137\n",
      "Epoch [06/10] Step [480/506]: cls_loss=0.0057\n",
      "Epoch [06/10] Step [485/506]: cls_loss=0.0362\n",
      "Epoch [06/10] Step [490/506]: cls_loss=0.0062\n",
      "Epoch [06/10] Step [495/506]: cls_loss=0.0015\n",
      "Epoch [06/10] Step [500/506]: cls_loss=0.1733\n",
      "Epoch [06/10] Step [505/506]: cls_loss=0.0020\n",
      "At the end of Epoch: 6\n",
      "Validation loss:  0.5992781519889832\n",
      "Accuracy: 0.8652561247216035\n",
      "F1 score (Macro): 0.8625116358820785\n",
      "F1 score (Per class): [0.86532951 0.84674752 0.87545788]\n",
      "Precision score (Per class): [0.80748663 0.78367347 0.96241611]\n",
      "Recall score (Per class): [0.93209877 0.92086331 0.80291153]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  3.070358991622925\n",
      "Accuracy: 0.4837935174069628\n",
      "F1 score (Macro): 0.47532232108350975\n",
      "F1 score (Per class): [0.35341365 0.51072125 0.56183206]\n",
      "Precision score (Per class): [0.24242424 0.55982906 0.77966102]\n",
      "Recall score (Per class): [0.65185185 0.46953405 0.43914081]\n",
      "German Test:\n",
      "Validation loss:  2.619374990463257\n",
      "Accuracy: 0.508495145631068\n",
      "F1 score (Macro): 0.47449455606514723\n",
      "F1 score (Per class): [0.31910112 0.48863636 0.61574618]\n",
      "Precision score (Per class): [0.21068249 0.46994536 0.86184211]\n",
      "Recall score (Per class): [0.65740741 0.50887574 0.47897623]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  3.4660730361938477\n",
      "Accuracy: 0.419284940411701\n",
      "F1 score (Macro): 0.41581883691343874\n",
      "F1 score (Per class): [0.38596491 0.46906475 0.39242685]\n",
      "Precision score (Per class): [0.26252983 0.68776371 0.42696629]\n",
      "Recall score (Per class): [0.72847682 0.3558952  0.36305732]\n",
      "Epoch: 7/10\n",
      "Epoch [07/10] Step [000/506]: cls_loss=0.0052\n",
      "Epoch [07/10] Step [005/506]: cls_loss=0.0538\n",
      "Epoch [07/10] Step [010/506]: cls_loss=0.0030\n",
      "Epoch [07/10] Step [015/506]: cls_loss=0.0755\n",
      "Epoch [07/10] Step [020/506]: cls_loss=0.0058\n",
      "Epoch [07/10] Step [025/506]: cls_loss=0.0974\n",
      "Epoch [07/10] Step [030/506]: cls_loss=0.0195\n",
      "Epoch [07/10] Step [035/506]: cls_loss=0.0588\n",
      "Epoch [07/10] Step [040/506]: cls_loss=0.0084\n",
      "Epoch [07/10] Step [045/506]: cls_loss=0.0017\n",
      "Epoch [07/10] Step [050/506]: cls_loss=0.0333\n",
      "Epoch [07/10] Step [055/506]: cls_loss=0.1334\n",
      "Epoch [07/10] Step [060/506]: cls_loss=0.0554\n",
      "Epoch [07/10] Step [065/506]: cls_loss=0.0021\n",
      "Epoch [07/10] Step [070/506]: cls_loss=0.0155\n",
      "Epoch [07/10] Step [075/506]: cls_loss=0.0339\n",
      "Epoch [07/10] Step [080/506]: cls_loss=0.0160\n",
      "Epoch [07/10] Step [085/506]: cls_loss=0.0124\n",
      "Epoch [07/10] Step [090/506]: cls_loss=0.3006\n",
      "Epoch [07/10] Step [095/506]: cls_loss=0.0391\n",
      "Epoch [07/10] Step [100/506]: cls_loss=0.1672\n",
      "Epoch [07/10] Step [105/506]: cls_loss=0.0196\n",
      "Epoch [07/10] Step [110/506]: cls_loss=0.0603\n",
      "Epoch [07/10] Step [115/506]: cls_loss=0.0078\n",
      "Epoch [07/10] Step [120/506]: cls_loss=0.0253\n",
      "Epoch [07/10] Step [125/506]: cls_loss=0.0192\n",
      "Epoch [07/10] Step [130/506]: cls_loss=0.0176\n",
      "Epoch [07/10] Step [135/506]: cls_loss=0.0752\n",
      "Epoch [07/10] Step [140/506]: cls_loss=0.1413\n",
      "Epoch [07/10] Step [145/506]: cls_loss=0.0432\n",
      "Epoch [07/10] Step [150/506]: cls_loss=0.0622\n",
      "Epoch [07/10] Step [155/506]: cls_loss=0.0305\n",
      "Epoch [07/10] Step [160/506]: cls_loss=0.1381\n",
      "Epoch [07/10] Step [165/506]: cls_loss=0.0041\n",
      "Epoch [07/10] Step [170/506]: cls_loss=0.0450\n",
      "Epoch [07/10] Step [175/506]: cls_loss=0.0031\n",
      "Epoch [07/10] Step [180/506]: cls_loss=0.0095\n",
      "Epoch [07/10] Step [185/506]: cls_loss=0.0019\n",
      "Epoch [07/10] Step [190/506]: cls_loss=0.0409\n",
      "Epoch [07/10] Step [195/506]: cls_loss=0.0687\n",
      "Epoch [07/10] Step [200/506]: cls_loss=0.0638\n",
      "Epoch [07/10] Step [205/506]: cls_loss=0.0067\n",
      "Epoch [07/10] Step [210/506]: cls_loss=0.0656\n",
      "Epoch [07/10] Step [215/506]: cls_loss=0.0266\n",
      "Epoch [07/10] Step [220/506]: cls_loss=0.0236\n",
      "Epoch [07/10] Step [225/506]: cls_loss=0.0821\n",
      "Epoch [07/10] Step [230/506]: cls_loss=0.0270\n",
      "Epoch [07/10] Step [235/506]: cls_loss=0.0070\n",
      "Epoch [07/10] Step [240/506]: cls_loss=0.0145\n",
      "Epoch [07/10] Step [245/506]: cls_loss=0.0769\n",
      "Epoch [07/10] Step [250/506]: cls_loss=0.0312\n",
      "Epoch [07/10] Step [255/506]: cls_loss=0.0081\n",
      "Epoch [07/10] Step [260/506]: cls_loss=0.0493\n",
      "Epoch [07/10] Step [265/506]: cls_loss=0.0309\n",
      "Epoch [07/10] Step [270/506]: cls_loss=0.0891\n",
      "Epoch [07/10] Step [275/506]: cls_loss=0.0186\n",
      "Epoch [07/10] Step [280/506]: cls_loss=0.0149\n",
      "Epoch [07/10] Step [285/506]: cls_loss=0.0177\n",
      "Epoch [07/10] Step [290/506]: cls_loss=0.0035\n",
      "Epoch [07/10] Step [295/506]: cls_loss=0.1541\n",
      "Epoch [07/10] Step [300/506]: cls_loss=0.0966\n",
      "Epoch [07/10] Step [305/506]: cls_loss=0.0041\n",
      "Epoch [07/10] Step [310/506]: cls_loss=0.1024\n",
      "Epoch [07/10] Step [315/506]: cls_loss=0.0079\n",
      "Epoch [07/10] Step [320/506]: cls_loss=0.0132\n",
      "Epoch [07/10] Step [325/506]: cls_loss=0.0209\n",
      "Epoch [07/10] Step [330/506]: cls_loss=0.1613\n",
      "Epoch [07/10] Step [335/506]: cls_loss=0.0453\n",
      "Epoch [07/10] Step [340/506]: cls_loss=0.0077\n",
      "Epoch [07/10] Step [345/506]: cls_loss=0.0037\n",
      "Epoch [07/10] Step [350/506]: cls_loss=0.0139\n",
      "Epoch [07/10] Step [355/506]: cls_loss=0.0011\n",
      "Epoch [07/10] Step [360/506]: cls_loss=0.1040\n",
      "Epoch [07/10] Step [365/506]: cls_loss=0.0231\n",
      "Epoch [07/10] Step [370/506]: cls_loss=0.0046\n",
      "Epoch [07/10] Step [375/506]: cls_loss=0.0093\n",
      "Epoch [07/10] Step [380/506]: cls_loss=0.0036\n",
      "Epoch [07/10] Step [385/506]: cls_loss=0.0715\n",
      "Epoch [07/10] Step [390/506]: cls_loss=0.0168\n",
      "Epoch [07/10] Step [395/506]: cls_loss=0.0030\n",
      "Epoch [07/10] Step [400/506]: cls_loss=0.0091\n",
      "Epoch [07/10] Step [405/506]: cls_loss=0.0209\n",
      "Epoch [07/10] Step [410/506]: cls_loss=0.0880\n",
      "Epoch [07/10] Step [415/506]: cls_loss=0.0132\n",
      "Epoch [07/10] Step [420/506]: cls_loss=0.0107\n",
      "Epoch [07/10] Step [425/506]: cls_loss=0.0366\n",
      "Epoch [07/10] Step [430/506]: cls_loss=0.1047\n",
      "Epoch [07/10] Step [435/506]: cls_loss=0.0217\n",
      "Epoch [07/10] Step [440/506]: cls_loss=0.0112\n",
      "Epoch [07/10] Step [445/506]: cls_loss=0.0192\n",
      "Epoch [07/10] Step [450/506]: cls_loss=0.0139\n",
      "Epoch [07/10] Step [455/506]: cls_loss=0.0240\n",
      "Epoch [07/10] Step [460/506]: cls_loss=0.0120\n",
      "Epoch [07/10] Step [465/506]: cls_loss=0.0111\n",
      "Epoch [07/10] Step [470/506]: cls_loss=0.0098\n",
      "Epoch [07/10] Step [475/506]: cls_loss=0.0231\n",
      "Epoch [07/10] Step [480/506]: cls_loss=0.0030\n",
      "Epoch [07/10] Step [485/506]: cls_loss=0.0054\n",
      "Epoch [07/10] Step [490/506]: cls_loss=0.1404\n",
      "Epoch [07/10] Step [495/506]: cls_loss=0.0387\n",
      "Epoch [07/10] Step [500/506]: cls_loss=0.1753\n",
      "Epoch [07/10] Step [505/506]: cls_loss=0.0003\n",
      "At the end of Epoch: 7\n",
      "Validation loss:  0.3790297210216522\n",
      "Accuracy: 0.8930957683741648\n",
      "F1 score (Macro): 0.8874604579307221\n",
      "F1 score (Per class): [0.88090349 0.87198068 0.90949721]\n",
      "Precision score (Per class): [0.87909836 0.8783455  0.90746934]\n",
      "Recall score (Per class): [0.88271605 0.86570743 0.91153415]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  2.591560125350952\n",
      "Accuracy: 0.4957983193277311\n",
      "F1 score (Macro): 0.46678525539646687\n",
      "F1 score (Per class): [0.32957111 0.45217391 0.61861075]\n",
      "Precision score (Per class): [0.23701299 0.57458564 0.68604651]\n",
      "Recall score (Per class): [0.54074074 0.37275986 0.56324582]\n",
      "German Test:\n",
      "Validation loss:  1.94314444065094\n",
      "Accuracy: 0.5728155339805825\n",
      "F1 score (Macro): 0.49224644830863484\n",
      "F1 score (Per class): [0.33160622 0.43598616 0.70914697]\n",
      "Precision score (Per class): [0.23021583 0.525      0.80985915]\n",
      "Recall score (Per class): [0.59259259 0.37278107 0.63071298]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  2.9077720642089844\n",
      "Accuracy: 0.4442036836403034\n",
      "F1 score (Macro): 0.4362126058078477\n",
      "F1 score (Per class): [0.39112903 0.52835408 0.3891547 ]\n",
      "Precision score (Per class): [0.28115942 0.72075472 0.38977636]\n",
      "Recall score (Per class): [0.64238411 0.41703057 0.38853503]\n",
      "Epoch: 8/10\n",
      "Epoch [08/10] Step [000/506]: cls_loss=0.0309\n",
      "Epoch [08/10] Step [005/506]: cls_loss=0.1614\n",
      "Epoch [08/10] Step [010/506]: cls_loss=0.0611\n",
      "Epoch [08/10] Step [015/506]: cls_loss=0.0627\n",
      "Epoch [08/10] Step [020/506]: cls_loss=0.0034\n",
      "Epoch [08/10] Step [025/506]: cls_loss=0.0987\n",
      "Epoch [08/10] Step [030/506]: cls_loss=0.0144\n",
      "Epoch [08/10] Step [035/506]: cls_loss=0.0118\n",
      "Epoch [08/10] Step [040/506]: cls_loss=0.0047\n",
      "Epoch [08/10] Step [045/506]: cls_loss=0.0045\n",
      "Epoch [08/10] Step [050/506]: cls_loss=0.0311\n",
      "Epoch [08/10] Step [055/506]: cls_loss=0.0031\n",
      "Epoch [08/10] Step [060/506]: cls_loss=0.0372\n",
      "Epoch [08/10] Step [065/506]: cls_loss=0.0037\n",
      "Epoch [08/10] Step [070/506]: cls_loss=0.0161\n",
      "Epoch [08/10] Step [075/506]: cls_loss=0.0025\n",
      "Epoch [08/10] Step [080/506]: cls_loss=0.0029\n",
      "Epoch [08/10] Step [085/506]: cls_loss=0.0303\n",
      "Epoch [08/10] Step [090/506]: cls_loss=0.0449\n",
      "Epoch [08/10] Step [095/506]: cls_loss=0.0082\n",
      "Epoch [08/10] Step [100/506]: cls_loss=0.0006\n",
      "Epoch [08/10] Step [105/506]: cls_loss=0.0028\n",
      "Epoch [08/10] Step [110/506]: cls_loss=0.0383\n",
      "Epoch [08/10] Step [115/506]: cls_loss=0.0082\n",
      "Epoch [08/10] Step [120/506]: cls_loss=0.0037\n",
      "Epoch [08/10] Step [125/506]: cls_loss=0.0022\n",
      "Epoch [08/10] Step [130/506]: cls_loss=0.0044\n",
      "Epoch [08/10] Step [135/506]: cls_loss=0.0959\n",
      "Epoch [08/10] Step [140/506]: cls_loss=0.0214\n",
      "Epoch [08/10] Step [145/506]: cls_loss=0.0503\n",
      "Epoch [08/10] Step [150/506]: cls_loss=0.0011\n",
      "Epoch [08/10] Step [155/506]: cls_loss=0.0264\n",
      "Epoch [08/10] Step [160/506]: cls_loss=0.0863\n",
      "Epoch [08/10] Step [165/506]: cls_loss=0.0052\n",
      "Epoch [08/10] Step [170/506]: cls_loss=0.0486\n",
      "Epoch [08/10] Step [175/506]: cls_loss=0.0017\n",
      "Epoch [08/10] Step [180/506]: cls_loss=0.0174\n",
      "Epoch [08/10] Step [185/506]: cls_loss=0.0007\n",
      "Epoch [08/10] Step [190/506]: cls_loss=0.0369\n",
      "Epoch [08/10] Step [195/506]: cls_loss=0.0117\n",
      "Epoch [08/10] Step [200/506]: cls_loss=0.0184\n",
      "Epoch [08/10] Step [205/506]: cls_loss=0.0039\n",
      "Epoch [08/10] Step [210/506]: cls_loss=0.0015\n",
      "Epoch [08/10] Step [215/506]: cls_loss=0.0009\n",
      "Epoch [08/10] Step [220/506]: cls_loss=0.0129\n",
      "Epoch [08/10] Step [225/506]: cls_loss=0.0005\n",
      "Epoch [08/10] Step [230/506]: cls_loss=0.0220\n",
      "Epoch [08/10] Step [235/506]: cls_loss=0.0011\n",
      "Epoch [08/10] Step [240/506]: cls_loss=0.0040\n",
      "Epoch [08/10] Step [245/506]: cls_loss=0.0007\n",
      "Epoch [08/10] Step [250/506]: cls_loss=0.0007\n",
      "Epoch [08/10] Step [255/506]: cls_loss=0.0014\n",
      "Epoch [08/10] Step [260/506]: cls_loss=0.1185\n",
      "Epoch [08/10] Step [265/506]: cls_loss=0.0060\n",
      "Epoch [08/10] Step [270/506]: cls_loss=0.0119\n",
      "Epoch [08/10] Step [275/506]: cls_loss=0.0115\n",
      "Epoch [08/10] Step [280/506]: cls_loss=0.0050\n",
      "Epoch [08/10] Step [285/506]: cls_loss=0.0273\n",
      "Epoch [08/10] Step [290/506]: cls_loss=0.0211\n",
      "Epoch [08/10] Step [295/506]: cls_loss=0.0149\n",
      "Epoch [08/10] Step [300/506]: cls_loss=0.0401\n",
      "Epoch [08/10] Step [305/506]: cls_loss=0.0045\n",
      "Epoch [08/10] Step [310/506]: cls_loss=0.0151\n",
      "Epoch [08/10] Step [315/506]: cls_loss=0.0291\n",
      "Epoch [08/10] Step [320/506]: cls_loss=0.0812\n",
      "Epoch [08/10] Step [325/506]: cls_loss=0.1321\n",
      "Epoch [08/10] Step [330/506]: cls_loss=0.0188\n",
      "Epoch [08/10] Step [335/506]: cls_loss=0.0037\n",
      "Epoch [08/10] Step [340/506]: cls_loss=0.0092\n",
      "Epoch [08/10] Step [345/506]: cls_loss=0.0299\n",
      "Epoch [08/10] Step [350/506]: cls_loss=0.0165\n",
      "Epoch [08/10] Step [355/506]: cls_loss=0.1146\n",
      "Epoch [08/10] Step [360/506]: cls_loss=0.0064\n",
      "Epoch [08/10] Step [365/506]: cls_loss=0.0160\n",
      "Epoch [08/10] Step [370/506]: cls_loss=0.0346\n",
      "Epoch [08/10] Step [375/506]: cls_loss=0.0029\n",
      "Epoch [08/10] Step [380/506]: cls_loss=0.0743\n",
      "Epoch [08/10] Step [385/506]: cls_loss=0.0499\n",
      "Epoch [08/10] Step [390/506]: cls_loss=0.0033\n",
      "Epoch [08/10] Step [395/506]: cls_loss=0.0154\n",
      "Epoch [08/10] Step [400/506]: cls_loss=0.0053\n",
      "Epoch [08/10] Step [405/506]: cls_loss=0.0126\n",
      "Epoch [08/10] Step [410/506]: cls_loss=0.0044\n",
      "Epoch [08/10] Step [415/506]: cls_loss=0.0436\n",
      "Epoch [08/10] Step [420/506]: cls_loss=0.0734\n",
      "Epoch [08/10] Step [425/506]: cls_loss=0.0021\n",
      "Epoch [08/10] Step [430/506]: cls_loss=0.0015\n",
      "Epoch [08/10] Step [435/506]: cls_loss=0.0306\n",
      "Epoch [08/10] Step [440/506]: cls_loss=0.0144\n",
      "Epoch [08/10] Step [445/506]: cls_loss=0.0083\n",
      "Epoch [08/10] Step [450/506]: cls_loss=0.0016\n",
      "Epoch [08/10] Step [455/506]: cls_loss=0.0068\n",
      "Epoch [08/10] Step [460/506]: cls_loss=0.0043\n",
      "Epoch [08/10] Step [465/506]: cls_loss=0.0159\n",
      "Epoch [08/10] Step [470/506]: cls_loss=0.0495\n",
      "Epoch [08/10] Step [475/506]: cls_loss=0.0033\n",
      "Epoch [08/10] Step [480/506]: cls_loss=0.0015\n",
      "Epoch [08/10] Step [485/506]: cls_loss=0.0007\n",
      "Epoch [08/10] Step [490/506]: cls_loss=0.0800\n",
      "Epoch [08/10] Step [495/506]: cls_loss=0.0044\n",
      "Epoch [08/10] Step [500/506]: cls_loss=0.1259\n",
      "Epoch [08/10] Step [505/506]: cls_loss=0.0011\n",
      "At the end of Epoch: 8\n",
      "Validation loss:  0.4607522189617157\n",
      "Accuracy: 0.8808463251670379\n",
      "F1 score (Macro): 0.8739832082488203\n",
      "F1 score (Per class): [0.85985248 0.85909091 0.90300624]\n",
      "Precision score (Per class): [0.8812095  0.81641469 0.91494253]\n",
      "Recall score (Per class): [0.83950617 0.90647482 0.89137738]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  2.586707830429077\n",
      "Accuracy: 0.5090036014405762\n",
      "F1 score (Macro): 0.48977902713854443\n",
      "F1 score (Per class): [0.35714286 0.5271028  0.58509142]\n",
      "Precision score (Per class): [0.26315789 0.55078125 0.71232877]\n",
      "Recall score (Per class): [0.55555556 0.50537634 0.49642005]\n",
      "German Test:\n",
      "Validation loss:  1.897445559501648\n",
      "Accuracy: 0.5825242718446602\n",
      "F1 score (Macro): 0.4880255861926262\n",
      "F1 score (Per class): [0.31944444 0.42818428 0.71644803]\n",
      "Precision score (Per class): [0.25555556 0.395      0.79954955]\n",
      "Recall score (Per class): [0.42592593 0.46745562 0.64899452]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  3.004633665084839\n",
      "Accuracy: 0.4452871072589382\n",
      "F1 score (Macro): 0.431499701848539\n",
      "F1 score (Per class): [0.375      0.53488372 0.38461538]\n",
      "Precision score (Per class): [0.28282828 0.65506329 0.38709677]\n",
      "Recall score (Per class): [0.55629139 0.45196507 0.38216561]\n",
      "Epoch: 9/10\n",
      "Epoch [09/10] Step [000/506]: cls_loss=0.0205\n",
      "Epoch [09/10] Step [005/506]: cls_loss=0.0297\n",
      "Epoch [09/10] Step [010/506]: cls_loss=0.0212\n",
      "Epoch [09/10] Step [015/506]: cls_loss=0.0013\n",
      "Epoch [09/10] Step [020/506]: cls_loss=0.0100\n",
      "Epoch [09/10] Step [025/506]: cls_loss=0.0026\n",
      "Epoch [09/10] Step [030/506]: cls_loss=0.0825\n",
      "Epoch [09/10] Step [035/506]: cls_loss=0.2475\n",
      "Epoch [09/10] Step [040/506]: cls_loss=0.0064\n",
      "Epoch [09/10] Step [045/506]: cls_loss=0.0011\n",
      "Epoch [09/10] Step [050/506]: cls_loss=0.0928\n",
      "Epoch [09/10] Step [055/506]: cls_loss=0.0100\n",
      "Epoch [09/10] Step [060/506]: cls_loss=0.0897\n",
      "Epoch [09/10] Step [065/506]: cls_loss=0.0803\n",
      "Epoch [09/10] Step [070/506]: cls_loss=0.0049\n",
      "Epoch [09/10] Step [075/506]: cls_loss=0.0722\n",
      "Epoch [09/10] Step [080/506]: cls_loss=0.0369\n",
      "Epoch [09/10] Step [085/506]: cls_loss=0.0040\n",
      "Epoch [09/10] Step [090/506]: cls_loss=0.0127\n",
      "Epoch [09/10] Step [095/506]: cls_loss=0.0005\n",
      "Epoch [09/10] Step [100/506]: cls_loss=0.1583\n",
      "Epoch [09/10] Step [105/506]: cls_loss=0.0025\n",
      "Epoch [09/10] Step [110/506]: cls_loss=0.0619\n",
      "Epoch [09/10] Step [115/506]: cls_loss=0.0142\n",
      "Epoch [09/10] Step [120/506]: cls_loss=0.0958\n",
      "Epoch [09/10] Step [125/506]: cls_loss=0.1091\n",
      "Epoch [09/10] Step [130/506]: cls_loss=0.0274\n",
      "Epoch [09/10] Step [135/506]: cls_loss=0.0286\n",
      "Epoch [09/10] Step [140/506]: cls_loss=0.0099\n",
      "Epoch [09/10] Step [145/506]: cls_loss=0.1817\n",
      "Epoch [09/10] Step [150/506]: cls_loss=0.0036\n",
      "Epoch [09/10] Step [155/506]: cls_loss=0.0575\n",
      "Epoch [09/10] Step [160/506]: cls_loss=0.0123\n",
      "Epoch [09/10] Step [165/506]: cls_loss=0.0621\n",
      "Epoch [09/10] Step [170/506]: cls_loss=0.0050\n",
      "Epoch [09/10] Step [175/506]: cls_loss=0.0133\n",
      "Epoch [09/10] Step [180/506]: cls_loss=0.0112\n",
      "Epoch [09/10] Step [185/506]: cls_loss=0.0588\n",
      "Epoch [09/10] Step [190/506]: cls_loss=0.0017\n",
      "Epoch [09/10] Step [195/506]: cls_loss=0.0060\n",
      "Epoch [09/10] Step [200/506]: cls_loss=0.0282\n",
      "Epoch [09/10] Step [205/506]: cls_loss=0.0071\n",
      "Epoch [09/10] Step [210/506]: cls_loss=0.0332\n",
      "Epoch [09/10] Step [215/506]: cls_loss=0.0892\n",
      "Epoch [09/10] Step [220/506]: cls_loss=0.0148\n",
      "Epoch [09/10] Step [225/506]: cls_loss=0.0335\n",
      "Epoch [09/10] Step [230/506]: cls_loss=0.0495\n",
      "Epoch [09/10] Step [235/506]: cls_loss=0.0443\n",
      "Epoch [09/10] Step [240/506]: cls_loss=0.0424\n",
      "Epoch [09/10] Step [245/506]: cls_loss=0.0619\n",
      "Epoch [09/10] Step [250/506]: cls_loss=0.0032\n",
      "Epoch [09/10] Step [255/506]: cls_loss=0.0226\n",
      "Epoch [09/10] Step [260/506]: cls_loss=0.1400\n",
      "Epoch [09/10] Step [265/506]: cls_loss=0.0274\n",
      "Epoch [09/10] Step [270/506]: cls_loss=0.0036\n",
      "Epoch [09/10] Step [275/506]: cls_loss=0.0557\n",
      "Epoch [09/10] Step [280/506]: cls_loss=0.0034\n",
      "Epoch [09/10] Step [285/506]: cls_loss=0.0949\n",
      "Epoch [09/10] Step [290/506]: cls_loss=0.0062\n",
      "Epoch [09/10] Step [295/506]: cls_loss=0.0031\n",
      "Epoch [09/10] Step [300/506]: cls_loss=0.0197\n",
      "Epoch [09/10] Step [305/506]: cls_loss=0.0148\n",
      "Epoch [09/10] Step [310/506]: cls_loss=0.0081\n",
      "Epoch [09/10] Step [315/506]: cls_loss=0.0293\n",
      "Epoch [09/10] Step [320/506]: cls_loss=0.0057\n",
      "Epoch [09/10] Step [325/506]: cls_loss=0.0251\n",
      "Epoch [09/10] Step [330/506]: cls_loss=0.0817\n",
      "Epoch [09/10] Step [335/506]: cls_loss=0.0016\n",
      "Epoch [09/10] Step [340/506]: cls_loss=0.0079\n",
      "Epoch [09/10] Step [345/506]: cls_loss=0.0053\n",
      "Epoch [09/10] Step [350/506]: cls_loss=0.0049\n",
      "Epoch [09/10] Step [355/506]: cls_loss=0.0018\n",
      "Epoch [09/10] Step [360/506]: cls_loss=0.0010\n",
      "Epoch [09/10] Step [365/506]: cls_loss=0.0017\n",
      "Epoch [09/10] Step [370/506]: cls_loss=0.0024\n",
      "Epoch [09/10] Step [375/506]: cls_loss=0.0009\n",
      "Epoch [09/10] Step [380/506]: cls_loss=0.0031\n",
      "Epoch [09/10] Step [385/506]: cls_loss=0.0721\n",
      "Epoch [09/10] Step [390/506]: cls_loss=0.1299\n",
      "Epoch [09/10] Step [395/506]: cls_loss=0.0078\n",
      "Epoch [09/10] Step [400/506]: cls_loss=0.0024\n",
      "Epoch [09/10] Step [405/506]: cls_loss=0.0025\n",
      "Epoch [09/10] Step [410/506]: cls_loss=0.1507\n",
      "Epoch [09/10] Step [415/506]: cls_loss=0.0166\n",
      "Epoch [09/10] Step [420/506]: cls_loss=0.1575\n",
      "Epoch [09/10] Step [425/506]: cls_loss=0.0025\n",
      "Epoch [09/10] Step [430/506]: cls_loss=0.0013\n",
      "Epoch [09/10] Step [435/506]: cls_loss=0.0098\n",
      "Epoch [09/10] Step [440/506]: cls_loss=0.0913\n",
      "Epoch [09/10] Step [445/506]: cls_loss=0.1187\n",
      "Epoch [09/10] Step [450/506]: cls_loss=0.0569\n",
      "Epoch [09/10] Step [455/506]: cls_loss=0.0046\n",
      "Epoch [09/10] Step [460/506]: cls_loss=0.1894\n",
      "Epoch [09/10] Step [465/506]: cls_loss=0.0019\n",
      "Epoch [09/10] Step [470/506]: cls_loss=0.0072\n",
      "Epoch [09/10] Step [475/506]: cls_loss=0.0568\n",
      "Epoch [09/10] Step [480/506]: cls_loss=0.0071\n",
      "Epoch [09/10] Step [485/506]: cls_loss=0.1740\n",
      "Epoch [09/10] Step [490/506]: cls_loss=0.0016\n",
      "Epoch [09/10] Step [495/506]: cls_loss=0.1802\n",
      "Epoch [09/10] Step [500/506]: cls_loss=0.0573\n",
      "Epoch [09/10] Step [505/506]: cls_loss=0.0062\n",
      "At the end of Epoch: 9\n",
      "Validation loss:  0.4333856403827667\n",
      "Accuracy: 0.873608017817372\n",
      "F1 score (Macro): 0.870447569499543\n",
      "F1 score (Per class): [0.87261785 0.8534202  0.88530466]\n",
      "Precision score (Per class): [0.85127202 0.7797619  0.94878361]\n",
      "Recall score (Per class): [0.89506173 0.94244604 0.82978723]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  2.974646806716919\n",
      "Accuracy: 0.4417767106842737\n",
      "F1 score (Macro): 0.43821037528627027\n",
      "F1 score (Per class): [0.33399602 0.51492537 0.46570973]\n",
      "Precision score (Per class): [0.22826087 0.53696498 0.70192308]\n",
      "Recall score (Per class): [0.62222222 0.49462366 0.34844869]\n",
      "German Test:\n",
      "Validation loss:  2.328171968460083\n",
      "Accuracy: 0.5315533980582524\n",
      "F1 score (Macro): 0.49159179737695347\n",
      "F1 score (Per class): [0.3254717  0.50857143 0.64073227]\n",
      "Precision score (Per class): [0.21835443 0.49171271 0.85626911]\n",
      "Recall score (Per class): [0.63888889 0.52662722 0.511883  ]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  2.801257848739624\n",
      "Accuracy: 0.447453954496208\n",
      "F1 score (Macro): 0.4306486992835978\n",
      "F1 score (Per class): [0.37924152 0.55683269 0.35587189]\n",
      "Precision score (Per class): [0.27142857 0.67076923 0.40322581]\n",
      "Recall score (Per class): [0.62913907 0.47598253 0.31847134]\n"
     ]
    }
   ],
   "source": [
    "src_encoder, src_classifier = pretrain(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0020352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10\n",
      "Epoch [00/10] Step [000/127]: cls_loss=1.0818\n",
      "Epoch [00/10] Step [005/127]: cls_loss=1.0061\n",
      "Epoch [00/10] Step [010/127]: cls_loss=0.8443\n",
      "Epoch [00/10] Step [015/127]: cls_loss=1.0951\n",
      "Epoch [00/10] Step [020/127]: cls_loss=0.9052\n",
      "Epoch [00/10] Step [025/127]: cls_loss=0.7875\n",
      "Epoch [00/10] Step [030/127]: cls_loss=0.8519\n",
      "Epoch [00/10] Step [035/127]: cls_loss=0.9050\n",
      "Epoch [00/10] Step [040/127]: cls_loss=0.8261\n",
      "Epoch [00/10] Step [045/127]: cls_loss=0.9965\n",
      "Epoch [00/10] Step [050/127]: cls_loss=0.9563\n",
      "Epoch [00/10] Step [055/127]: cls_loss=0.6649\n",
      "Epoch [00/10] Step [060/127]: cls_loss=0.8563\n",
      "Epoch [00/10] Step [065/127]: cls_loss=0.8909\n",
      "Epoch [00/10] Step [070/127]: cls_loss=0.6137\n",
      "Epoch [00/10] Step [075/127]: cls_loss=0.8152\n",
      "Epoch [00/10] Step [080/127]: cls_loss=0.5606\n",
      "Epoch [00/10] Step [085/127]: cls_loss=0.6402\n",
      "Epoch [00/10] Step [090/127]: cls_loss=0.7340\n",
      "Epoch [00/10] Step [095/127]: cls_loss=0.7308\n",
      "Epoch [00/10] Step [100/127]: cls_loss=0.5439\n",
      "Epoch [00/10] Step [105/127]: cls_loss=0.8152\n",
      "Epoch [00/10] Step [110/127]: cls_loss=0.7392\n",
      "Epoch [00/10] Step [115/127]: cls_loss=0.8447\n",
      "Epoch [00/10] Step [120/127]: cls_loss=0.7120\n",
      "Epoch [00/10] Step [125/127]: cls_loss=0.6223\n",
      "At the end of Epoch: 0\n",
      "Validation loss:  0.6503218412399292\n",
      "Accuracy: 0.7282850779510023\n",
      "F1 score (Macro): 0.6841947398848126\n",
      "F1 score (Per class): [0.61016949 0.64835165 0.79406308]\n",
      "Precision score (Per class): [0.675      0.7972973  0.72542373]\n",
      "Recall score (Per class): [0.55670103 0.5462963  0.87704918]\n",
      "Epoch: 1/10\n",
      "Epoch [01/10] Step [000/127]: cls_loss=0.6500\n",
      "Epoch [01/10] Step [005/127]: cls_loss=0.6582\n",
      "Epoch [01/10] Step [010/127]: cls_loss=0.5752\n",
      "Epoch [01/10] Step [015/127]: cls_loss=0.7081\n",
      "Epoch [01/10] Step [020/127]: cls_loss=0.7309\n",
      "Epoch [01/10] Step [025/127]: cls_loss=0.4642\n",
      "Epoch [01/10] Step [030/127]: cls_loss=0.6190\n",
      "Epoch [01/10] Step [035/127]: cls_loss=0.5917\n",
      "Epoch [01/10] Step [040/127]: cls_loss=0.3626\n",
      "Epoch [01/10] Step [045/127]: cls_loss=0.6462\n",
      "Epoch [01/10] Step [050/127]: cls_loss=0.3994\n",
      "Epoch [01/10] Step [055/127]: cls_loss=0.4565\n",
      "Epoch [01/10] Step [060/127]: cls_loss=0.6088\n",
      "Epoch [01/10] Step [065/127]: cls_loss=0.4351\n",
      "Epoch [01/10] Step [070/127]: cls_loss=0.5195\n",
      "Epoch [01/10] Step [075/127]: cls_loss=0.6395\n",
      "Epoch [01/10] Step [080/127]: cls_loss=0.5778\n",
      "Epoch [01/10] Step [085/127]: cls_loss=0.4303\n",
      "Epoch [01/10] Step [090/127]: cls_loss=0.5800\n",
      "Epoch [01/10] Step [095/127]: cls_loss=0.3727\n",
      "Epoch [01/10] Step [100/127]: cls_loss=0.2380\n",
      "Epoch [01/10] Step [105/127]: cls_loss=0.6134\n",
      "Epoch [01/10] Step [110/127]: cls_loss=0.4263\n",
      "Epoch [01/10] Step [115/127]: cls_loss=0.4141\n",
      "Epoch [01/10] Step [120/127]: cls_loss=0.5508\n",
      "Epoch [01/10] Step [125/127]: cls_loss=0.3240\n",
      "At the end of Epoch: 1\n",
      "Validation loss:  0.7155512571334839\n",
      "Accuracy: 0.7305122494432071\n",
      "F1 score (Macro): 0.6994561076454534\n",
      "F1 score (Per class): [0.63855422 0.66949153 0.79032258]\n",
      "Precision score (Per class): [0.76811594 0.6171875  0.77777778]\n",
      "Recall score (Per class): [0.54639175 0.73148148 0.80327869]\n",
      "Epoch: 2/10\n",
      "Epoch [02/10] Step [000/127]: cls_loss=0.5424\n",
      "Epoch [02/10] Step [005/127]: cls_loss=0.2234\n",
      "Epoch [02/10] Step [010/127]: cls_loss=0.5847\n",
      "Epoch [02/10] Step [015/127]: cls_loss=0.3970\n",
      "Epoch [02/10] Step [020/127]: cls_loss=0.5163\n",
      "Epoch [02/10] Step [025/127]: cls_loss=0.3459\n",
      "Epoch [02/10] Step [030/127]: cls_loss=0.3763\n",
      "Epoch [02/10] Step [035/127]: cls_loss=0.7541\n",
      "Epoch [02/10] Step [040/127]: cls_loss=0.1817\n",
      "Epoch [02/10] Step [045/127]: cls_loss=0.3568\n",
      "Epoch [02/10] Step [050/127]: cls_loss=0.1555\n",
      "Epoch [02/10] Step [055/127]: cls_loss=0.1772\n",
      "Epoch [02/10] Step [060/127]: cls_loss=0.2808\n",
      "Epoch [02/10] Step [065/127]: cls_loss=0.0764\n",
      "Epoch [02/10] Step [070/127]: cls_loss=0.1531\n",
      "Epoch [02/10] Step [075/127]: cls_loss=0.3189\n",
      "Epoch [02/10] Step [080/127]: cls_loss=0.3404\n",
      "Epoch [02/10] Step [085/127]: cls_loss=0.4217\n",
      "Epoch [02/10] Step [090/127]: cls_loss=0.2663\n",
      "Epoch [02/10] Step [095/127]: cls_loss=0.2481\n",
      "Epoch [02/10] Step [100/127]: cls_loss=0.0853\n",
      "Epoch [02/10] Step [105/127]: cls_loss=0.4492\n",
      "Epoch [02/10] Step [110/127]: cls_loss=0.2100\n",
      "Epoch [02/10] Step [115/127]: cls_loss=0.1922\n",
      "Epoch [02/10] Step [120/127]: cls_loss=0.3779\n",
      "Epoch [02/10] Step [125/127]: cls_loss=0.1334\n",
      "At the end of Epoch: 2\n",
      "Validation loss:  0.9109726548194885\n",
      "Accuracy: 0.6525612472160356\n",
      "F1 score (Macro): 0.642935354584449\n",
      "F1 score (Per class): [0.64327485 0.57525084 0.71028037]\n",
      "Precision score (Per class): [0.74324324 0.45026178 0.82608696]\n",
      "Recall score (Per class): [0.56701031 0.7962963  0.62295082]\n",
      "Epoch: 3/10\n",
      "Epoch [03/10] Step [000/127]: cls_loss=0.4404\n",
      "Epoch [03/10] Step [005/127]: cls_loss=0.2140\n",
      "Epoch [03/10] Step [010/127]: cls_loss=0.5971\n",
      "Epoch [03/10] Step [015/127]: cls_loss=0.2863\n",
      "Epoch [03/10] Step [020/127]: cls_loss=0.3359\n",
      "Epoch [03/10] Step [025/127]: cls_loss=0.2734\n",
      "Epoch [03/10] Step [030/127]: cls_loss=0.5059\n",
      "Epoch [03/10] Step [035/127]: cls_loss=0.1956\n",
      "Epoch [03/10] Step [040/127]: cls_loss=0.1054\n",
      "Epoch [03/10] Step [045/127]: cls_loss=0.1803\n",
      "Epoch [03/10] Step [050/127]: cls_loss=0.1052\n",
      "Epoch [03/10] Step [055/127]: cls_loss=0.1692\n",
      "Epoch [03/10] Step [060/127]: cls_loss=0.2201\n",
      "Epoch [03/10] Step [065/127]: cls_loss=0.1875\n",
      "Epoch [03/10] Step [070/127]: cls_loss=0.1950\n",
      "Epoch [03/10] Step [075/127]: cls_loss=0.1443\n",
      "Epoch [03/10] Step [080/127]: cls_loss=0.1950\n",
      "Epoch [03/10] Step [085/127]: cls_loss=0.1027\n",
      "Epoch [03/10] Step [090/127]: cls_loss=0.5078\n",
      "Epoch [03/10] Step [095/127]: cls_loss=0.1283\n",
      "Epoch [03/10] Step [100/127]: cls_loss=0.2323\n",
      "Epoch [03/10] Step [105/127]: cls_loss=0.6214\n",
      "Epoch [03/10] Step [110/127]: cls_loss=0.2374\n",
      "Epoch [03/10] Step [115/127]: cls_loss=0.5709\n",
      "Epoch [03/10] Step [120/127]: cls_loss=0.1004\n",
      "Epoch [03/10] Step [125/127]: cls_loss=0.2369\n",
      "At the end of Epoch: 3\n",
      "Validation loss:  0.7719160318374634\n",
      "Accuracy: 0.734966592427617\n",
      "F1 score (Macro): 0.6902830644185282\n",
      "F1 score (Per class): [0.6627907  0.59898477 0.80907372]\n",
      "Precision score (Per class): [0.76       0.66292135 0.75087719]\n",
      "Recall score (Per class): [0.58762887 0.5462963  0.87704918]\n",
      "Epoch: 4/10\n",
      "Epoch [04/10] Step [000/127]: cls_loss=0.1055\n",
      "Epoch [04/10] Step [005/127]: cls_loss=0.2507\n",
      "Epoch [04/10] Step [010/127]: cls_loss=0.2709\n",
      "Epoch [04/10] Step [015/127]: cls_loss=0.3452\n",
      "Epoch [04/10] Step [020/127]: cls_loss=0.1280\n",
      "Epoch [04/10] Step [025/127]: cls_loss=0.4125\n",
      "Epoch [04/10] Step [030/127]: cls_loss=0.2162\n",
      "Epoch [04/10] Step [035/127]: cls_loss=0.6528\n",
      "Epoch [04/10] Step [040/127]: cls_loss=0.2609\n",
      "Epoch [04/10] Step [045/127]: cls_loss=0.0979\n",
      "Epoch [04/10] Step [050/127]: cls_loss=0.5493\n",
      "Epoch [04/10] Step [055/127]: cls_loss=0.1112\n",
      "Epoch [04/10] Step [060/127]: cls_loss=0.5387\n",
      "Epoch [04/10] Step [065/127]: cls_loss=0.0294\n",
      "Epoch [04/10] Step [070/127]: cls_loss=0.1473\n",
      "Epoch [04/10] Step [075/127]: cls_loss=0.1284\n",
      "Epoch [04/10] Step [080/127]: cls_loss=0.0571\n",
      "Epoch [04/10] Step [085/127]: cls_loss=0.2905\n",
      "Epoch [04/10] Step [090/127]: cls_loss=0.2648\n",
      "Epoch [04/10] Step [095/127]: cls_loss=0.0819\n",
      "Epoch [04/10] Step [100/127]: cls_loss=0.1738\n",
      "Epoch [04/10] Step [105/127]: cls_loss=0.2183\n",
      "Epoch [04/10] Step [110/127]: cls_loss=0.1018\n",
      "Epoch [04/10] Step [115/127]: cls_loss=0.1289\n",
      "Epoch [04/10] Step [120/127]: cls_loss=0.0602\n",
      "Epoch [04/10] Step [125/127]: cls_loss=0.0267\n",
      "At the end of Epoch: 4\n",
      "Validation loss:  0.8369865417480469\n",
      "Accuracy: 0.7505567928730512\n",
      "F1 score (Macro): 0.7195048008862365\n",
      "F1 score (Per class): [0.69791667 0.64186047 0.81873727]\n",
      "Precision score (Per class): [0.70526316 0.64485981 0.81376518]\n",
      "Recall score (Per class): [0.69072165 0.63888889 0.82377049]\n",
      "Epoch: 5/10\n",
      "Epoch [05/10] Step [000/127]: cls_loss=0.0174\n",
      "Epoch [05/10] Step [005/127]: cls_loss=0.0262\n",
      "Epoch [05/10] Step [010/127]: cls_loss=0.3599\n",
      "Epoch [05/10] Step [015/127]: cls_loss=0.0569\n",
      "Epoch [05/10] Step [020/127]: cls_loss=0.0197\n",
      "Epoch [05/10] Step [025/127]: cls_loss=0.0410\n",
      "Epoch [05/10] Step [030/127]: cls_loss=0.0924\n",
      "Epoch [05/10] Step [035/127]: cls_loss=0.0772\n",
      "Epoch [05/10] Step [040/127]: cls_loss=0.0583\n",
      "Epoch [05/10] Step [045/127]: cls_loss=0.2250\n",
      "Epoch [05/10] Step [050/127]: cls_loss=0.0510\n",
      "Epoch [05/10] Step [055/127]: cls_loss=0.1107\n",
      "Epoch [05/10] Step [060/127]: cls_loss=0.2667\n",
      "Epoch [05/10] Step [065/127]: cls_loss=0.0333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [05/10] Step [070/127]: cls_loss=0.0204\n",
      "Epoch [05/10] Step [075/127]: cls_loss=0.0360\n",
      "Epoch [05/10] Step [080/127]: cls_loss=0.1259\n",
      "Epoch [05/10] Step [085/127]: cls_loss=0.0680\n",
      "Epoch [05/10] Step [090/127]: cls_loss=0.5151\n",
      "Epoch [05/10] Step [095/127]: cls_loss=0.0298\n",
      "Epoch [05/10] Step [100/127]: cls_loss=0.0233\n",
      "Epoch [05/10] Step [105/127]: cls_loss=0.1870\n",
      "Epoch [05/10] Step [110/127]: cls_loss=0.0199\n",
      "Epoch [05/10] Step [115/127]: cls_loss=0.1852\n",
      "Epoch [05/10] Step [120/127]: cls_loss=0.0169\n",
      "Epoch [05/10] Step [125/127]: cls_loss=0.0328\n",
      "At the end of Epoch: 5\n",
      "Validation loss:  0.8925796747207642\n",
      "Accuracy: 0.7572383073496659\n",
      "F1 score (Macro): 0.7182298040012317\n",
      "F1 score (Per class): [0.71698113 0.60638298 0.8313253 ]\n",
      "Precision score (Per class): [0.66086957 0.7125     0.81496063]\n",
      "Recall score (Per class): [0.78350515 0.52777778 0.84836066]\n",
      "Epoch: 6/10\n",
      "Epoch [06/10] Step [000/127]: cls_loss=0.0363\n",
      "Epoch [06/10] Step [005/127]: cls_loss=0.0284\n",
      "Epoch [06/10] Step [010/127]: cls_loss=0.1743\n",
      "Epoch [06/10] Step [015/127]: cls_loss=0.0638\n",
      "Epoch [06/10] Step [020/127]: cls_loss=0.0231\n",
      "Epoch [06/10] Step [025/127]: cls_loss=0.0200\n",
      "Epoch [06/10] Step [030/127]: cls_loss=0.1113\n",
      "Epoch [06/10] Step [035/127]: cls_loss=0.1456\n",
      "Epoch [06/10] Step [040/127]: cls_loss=0.2322\n",
      "Epoch [06/10] Step [045/127]: cls_loss=0.0251\n",
      "Epoch [06/10] Step [050/127]: cls_loss=0.0255\n",
      "Epoch [06/10] Step [055/127]: cls_loss=0.0295\n",
      "Epoch [06/10] Step [060/127]: cls_loss=0.1823\n",
      "Epoch [06/10] Step [065/127]: cls_loss=0.0277\n",
      "Epoch [06/10] Step [070/127]: cls_loss=0.0644\n",
      "Epoch [06/10] Step [075/127]: cls_loss=0.0826\n",
      "Epoch [06/10] Step [080/127]: cls_loss=0.0088\n",
      "Epoch [06/10] Step [085/127]: cls_loss=0.0079\n",
      "Epoch [06/10] Step [090/127]: cls_loss=0.2083\n",
      "Epoch [06/10] Step [095/127]: cls_loss=0.0200\n",
      "Epoch [06/10] Step [100/127]: cls_loss=0.0064\n",
      "Epoch [06/10] Step [105/127]: cls_loss=0.1692\n",
      "Epoch [06/10] Step [110/127]: cls_loss=0.0406\n",
      "Epoch [06/10] Step [115/127]: cls_loss=0.0528\n",
      "Epoch [06/10] Step [120/127]: cls_loss=0.0072\n",
      "Epoch [06/10] Step [125/127]: cls_loss=0.2037\n",
      "At the end of Epoch: 6\n",
      "Validation loss:  0.9825409054756165\n",
      "Accuracy: 0.7594654788418709\n",
      "F1 score (Macro): 0.7278523754553997\n",
      "F1 score (Per class): [0.67015707 0.69268293 0.82071713]\n",
      "Precision score (Per class): [0.68085106 0.73195876 0.79844961]\n",
      "Recall score (Per class): [0.65979381 0.65740741 0.8442623 ]\n",
      "Epoch: 7/10\n",
      "Epoch [07/10] Step [000/127]: cls_loss=0.0087\n",
      "Epoch [07/10] Step [005/127]: cls_loss=0.0148\n",
      "Epoch [07/10] Step [010/127]: cls_loss=0.1897\n",
      "Epoch [07/10] Step [015/127]: cls_loss=0.0154\n",
      "Epoch [07/10] Step [020/127]: cls_loss=0.0123\n",
      "Epoch [07/10] Step [025/127]: cls_loss=0.0099\n",
      "Epoch [07/10] Step [030/127]: cls_loss=0.0823\n",
      "Epoch [07/10] Step [035/127]: cls_loss=0.0207\n",
      "Epoch [07/10] Step [040/127]: cls_loss=0.0047\n",
      "Epoch [07/10] Step [045/127]: cls_loss=0.0059\n",
      "Epoch [07/10] Step [050/127]: cls_loss=0.0051\n",
      "Epoch [07/10] Step [055/127]: cls_loss=0.2131\n",
      "Epoch [07/10] Step [060/127]: cls_loss=0.0141\n",
      "Epoch [07/10] Step [065/127]: cls_loss=0.0069\n",
      "Epoch [07/10] Step [070/127]: cls_loss=0.0098\n",
      "Epoch [07/10] Step [075/127]: cls_loss=0.0377\n",
      "Epoch [07/10] Step [080/127]: cls_loss=0.1124\n",
      "Epoch [07/10] Step [085/127]: cls_loss=0.0301\n",
      "Epoch [07/10] Step [090/127]: cls_loss=0.2135\n",
      "Epoch [07/10] Step [095/127]: cls_loss=0.0067\n",
      "Epoch [07/10] Step [100/127]: cls_loss=0.0189\n",
      "Epoch [07/10] Step [105/127]: cls_loss=0.0403\n",
      "Epoch [07/10] Step [110/127]: cls_loss=0.0202\n",
      "Epoch [07/10] Step [115/127]: cls_loss=0.2774\n",
      "Epoch [07/10] Step [120/127]: cls_loss=0.0620\n",
      "Epoch [07/10] Step [125/127]: cls_loss=0.0291\n",
      "At the end of Epoch: 7\n",
      "Validation loss:  1.0275959968566895\n",
      "Accuracy: 0.7505567928730512\n",
      "F1 score (Macro): 0.7112777938999875\n",
      "F1 score (Per class): [0.67264574 0.63905325 0.82213439]\n",
      "Precision score (Per class): [0.5952381  0.8852459  0.79389313]\n",
      "Recall score (Per class): [0.77319588 0.5        0.85245902]\n",
      "Epoch: 8/10\n",
      "Epoch [08/10] Step [000/127]: cls_loss=0.0123\n",
      "Epoch [08/10] Step [005/127]: cls_loss=0.0658\n",
      "Epoch [08/10] Step [010/127]: cls_loss=0.2939\n",
      "Epoch [08/10] Step [015/127]: cls_loss=0.0127\n",
      "Epoch [08/10] Step [020/127]: cls_loss=0.0049\n",
      "Epoch [08/10] Step [025/127]: cls_loss=0.0329\n",
      "Epoch [08/10] Step [030/127]: cls_loss=0.0080\n",
      "Epoch [08/10] Step [035/127]: cls_loss=0.0110\n",
      "Epoch [08/10] Step [040/127]: cls_loss=0.1029\n",
      "Epoch [08/10] Step [045/127]: cls_loss=0.0078\n",
      "Epoch [08/10] Step [050/127]: cls_loss=0.0048\n",
      "Epoch [08/10] Step [055/127]: cls_loss=0.0101\n",
      "Epoch [08/10] Step [060/127]: cls_loss=0.0339\n",
      "Epoch [08/10] Step [065/127]: cls_loss=0.0220\n",
      "Epoch [08/10] Step [070/127]: cls_loss=0.0084\n",
      "Epoch [08/10] Step [075/127]: cls_loss=0.0041\n",
      "Epoch [08/10] Step [080/127]: cls_loss=0.0090\n",
      "Epoch [08/10] Step [085/127]: cls_loss=0.0044\n",
      "Epoch [08/10] Step [090/127]: cls_loss=0.0818\n",
      "Epoch [08/10] Step [095/127]: cls_loss=0.1402\n",
      "Epoch [08/10] Step [100/127]: cls_loss=0.0036\n",
      "Epoch [08/10] Step [105/127]: cls_loss=0.1035\n",
      "Epoch [08/10] Step [110/127]: cls_loss=0.0445\n",
      "Epoch [08/10] Step [115/127]: cls_loss=0.0215\n",
      "Epoch [08/10] Step [120/127]: cls_loss=0.0144\n",
      "Epoch [08/10] Step [125/127]: cls_loss=0.0382\n",
      "At the end of Epoch: 8\n",
      "Validation loss:  1.2932021617889404\n",
      "Accuracy: 0.7060133630289532\n",
      "F1 score (Macro): 0.6760242089523686\n",
      "F1 score (Per class): [0.60162602 0.64088398 0.78556263]\n",
      "Precision score (Per class): [0.4966443  0.79452055 0.81497797]\n",
      "Recall score (Per class): [0.7628866  0.53703704 0.75819672]\n",
      "Epoch: 9/10\n",
      "Epoch [09/10] Step [000/127]: cls_loss=0.0383\n",
      "Epoch [09/10] Step [005/127]: cls_loss=0.0035\n",
      "Epoch [09/10] Step [010/127]: cls_loss=0.2055\n",
      "Epoch [09/10] Step [015/127]: cls_loss=0.0087\n",
      "Epoch [09/10] Step [020/127]: cls_loss=0.0069\n",
      "Epoch [09/10] Step [025/127]: cls_loss=0.0354\n",
      "Epoch [09/10] Step [030/127]: cls_loss=0.1500\n",
      "Epoch [09/10] Step [035/127]: cls_loss=0.0053\n",
      "Epoch [09/10] Step [040/127]: cls_loss=0.0044\n",
      "Epoch [09/10] Step [045/127]: cls_loss=0.0047\n",
      "Epoch [09/10] Step [050/127]: cls_loss=0.0031\n",
      "Epoch [09/10] Step [055/127]: cls_loss=0.0149\n",
      "Epoch [09/10] Step [060/127]: cls_loss=0.0211\n",
      "Epoch [09/10] Step [065/127]: cls_loss=0.0246\n",
      "Epoch [09/10] Step [070/127]: cls_loss=0.0117\n",
      "Epoch [09/10] Step [075/127]: cls_loss=0.2358\n",
      "Epoch [09/10] Step [080/127]: cls_loss=0.0091\n",
      "Epoch [09/10] Step [085/127]: cls_loss=0.0116\n",
      "Epoch [09/10] Step [090/127]: cls_loss=0.0799\n",
      "Epoch [09/10] Step [095/127]: cls_loss=0.0218\n",
      "Epoch [09/10] Step [100/127]: cls_loss=0.0046\n",
      "Epoch [09/10] Step [105/127]: cls_loss=0.0497\n",
      "Epoch [09/10] Step [110/127]: cls_loss=0.5348\n",
      "Epoch [09/10] Step [115/127]: cls_loss=0.0786\n",
      "Epoch [09/10] Step [120/127]: cls_loss=0.0523\n",
      "Epoch [09/10] Step [125/127]: cls_loss=0.0210\n",
      "At the end of Epoch: 9\n",
      "Validation loss:  1.0211585760116577\n",
      "Accuracy: 0.7438752783964365\n",
      "F1 score (Macro): 0.7133673610254055\n",
      "F1 score (Per class): [0.68202765 0.65240642 0.80566802]\n",
      "Precision score (Per class): [0.61666667 0.7721519  0.796     ]\n",
      "Recall score (Per class): [0.7628866  0.56481481 0.81557377]\n"
     ]
    }
   ],
   "source": [
    "src_encoder, bi_lstm_classifier = pretrain(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfeeb7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(src_encoder.state_dict(), 'model/src_encoder_12_11_bilstm.pth')\n",
    "torch.save(bi_lstm_classifier.state_dict(), 'model/bi_lstm_classifier_12_11_bilstm_attention.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25738671",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(src_encoder.state_dict(), 'model/src_encoder_12_11_class_weights.pth')\n",
    "torch.save(src_classifier.state_dict(), 'model/src_classifier_12_11_class_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d08508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(src_encoder.state_dict(), 'model/src_encoder_class_weights_oversampling.pth')\n",
    "torch.save(src_classifier.state_dict(), 'model/src_classifier_class_weights_oversampling.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c13d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(src_encoder.state_dict(), 'model/src_encoder_12_12_4.pth')\n",
    "torch.save(src_classifier.state_dict(), 'model/src_classifier_12_12_4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6b77b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(src_encoder.state_dict(), 'model/src_encoder.pth')\n",
    "torch.save(src_classifier.state_dict(), 'model/src_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8cdfdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_encoder.load_state_dict(torch.load(\"model/pretrain/src_encoder_12_11_bt_class_weights_epoch_0.pth\"))\n",
    "src_classifier.load_state_dict(torch.load(\"model/pretrain/src_classifier_12_11_bt_class_weights_epoch_0.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79b53036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tgt_encoder = BertEncoder().to(device)\n",
    "tgt_encoder.load_state_dict(src_encoder.state_dict())\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61e8d673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0251,  0.0127,  0.0212,  ..., -0.0266, -0.0276,  0.0049],\n",
      "        [-0.0043,  0.0267,  0.0187,  ...,  0.0135,  0.0216, -0.0220],\n",
      "        [-0.0293, -0.0226,  0.0256,  ...,  0.0311, -0.0212, -0.0123]])\n"
     ]
    }
   ],
   "source": [
    "#src_classifier_11\n",
    "for name, param in src_classifier.state_dict().items():\n",
    "    if name == 'classifier.weight':\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af680e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0251,  0.0127,  0.0212,  ..., -0.0266, -0.0276,  0.0049],\n",
      "        [-0.0043,  0.0267,  0.0187,  ...,  0.0135,  0.0216, -0.0220],\n",
      "        [-0.0293, -0.0226,  0.0256,  ...,  0.0311, -0.0212, -0.0123]])\n"
     ]
    }
   ],
   "source": [
    "#src_classifier_11_2\n",
    "for name, param in src_classifier.state_dict().items():\n",
    "    if name == 'classifier.weight':\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2df3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_iindices, french_clabels, french_ipreds = check_errors(tgt_encoder, src_classifier, french_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "059a41e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>stance</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>preds</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>819</td>\n",
       "      <td>984</td>\n",
       "      <td>Donc à Nice, on \"joue\" avec le #COVID19 en ess...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Donc Nice, on \"joue\" avec le COVID19 en essaya...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>824</td>\n",
       "      <td>990</td>\n",
       "      <td>Mauricette, veux-tu m'épouser ?\\n- T'es vaccin...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Mauricette, veux-tu m'pouser ? - T'es vaccin ?...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>825</td>\n",
       "      <td>991</td>\n",
       "      <td>Le seul pays qui se foire sur les vaccinations...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Le seul pays qui se foire sur les vaccinations...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>827</td>\n",
       "      <td>994</td>\n",
       "      <td>N'hésitez pas à vous faire vacciner : https://...</td>\n",
       "      <td>positive</td>\n",
       "      <td>N'hsitez pas vous faire vacciner</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>829</td>\n",
       "      <td>996</td>\n",
       "      <td>@ViteMaDose_off puisque astrazeneca n’est pas ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>puisque astrazeneca nest pas disponible aux mo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Unnamed: 0                                              tweet  \\\n",
       "376    819         984  Donc à Nice, on \"joue\" avec le #COVID19 en ess...   \n",
       "377    824         990  Mauricette, veux-tu m'épouser ?\\n- T'es vaccin...   \n",
       "378    825         991  Le seul pays qui se foire sur les vaccinations...   \n",
       "379    827         994  N'hésitez pas à vous faire vacciner : https://...   \n",
       "380    829         996  @ViteMaDose_off puisque astrazeneca n’est pas ...   \n",
       "\n",
       "       stance                                    processed_tweet     preds  \\\n",
       "376   neutral  Donc Nice, on \"joue\" avec le COVID19 en essaya...  positive   \n",
       "377  positive  Mauricette, veux-tu m'pouser ? - T'es vaccin ?...  negative   \n",
       "378  positive  Le seul pays qui se foire sur les vaccinations...  negative   \n",
       "379  positive                  N'hsitez pas vous faire vacciner   negative   \n",
       "380  positive  puisque astrazeneca nest pas disponible aux mo...  negative   \n",
       "\n",
       "       labels  \n",
       "376   neutral  \n",
       "377  positive  \n",
       "378  positive  \n",
       "379  positive  \n",
       "380  positive  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_incorrect_examples = vaccin_eu_french.iloc[french_iindices]\n",
    "french_incorrect_examples.reset_index(inplace=True)\n",
    "\n",
    "labels_inv_encoding = {\n",
    "    0: \"negative\",\n",
    "    1: \"neutral\",\n",
    "    2: \"positive\"\n",
    "}\n",
    "\n",
    "french_ipreds = pd.Series(french_ipreds)\n",
    "french_clables = pd.Series(french_clabels)\n",
    "french_incorrect_examples['preds'] = french_ipreds\n",
    "french_incorrect_examples['labels'] = french_clabels\n",
    "french_incorrect_examples['preds'] = french_incorrect_examples['preds'].map(labels_inv_encoding)\n",
    "french_incorrect_examples['labels'] = french_incorrect_examples['labels'].map(labels_inv_encoding)\n",
    "french_incorrect_examples.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a41af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_incorrect_examples.to_csv('processed_datasets/french_incorrect_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "284aad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  1.966426968574524\n",
      "Accuracy: 0.4489795918367347\n",
      "F1 score (Macro): 0.42400971292489986\n",
      "F1 score (Per class): [0.32793522 0.37681159 0.56728232]\n",
      "Precision score (Per class): [0.22562674 0.57777778 0.63421829]\n",
      "Recall score (Per class): [0.6        0.27956989 0.51312649]\n",
      "German Test:\n",
      "Validation loss:  1.4950343370437622\n",
      "Accuracy: 0.5327669902912622\n",
      "F1 score (Macro): 0.4788164594568169\n",
      "F1 score (Per class): [0.30392157 0.48101266 0.65151515]\n",
      "Precision score (Per class): [0.20666667 0.5170068  0.79840849]\n",
      "Recall score (Per class): [0.57407407 0.44970414 0.55027422]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  2.1447253227233887\n",
      "Accuracy: 0.4247020585048754\n",
      "F1 score (Macro): 0.42398383226545033\n",
      "F1 score (Per class): [0.41338583 0.4338118  0.42475387]\n",
      "Precision score (Per class): [0.29411765 0.80473373 0.38035264]\n",
      "Recall score (Per class): [0.69536424 0.29694323 0.48089172]\n"
     ]
    }
   ],
   "source": [
    "print(\"French Test: \\n\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, french_dataloader)\n",
    "print(\"German Test:\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, german_dataloader)\n",
    "print(\"Italian Test: \\n\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, italian_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fca25b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  2.673867702484131\n",
      "Accuracy: 0.56062424969988\n",
      "F1 score (Macro): 0.5059018539262339\n",
      "F1 score (Per class): [0.31939163 0.55782313 0.6404908 ]\n",
      "Precision score (Per class): [0.328125   0.53074434 0.65909091]\n",
      "Recall score (Per class): [0.31111111 0.58781362 0.62291169]\n",
      "German Test:\n",
      "Validation loss:  1.8969080448150635\n",
      "Accuracy: 0.6468446601941747\n",
      "F1 score (Macro): 0.5025464720522431\n",
      "F1 score (Per class): [0.25301205 0.48309179 0.77153558]\n",
      "Precision score (Per class): [0.36206897 0.40816327 0.79078695]\n",
      "Recall score (Per class): [0.19444444 0.59171598 0.75319927]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  2.790982723236084\n",
      "Accuracy: 0.5222101841820151\n",
      "F1 score (Macro): 0.4756129016654886\n",
      "F1 score (Per class): [0.32472325 0.60839161 0.49372385]\n",
      "Precision score (Per class): [0.36666667 0.6525     0.43920596]\n",
      "Recall score (Per class): [0.29139073 0.569869   0.56369427]\n"
     ]
    }
   ],
   "source": [
    "# contrastive loss on backtranslated data\n",
    "print(\"French Test: \\n\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, french_dataloader)\n",
    "print(\"German Test:\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, german_dataloader)\n",
    "print(\"Italian Test: \\n\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, italian_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe68e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  1.5251106023788452\n",
      "Accuracy: 0.5414165666266506\n",
      "F1 score (Macro): 0.4365032698567224\n",
      "F1 score (Per class): [0.272      0.36040609 0.67710372]\n",
      "Precision score (Per class): [0.29565217 0.6173913  0.57379768]\n",
      "Recall score (Per class): [0.25185185 0.25448029 0.82577566]\n",
      "German Test:\n",
      "Validation loss:  0.9986060261726379\n",
      "Accuracy: 0.6929611650485437\n",
      "F1 score (Macro): 0.49419930366650383\n",
      "F1 score (Per class): [0.25988701 0.40625    0.81646091]\n",
      "Precision score (Per class): [0.33333333 0.59770115 0.74251497]\n",
      "Recall score (Per class): [0.21296296 0.30769231 0.90676417]\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  2.0272860527038574\n",
      "Accuracy: 0.39978331527627303\n",
      "F1 score (Macro): 0.3516716068985614\n",
      "F1 score (Per class): [0.27540984 0.27106227 0.50854271]\n",
      "Precision score (Per class): [0.27272727 0.84090909 0.37151248]\n",
      "Recall score (Per class): [0.2781457  0.16157205 0.80573248]\n"
     ]
    }
   ],
   "source": [
    "print(\"French Test: \\n\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, french_dataloader)\n",
    "print(\"German Test:\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, german_dataloader)\n",
    "print(\"Italian Test: \\n\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, italian_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6f28418",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfb757b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(src_encoder, discriminator,\n",
    "          src_classifier, src_data_loader, tgt_data_train_loader):\n",
    "    \"\"\"Train encoder for target language.\"\"\"\n",
    "    \n",
    "    global tgt_encoder\n",
    "    \n",
    "    src_encoder.eval()\n",
    "#     bi_lstm_classifier.eval()\n",
    "    src_classifier.eval()\n",
    "    tgt_encoder.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    # setup criterion and optimizer\n",
    "#     BCELoss = nn.BCELoss()\n",
    "    BCELoss = nn.BCEWithLogitsLoss()\n",
    "    KLDivLoss = nn.KLDivLoss(reduction='batchmean')\n",
    "    optimizer_G = torch.optim.AdamW(tgt_encoder.parameters(), lr=d_learning_rate)\n",
    "    optimizer_D = torch.optim.AdamW(discriminator.parameters(), lr=d_learning_rate)\n",
    "    len_data_loader = min(len(src_data_loader), len(tgt_data_train_loader))\n",
    "\n",
    "    for epoch in range(adapt_epochs):\n",
    "        print(f\"Epoch: {epoch}/{adapt_epochs}\")\n",
    "        # zip source and target data pair\n",
    "        data_zip = enumerate(zip(src_data_loader, tgt_data_train_loader))\n",
    "        for step, ((inputs_src, src_mask, _), (inputs_tgt, tgt_mask)) in data_zip:\n",
    "            inputs_src = inputs_src.to(device)\n",
    "            src_mask = src_mask.to(device)\n",
    "\n",
    "            inputs_tgt = inputs_tgt.to(device)\n",
    "            tgt_mask = tgt_mask.to(device)\n",
    "\n",
    "            # zero gradients for optimizer\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # extract and concat features\n",
    "            with torch.no_grad():\n",
    "                feat_src = src_encoder(inputs_src, src_mask)\n",
    "#                 feat_src_cls = feat_src[:, 0, :]\n",
    "            feat_src_tgt = tgt_encoder(inputs_src, src_mask)\n",
    "#             feat_src_tgt_cls = feat_src_tgt[:, 0, :]\n",
    "            feat_tgt = tgt_encoder(inputs_tgt, tgt_mask)\n",
    "#             feat_tgt_cls = feat_tgt[:, 0, :]\n",
    "#             feat_concat = torch.cat((feat_src_tgt_cls, feat_tgt_cls), 0)\n",
    "            feat_concat = torch.cat((feat_src_tgt, feat_tgt), 0)\n",
    "\n",
    "            # predict on discriminator\n",
    "            pred_concat = discriminator(feat_concat.detach())\n",
    "\n",
    "            label_src = torch.ones(feat_src_tgt.size(0)).to(device).unsqueeze(1)\n",
    "            label_tgt = torch.zeros(feat_tgt.size(0)).to(device).unsqueeze(1)\n",
    "            label_concat = torch.cat((label_src, label_tgt), 0)\n",
    "\n",
    "            # compute loss for discriminator\n",
    "            dis_loss = BCELoss(pred_concat, label_concat)\n",
    "            dis_loss.backward()\n",
    "\n",
    "            for p in discriminator.parameters():\n",
    "                p.data.clamp_(-clip_value, clip_value)\n",
    "            # optimize discriminator\n",
    "            optimizer_D.step()\n",
    "\n",
    "            pred_cls = torch.squeeze(pred_concat.max(1)[1])\n",
    "            acc = (pred_cls == label_concat).float().mean()\n",
    "\n",
    "            # zero gradients for optimizer\n",
    "            optimizer_G.zero_grad()\n",
    "            T = temperature\n",
    "\n",
    "            # predict on discriminator\n",
    "#             pred_tgt = discriminator(feat_tgt_cls)\n",
    "            pred_tgt = discriminator(feat_tgt)\n",
    "\n",
    "            # logits for KL-divergence\n",
    "            with torch.no_grad():\n",
    "                src_prob = F.softmax(src_classifier(feat_src) / T, dim=-1)\n",
    "            tgt_prob = F.log_softmax(src_classifier(feat_src_tgt) / T, dim=-1)\n",
    "            kd_loss = KLDivLoss(tgt_prob, src_prob.detach()) * T * T\n",
    "\n",
    "            # compute loss for target encoder\n",
    "            try:\n",
    "                gen_loss = BCELoss(pred_tgt.to(device), torch.ones(pred_tgt.size()).to(device))\n",
    "            except:\n",
    "                print(label_src.shape, \"\\n\")\n",
    "                print(feat_src_tgt.shape, \"\\n\")\n",
    "                print(feat_tgt.shape, \"\\n\")\n",
    "            loss_tgt = alpha * gen_loss + beta * kd_loss\n",
    "            #loss_tgt = gen_loss\n",
    "            loss_tgt.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(tgt_encoder.parameters(), max_grad_norm)\n",
    "            # optimize target encoder\n",
    "            optimizer_G.step()\n",
    "\n",
    "            if (step) % adapt_log_step == 0:\n",
    "                print(\"Epoch [%.2d/%.2d] Step [%.3d/%.3d]: \"\n",
    "                      \"acc=%.4f g_loss=%.4f d_loss=%.4f kd_loss=%.4f\"\n",
    "                      % (epoch,\n",
    "                         adapt_epochs,\n",
    "                         step,\n",
    "                         len_data_loader,\n",
    "                         acc.item(),\n",
    "                         gen_loss.item(),\n",
    "                         dis_loss.item(),\n",
    "                         kd_loss.item()))\n",
    "#         print(\"French Test: \\n\")\n",
    "#         evaluate_validation(tgt_encoder, src_classifier, french_dataloader)\n",
    "#         print(\"German Test:\")\n",
    "#         evaluate_validation(tgt_encoder, src_classifier, german_dataloader)\n",
    "        print(\"Italian Test: \\n\")\n",
    "        evaluate_validation(tgt_encoder, src_classifier, italian_dataloader)\n",
    "        torch.save(tgt_encoder.state_dict(), f\"model/adapt/italian/tgt_encoder_12_11_bt_class_weights_epoch_{epoch}.pth\")\n",
    "        \n",
    "\n",
    "    return tgt_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9823903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10\n",
      "Epoch [00/10] Step [000/161]: acc=0.5000 g_loss=0.4752 d_loss=0.7234 kd_loss=0.0269\n",
      "Epoch [00/10] Step [005/161]: acc=0.5000 g_loss=0.4757 d_loss=0.7236 kd_loss=0.0180\n",
      "Epoch [00/10] Step [010/161]: acc=0.5000 g_loss=0.4760 d_loss=0.7235 kd_loss=0.0462\n",
      "Epoch [00/10] Step [015/161]: acc=0.5000 g_loss=0.4763 d_loss=0.7232 kd_loss=0.0384\n",
      "Epoch [00/10] Step [020/161]: acc=0.5000 g_loss=0.4767 d_loss=0.7231 kd_loss=0.0223\n",
      "Epoch [00/10] Step [025/161]: acc=0.5000 g_loss=0.4770 d_loss=0.7231 kd_loss=0.0172\n",
      "Epoch [00/10] Step [030/161]: acc=0.5000 g_loss=0.4772 d_loss=0.7230 kd_loss=0.0331\n",
      "Epoch [00/10] Step [035/161]: acc=0.5000 g_loss=0.4776 d_loss=0.7230 kd_loss=0.0157\n",
      "Epoch [00/10] Step [040/161]: acc=0.5000 g_loss=0.4780 d_loss=0.7227 kd_loss=0.0270\n",
      "Epoch [00/10] Step [045/161]: acc=0.5000 g_loss=0.4782 d_loss=0.7225 kd_loss=0.0345\n",
      "Epoch [00/10] Step [050/161]: acc=0.5000 g_loss=0.4784 d_loss=0.7225 kd_loss=0.0393\n",
      "Epoch [00/10] Step [055/161]: acc=0.5000 g_loss=0.4785 d_loss=0.7226 kd_loss=0.0378\n",
      "Epoch [00/10] Step [060/161]: acc=0.5000 g_loss=0.4793 d_loss=0.7223 kd_loss=0.0307\n",
      "Epoch [00/10] Step [065/161]: acc=0.5000 g_loss=0.4798 d_loss=0.7221 kd_loss=0.0268\n",
      "Epoch [00/10] Step [070/161]: acc=0.5000 g_loss=0.4805 d_loss=0.7217 kd_loss=0.0341\n",
      "Epoch [00/10] Step [075/161]: acc=0.5000 g_loss=0.4813 d_loss=0.7215 kd_loss=0.0238\n",
      "Epoch [00/10] Step [080/161]: acc=0.5000 g_loss=0.4820 d_loss=0.7215 kd_loss=0.0254\n",
      "Epoch [00/10] Step [085/161]: acc=0.5000 g_loss=0.4830 d_loss=0.7209 kd_loss=0.0595\n",
      "Epoch [00/10] Step [090/161]: acc=0.5000 g_loss=0.4837 d_loss=0.7206 kd_loss=0.0265\n",
      "Epoch [00/10] Step [095/161]: acc=0.5000 g_loss=0.4844 d_loss=0.7206 kd_loss=0.0428\n",
      "Epoch [00/10] Step [100/161]: acc=0.5000 g_loss=0.4856 d_loss=0.7200 kd_loss=0.0269\n",
      "Epoch [00/10] Step [105/161]: acc=0.5000 g_loss=0.4868 d_loss=0.7196 kd_loss=0.0353\n",
      "Epoch [00/10] Step [110/161]: acc=0.5000 g_loss=0.4870 d_loss=0.7195 kd_loss=0.0363\n",
      "Epoch [00/10] Step [115/161]: acc=0.5000 g_loss=0.4875 d_loss=0.7198 kd_loss=0.0277\n",
      "Epoch [00/10] Step [120/161]: acc=0.5000 g_loss=0.4893 d_loss=0.7189 kd_loss=0.0317\n",
      "Epoch [00/10] Step [125/161]: acc=0.5000 g_loss=0.4896 d_loss=0.7190 kd_loss=0.0426\n",
      "Epoch [00/10] Step [130/161]: acc=0.5000 g_loss=0.4909 d_loss=0.7183 kd_loss=0.0234\n",
      "Epoch [00/10] Step [135/161]: acc=0.5000 g_loss=0.4910 d_loss=0.7186 kd_loss=0.0587\n",
      "Epoch [00/10] Step [140/161]: acc=0.5000 g_loss=0.4920 d_loss=0.7183 kd_loss=0.0246\n",
      "Epoch [00/10] Step [145/161]: acc=0.5000 g_loss=0.4931 d_loss=0.7179 kd_loss=0.0277\n",
      "Epoch [00/10] Step [150/161]: acc=0.5000 g_loss=0.4941 d_loss=0.7177 kd_loss=0.0212\n",
      "Epoch [00/10] Step [155/161]: acc=0.5000 g_loss=0.4947 d_loss=0.7164 kd_loss=0.0322\n",
      "Epoch [00/10] Step [160/161]: acc=0.2558 g_loss=0.4947 d_loss=0.6081 kd_loss=0.0459\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.1549046039581299\n",
      "Accuracy: 0.4593716143011918\n",
      "F1 score (Macro): 0.43943937737193267\n",
      "F1 score (Per class): [0.34146341 0.48255814 0.49429658]\n",
      "Precision score (Per class): [0.28899083 0.72173913 0.41052632]\n",
      "Recall score (Per class): [0.41721854 0.36244541 0.62101911]\n",
      "Epoch: 1/10\n",
      "Epoch [01/10] Step [000/161]: acc=0.5000 g_loss=0.4963 d_loss=0.7163 kd_loss=0.0405\n",
      "Epoch [01/10] Step [005/161]: acc=0.5000 g_loss=0.4989 d_loss=0.7152 kd_loss=0.0184\n",
      "Epoch [01/10] Step [010/161]: acc=0.5000 g_loss=0.4979 d_loss=0.7162 kd_loss=0.0154\n",
      "Epoch [01/10] Step [015/161]: acc=0.5000 g_loss=0.4980 d_loss=0.7146 kd_loss=0.0094\n",
      "Epoch [01/10] Step [020/161]: acc=0.5000 g_loss=0.4983 d_loss=0.7143 kd_loss=0.0100\n",
      "Epoch [01/10] Step [025/161]: acc=0.5000 g_loss=0.4947 d_loss=0.7179 kd_loss=0.0097\n",
      "Epoch [01/10] Step [030/161]: acc=0.5000 g_loss=0.4913 d_loss=0.7199 kd_loss=0.0140\n",
      "Epoch [01/10] Step [035/161]: acc=0.5000 g_loss=0.4893 d_loss=0.7202 kd_loss=0.0081\n",
      "Epoch [01/10] Step [040/161]: acc=0.5000 g_loss=0.4878 d_loss=0.7204 kd_loss=0.0082\n",
      "Epoch [01/10] Step [045/161]: acc=0.5000 g_loss=0.4866 d_loss=0.7200 kd_loss=0.0101\n",
      "Epoch [01/10] Step [050/161]: acc=0.5000 g_loss=0.4858 d_loss=0.7192 kd_loss=0.0097\n",
      "Epoch [01/10] Step [055/161]: acc=0.5000 g_loss=0.4829 d_loss=0.7209 kd_loss=0.0102\n",
      "Epoch [01/10] Step [060/161]: acc=0.5000 g_loss=0.4835 d_loss=0.7201 kd_loss=0.0210\n",
      "Epoch [01/10] Step [065/161]: acc=0.5000 g_loss=0.4817 d_loss=0.7211 kd_loss=0.0083\n",
      "Epoch [01/10] Step [070/161]: acc=0.5000 g_loss=0.4814 d_loss=0.7212 kd_loss=0.0075\n",
      "Epoch [01/10] Step [075/161]: acc=0.5000 g_loss=0.4819 d_loss=0.7206 kd_loss=0.0053\n",
      "Epoch [01/10] Step [080/161]: acc=0.5000 g_loss=0.4811 d_loss=0.7219 kd_loss=0.0081\n",
      "Epoch [01/10] Step [085/161]: acc=0.5000 g_loss=0.4812 d_loss=0.7218 kd_loss=0.0155\n",
      "Epoch [01/10] Step [090/161]: acc=0.5000 g_loss=0.4815 d_loss=0.7216 kd_loss=0.0122\n",
      "Epoch [01/10] Step [095/161]: acc=0.5000 g_loss=0.4814 d_loss=0.7213 kd_loss=0.0098\n",
      "Epoch [01/10] Step [100/161]: acc=0.5000 g_loss=0.4814 d_loss=0.7216 kd_loss=0.0096\n",
      "Epoch [01/10] Step [105/161]: acc=0.5000 g_loss=0.4821 d_loss=0.7212 kd_loss=0.0078\n",
      "Epoch [01/10] Step [110/161]: acc=0.5000 g_loss=0.4827 d_loss=0.7211 kd_loss=0.0100\n",
      "Epoch [01/10] Step [115/161]: acc=0.5000 g_loss=0.4832 d_loss=0.7205 kd_loss=0.0128\n",
      "Epoch [01/10] Step [120/161]: acc=0.5000 g_loss=0.4831 d_loss=0.7211 kd_loss=0.0111\n",
      "Epoch [01/10] Step [125/161]: acc=0.5000 g_loss=0.4834 d_loss=0.7213 kd_loss=0.0123\n",
      "Epoch [01/10] Step [130/161]: acc=0.5000 g_loss=0.4839 d_loss=0.7209 kd_loss=0.0160\n",
      "Epoch [01/10] Step [135/161]: acc=0.5000 g_loss=0.4842 d_loss=0.7211 kd_loss=0.0160\n",
      "Epoch [01/10] Step [140/161]: acc=0.5000 g_loss=0.4845 d_loss=0.7203 kd_loss=0.0294\n",
      "Epoch [01/10] Step [145/161]: acc=0.5000 g_loss=0.4861 d_loss=0.7198 kd_loss=0.0124\n",
      "Epoch [01/10] Step [150/161]: acc=0.5000 g_loss=0.4866 d_loss=0.7196 kd_loss=0.0077\n",
      "Epoch [01/10] Step [155/161]: acc=0.5000 g_loss=0.4860 d_loss=0.7201 kd_loss=0.0127\n",
      "Epoch [01/10] Step [160/161]: acc=0.2558 g_loss=0.4841 d_loss=0.6061 kd_loss=0.0124\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.1329710483551025\n",
      "Accuracy: 0.4539544962080173\n",
      "F1 score (Macro): 0.43895562574389274\n",
      "F1 score (Per class): [0.36315789 0.46715328 0.4865557 ]\n",
      "Precision score (Per class): [0.30131004 0.70484581 0.40685225]\n",
      "Recall score (Per class): [0.45695364 0.34934498 0.60509554]\n",
      "Epoch: 2/10\n",
      "Epoch [02/10] Step [000/161]: acc=0.5000 g_loss=0.4841 d_loss=0.7221 kd_loss=0.0130\n",
      "Epoch [02/10] Step [005/161]: acc=0.5000 g_loss=0.4860 d_loss=0.7202 kd_loss=0.0083\n",
      "Epoch [02/10] Step [010/161]: acc=0.5000 g_loss=0.4847 d_loss=0.7213 kd_loss=0.0090\n",
      "Epoch [02/10] Step [015/161]: acc=0.5000 g_loss=0.4847 d_loss=0.7205 kd_loss=0.0254\n",
      "Epoch [02/10] Step [020/161]: acc=0.5000 g_loss=0.4864 d_loss=0.7200 kd_loss=0.0103\n",
      "Epoch [02/10] Step [025/161]: acc=0.5000 g_loss=0.4858 d_loss=0.7202 kd_loss=0.0097\n",
      "Epoch [02/10] Step [030/161]: acc=0.5000 g_loss=0.4859 d_loss=0.7203 kd_loss=0.0123\n",
      "Epoch [02/10] Step [035/161]: acc=0.5000 g_loss=0.4858 d_loss=0.7201 kd_loss=0.0142\n",
      "Epoch [02/10] Step [040/161]: acc=0.5000 g_loss=0.4855 d_loss=0.7196 kd_loss=0.0194\n",
      "Epoch [02/10] Step [045/161]: acc=0.5000 g_loss=0.4854 d_loss=0.7188 kd_loss=0.0163\n",
      "Epoch [02/10] Step [050/161]: acc=0.5000 g_loss=0.4814 d_loss=0.7208 kd_loss=0.0260\n",
      "Epoch [02/10] Step [055/161]: acc=0.5000 g_loss=0.4819 d_loss=0.7214 kd_loss=0.0215\n",
      "Epoch [02/10] Step [060/161]: acc=0.5000 g_loss=0.4816 d_loss=0.7214 kd_loss=0.0153\n",
      "Epoch [02/10] Step [065/161]: acc=0.5000 g_loss=0.4808 d_loss=0.7215 kd_loss=0.0166\n",
      "Epoch [02/10] Step [070/161]: acc=0.5000 g_loss=0.4811 d_loss=0.7224 kd_loss=0.0218\n",
      "Epoch [02/10] Step [075/161]: acc=0.5000 g_loss=0.4830 d_loss=0.7201 kd_loss=0.0091\n",
      "Epoch [02/10] Step [080/161]: acc=0.5000 g_loss=0.4828 d_loss=0.7216 kd_loss=0.0133\n",
      "Epoch [02/10] Step [085/161]: acc=0.5000 g_loss=0.4842 d_loss=0.7214 kd_loss=0.0161\n",
      "Epoch [02/10] Step [090/161]: acc=0.5000 g_loss=0.4852 d_loss=0.7204 kd_loss=0.0178\n",
      "Epoch [02/10] Step [095/161]: acc=0.5000 g_loss=0.4854 d_loss=0.7203 kd_loss=0.0216\n",
      "Epoch [02/10] Step [100/161]: acc=0.5000 g_loss=0.4863 d_loss=0.7201 kd_loss=0.0167\n",
      "Epoch [02/10] Step [105/161]: acc=0.5000 g_loss=0.4871 d_loss=0.7196 kd_loss=0.0117\n",
      "Epoch [02/10] Step [110/161]: acc=0.5000 g_loss=0.4877 d_loss=0.7197 kd_loss=0.0106\n",
      "Epoch [02/10] Step [115/161]: acc=0.5000 g_loss=0.4888 d_loss=0.7189 kd_loss=0.0104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [02/10] Step [120/161]: acc=0.5000 g_loss=0.4887 d_loss=0.7200 kd_loss=0.0321\n",
      "Epoch [02/10] Step [125/161]: acc=0.5000 g_loss=0.4888 d_loss=0.7197 kd_loss=0.0162\n",
      "Epoch [02/10] Step [130/161]: acc=0.5000 g_loss=0.4892 d_loss=0.7198 kd_loss=0.0062\n",
      "Epoch [02/10] Step [135/161]: acc=0.5000 g_loss=0.4886 d_loss=0.7202 kd_loss=0.0188\n",
      "Epoch [02/10] Step [140/161]: acc=0.5000 g_loss=0.4900 d_loss=0.7187 kd_loss=0.0203\n",
      "Epoch [02/10] Step [145/161]: acc=0.5000 g_loss=0.4909 d_loss=0.7187 kd_loss=0.0134\n",
      "Epoch [02/10] Step [150/161]: acc=0.5000 g_loss=0.4911 d_loss=0.7189 kd_loss=0.0117\n",
      "Epoch [02/10] Step [155/161]: acc=0.5000 g_loss=0.4912 d_loss=0.7184 kd_loss=0.0159\n",
      "Epoch [02/10] Step [160/161]: acc=0.2558 g_loss=0.4855 d_loss=0.6095 kd_loss=0.0141\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.157983422279358\n",
      "Accuracy: 0.437703141928494\n",
      "F1 score (Macro): 0.42793598572841135\n",
      "F1 score (Per class): [0.35665914 0.4719764  0.45517241]\n",
      "Precision score (Per class): [0.27054795 0.72727273 0.40145985]\n",
      "Recall score (Per class): [0.52317881 0.34934498 0.52547771]\n",
      "Epoch: 3/10\n",
      "Epoch [03/10] Step [000/161]: acc=0.5000 g_loss=0.4864 d_loss=0.7224 kd_loss=0.0142\n",
      "Epoch [03/10] Step [005/161]: acc=0.5000 g_loss=0.4895 d_loss=0.7200 kd_loss=0.0050\n",
      "Epoch [03/10] Step [010/161]: acc=0.5000 g_loss=0.4871 d_loss=0.7209 kd_loss=0.0063\n",
      "Epoch [03/10] Step [015/161]: acc=0.5000 g_loss=0.4865 d_loss=0.7208 kd_loss=0.0116\n",
      "Epoch [03/10] Step [020/161]: acc=0.5000 g_loss=0.4878 d_loss=0.7196 kd_loss=0.0074\n",
      "Epoch [03/10] Step [025/161]: acc=0.5000 g_loss=0.4868 d_loss=0.7209 kd_loss=0.0097\n",
      "Epoch [03/10] Step [030/161]: acc=0.5000 g_loss=0.4843 d_loss=0.7221 kd_loss=0.0192\n",
      "Epoch [03/10] Step [035/161]: acc=0.5000 g_loss=0.4853 d_loss=0.7210 kd_loss=0.0078\n",
      "Epoch [03/10] Step [040/161]: acc=0.5000 g_loss=0.4844 d_loss=0.7202 kd_loss=0.0166\n",
      "Epoch [03/10] Step [045/161]: acc=0.5000 g_loss=0.4844 d_loss=0.7193 kd_loss=0.0221\n",
      "Epoch [03/10] Step [050/161]: acc=0.5000 g_loss=0.4811 d_loss=0.7206 kd_loss=0.0050\n",
      "Epoch [03/10] Step [055/161]: acc=0.5000 g_loss=0.4809 d_loss=0.7212 kd_loss=0.0259\n",
      "Epoch [03/10] Step [060/161]: acc=0.5000 g_loss=0.4825 d_loss=0.7201 kd_loss=0.0113\n",
      "Epoch [03/10] Step [065/161]: acc=0.5000 g_loss=0.4808 d_loss=0.7213 kd_loss=0.0056\n",
      "Epoch [03/10] Step [070/161]: acc=0.5000 g_loss=0.4794 d_loss=0.7228 kd_loss=0.0178\n",
      "Epoch [03/10] Step [075/161]: acc=0.5000 g_loss=0.4837 d_loss=0.7187 kd_loss=0.0110\n",
      "Epoch [03/10] Step [080/161]: acc=0.5000 g_loss=0.4832 d_loss=0.7208 kd_loss=0.0099\n",
      "Epoch [03/10] Step [085/161]: acc=0.5000 g_loss=0.4844 d_loss=0.7207 kd_loss=0.0239\n",
      "Epoch [03/10] Step [090/161]: acc=0.5000 g_loss=0.4868 d_loss=0.7196 kd_loss=0.0181\n",
      "Epoch [03/10] Step [095/161]: acc=0.5000 g_loss=0.4871 d_loss=0.7190 kd_loss=0.0102\n",
      "Epoch [03/10] Step [100/161]: acc=0.5000 g_loss=0.4883 d_loss=0.7189 kd_loss=0.0070\n",
      "Epoch [03/10] Step [105/161]: acc=0.5000 g_loss=0.4895 d_loss=0.7182 kd_loss=0.0094\n",
      "Epoch [03/10] Step [110/161]: acc=0.5000 g_loss=0.4907 d_loss=0.7185 kd_loss=0.0105\n",
      "Epoch [03/10] Step [115/161]: acc=0.5000 g_loss=0.4915 d_loss=0.7177 kd_loss=0.0097\n",
      "Epoch [03/10] Step [120/161]: acc=0.5000 g_loss=0.4927 d_loss=0.7184 kd_loss=0.0108\n",
      "Epoch [03/10] Step [125/161]: acc=0.5000 g_loss=0.4922 d_loss=0.7186 kd_loss=0.0160\n",
      "Epoch [03/10] Step [130/161]: acc=0.5000 g_loss=0.4934 d_loss=0.7183 kd_loss=0.0148\n",
      "Epoch [03/10] Step [135/161]: acc=0.5000 g_loss=0.4926 d_loss=0.7191 kd_loss=0.0092\n",
      "Epoch [03/10] Step [140/161]: acc=0.5000 g_loss=0.4930 d_loss=0.7182 kd_loss=0.0137\n",
      "Epoch [03/10] Step [145/161]: acc=0.5000 g_loss=0.4969 d_loss=0.7169 kd_loss=0.0144\n",
      "Epoch [03/10] Step [150/161]: acc=0.5000 g_loss=0.4953 d_loss=0.7179 kd_loss=0.0069\n",
      "Epoch [03/10] Step [155/161]: acc=0.5000 g_loss=0.4956 d_loss=0.7168 kd_loss=0.0202\n",
      "Epoch [03/10] Step [160/161]: acc=0.2558 g_loss=0.4905 d_loss=0.6110 kd_loss=0.0103\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.1464766263961792\n",
      "Accuracy: 0.4593716143011918\n",
      "F1 score (Macro): 0.44439653367533055\n",
      "F1 score (Per class): [0.36923077 0.46268657 0.50127226]\n",
      "Precision score (Per class): [0.30125523 0.73113208 0.41737288]\n",
      "Recall score (Per class): [0.47682119 0.33842795 0.62738854]\n",
      "Epoch: 4/10\n",
      "Epoch [04/10] Step [000/161]: acc=0.5000 g_loss=0.4920 d_loss=0.7213 kd_loss=0.0117\n",
      "Epoch [04/10] Step [005/161]: acc=0.5000 g_loss=0.4932 d_loss=0.7188 kd_loss=0.0242\n",
      "Epoch [04/10] Step [010/161]: acc=0.5000 g_loss=0.4924 d_loss=0.7201 kd_loss=0.0076\n",
      "Epoch [04/10] Step [015/161]: acc=0.5000 g_loss=0.4937 d_loss=0.7171 kd_loss=0.0117\n",
      "Epoch [04/10] Step [020/161]: acc=0.5000 g_loss=0.4945 d_loss=0.7178 kd_loss=0.0130\n",
      "Epoch [04/10] Step [025/161]: acc=0.5000 g_loss=0.4943 d_loss=0.7182 kd_loss=0.0084\n",
      "Epoch [04/10] Step [030/161]: acc=0.5000 g_loss=0.4909 d_loss=0.7196 kd_loss=0.0193\n",
      "Epoch [04/10] Step [035/161]: acc=0.5000 g_loss=0.4920 d_loss=0.7189 kd_loss=0.0272\n",
      "Epoch [04/10] Step [040/161]: acc=0.5000 g_loss=0.4915 d_loss=0.7171 kd_loss=0.0138\n",
      "Epoch [04/10] Step [045/161]: acc=0.5000 g_loss=0.4906 d_loss=0.7160 kd_loss=0.0127\n",
      "Epoch [04/10] Step [050/161]: acc=0.5000 g_loss=0.4857 d_loss=0.7187 kd_loss=0.0092\n",
      "Epoch [04/10] Step [055/161]: acc=0.5000 g_loss=0.4848 d_loss=0.7200 kd_loss=0.0167\n",
      "Epoch [04/10] Step [060/161]: acc=0.5000 g_loss=0.4847 d_loss=0.7191 kd_loss=0.0200\n",
      "Epoch [04/10] Step [065/161]: acc=0.5000 g_loss=0.4847 d_loss=0.7206 kd_loss=0.0108\n",
      "Epoch [04/10] Step [070/161]: acc=0.5000 g_loss=0.4819 d_loss=0.7240 kd_loss=0.0092\n",
      "Epoch [04/10] Step [075/161]: acc=0.5000 g_loss=0.4876 d_loss=0.7162 kd_loss=0.0184\n",
      "Epoch [04/10] Step [080/161]: acc=0.5000 g_loss=0.4882 d_loss=0.7198 kd_loss=0.0142\n",
      "Epoch [04/10] Step [085/161]: acc=0.5000 g_loss=0.4902 d_loss=0.7206 kd_loss=0.0052\n",
      "Epoch [04/10] Step [090/161]: acc=0.5000 g_loss=0.4945 d_loss=0.7171 kd_loss=0.0286\n",
      "Epoch [04/10] Step [095/161]: acc=0.5000 g_loss=0.4963 d_loss=0.7163 kd_loss=0.0111\n",
      "Epoch [04/10] Step [100/161]: acc=0.5000 g_loss=0.4985 d_loss=0.7159 kd_loss=0.0119\n",
      "Epoch [04/10] Step [105/161]: acc=0.5000 g_loss=0.4998 d_loss=0.7161 kd_loss=0.0217\n",
      "Epoch [04/10] Step [110/161]: acc=0.5000 g_loss=0.5015 d_loss=0.7158 kd_loss=0.0070\n",
      "Epoch [04/10] Step [115/161]: acc=0.5000 g_loss=0.5024 d_loss=0.7147 kd_loss=0.0176\n",
      "Epoch [04/10] Step [120/161]: acc=0.5000 g_loss=0.5030 d_loss=0.7160 kd_loss=0.0106\n",
      "Epoch [04/10] Step [125/161]: acc=0.5000 g_loss=0.5024 d_loss=0.7167 kd_loss=0.0128\n",
      "Epoch [04/10] Step [130/161]: acc=0.5000 g_loss=0.5005 d_loss=0.7174 kd_loss=0.0182\n",
      "Epoch [04/10] Step [135/161]: acc=0.5000 g_loss=0.4996 d_loss=0.7175 kd_loss=0.0105\n",
      "Epoch [04/10] Step [140/161]: acc=0.5000 g_loss=0.4997 d_loss=0.7174 kd_loss=0.0095\n",
      "Epoch [04/10] Step [145/161]: acc=0.5000 g_loss=0.5013 d_loss=0.7164 kd_loss=0.0172\n",
      "Epoch [04/10] Step [150/161]: acc=0.5000 g_loss=0.5010 d_loss=0.7166 kd_loss=0.0151\n",
      "Epoch [04/10] Step [155/161]: acc=0.5000 g_loss=0.4980 d_loss=0.7169 kd_loss=0.0157\n",
      "Epoch [04/10] Step [160/161]: acc=0.2558 g_loss=0.4923 d_loss=0.6130 kd_loss=0.0236\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.0828081369400024\n",
      "Accuracy: 0.4528710725893825\n",
      "F1 score (Macro): 0.43619017391572407\n",
      "F1 score (Per class): [0.342723   0.52910053 0.43674699]\n",
      "Precision score (Per class): [0.26545455 0.67114094 0.41428571]\n",
      "Recall score (Per class): [0.48344371 0.43668122 0.46178344]\n",
      "Epoch: 5/10\n",
      "Epoch [05/10] Step [000/161]: acc=0.5000 g_loss=0.4944 d_loss=0.7213 kd_loss=0.0238\n",
      "Epoch [05/10] Step [005/161]: acc=0.5000 g_loss=0.4968 d_loss=0.7173 kd_loss=0.0109\n",
      "Epoch [05/10] Step [010/161]: acc=0.5000 g_loss=0.4938 d_loss=0.7188 kd_loss=0.0182\n",
      "Epoch [05/10] Step [015/161]: acc=0.5000 g_loss=0.4953 d_loss=0.7153 kd_loss=0.0131\n",
      "Epoch [05/10] Step [020/161]: acc=0.5000 g_loss=0.4953 d_loss=0.7166 kd_loss=0.0110\n",
      "Epoch [05/10] Step [025/161]: acc=0.5000 g_loss=0.4951 d_loss=0.7159 kd_loss=0.0272\n",
      "Epoch [05/10] Step [030/161]: acc=0.5000 g_loss=0.4945 d_loss=0.7153 kd_loss=0.0084\n",
      "Epoch [05/10] Step [035/161]: acc=0.5000 g_loss=0.4931 d_loss=0.7160 kd_loss=0.0178\n",
      "Epoch [05/10] Step [040/161]: acc=0.5000 g_loss=0.4931 d_loss=0.7140 kd_loss=0.0112\n",
      "Epoch [05/10] Step [045/161]: acc=0.5000 g_loss=0.4909 d_loss=0.7143 kd_loss=0.0291\n",
      "Epoch [05/10] Step [050/161]: acc=0.5000 g_loss=0.4821 d_loss=0.7186 kd_loss=0.0123\n",
      "Epoch [05/10] Step [055/161]: acc=0.5000 g_loss=0.4838 d_loss=0.7206 kd_loss=0.0275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [05/10] Step [060/161]: acc=0.5000 g_loss=0.4835 d_loss=0.7180 kd_loss=0.0198\n",
      "Epoch [05/10] Step [065/161]: acc=0.5000 g_loss=0.4794 d_loss=0.7212 kd_loss=0.0203\n",
      "Epoch [05/10] Step [070/161]: acc=0.5000 g_loss=0.4807 d_loss=0.7228 kd_loss=0.0180\n",
      "Epoch [05/10] Step [075/161]: acc=0.5000 g_loss=0.4875 d_loss=0.7148 kd_loss=0.0076\n",
      "Epoch [05/10] Step [080/161]: acc=0.5000 g_loss=0.4861 d_loss=0.7187 kd_loss=0.0186\n",
      "Epoch [05/10] Step [085/161]: acc=0.5000 g_loss=0.4899 d_loss=0.7194 kd_loss=0.0074\n",
      "Epoch [05/10] Step [090/161]: acc=0.5000 g_loss=0.4934 d_loss=0.7170 kd_loss=0.0082\n",
      "Epoch [05/10] Step [095/161]: acc=0.5000 g_loss=0.4950 d_loss=0.7166 kd_loss=0.0133\n",
      "Epoch [05/10] Step [100/161]: acc=0.5000 g_loss=0.4998 d_loss=0.7147 kd_loss=0.0118\n",
      "Epoch [05/10] Step [105/161]: acc=0.5000 g_loss=0.5029 d_loss=0.7136 kd_loss=0.0136\n",
      "Epoch [05/10] Step [110/161]: acc=0.5000 g_loss=0.5048 d_loss=0.7146 kd_loss=0.0213\n",
      "Epoch [05/10] Step [115/161]: acc=0.5000 g_loss=0.5042 d_loss=0.7155 kd_loss=0.0178\n",
      "Epoch [05/10] Step [120/161]: acc=0.5000 g_loss=0.5090 d_loss=0.7151 kd_loss=0.0347\n",
      "Epoch [05/10] Step [125/161]: acc=0.5000 g_loss=0.5065 d_loss=0.7173 kd_loss=0.0120\n",
      "Epoch [05/10] Step [130/161]: acc=0.5000 g_loss=0.5078 d_loss=0.7155 kd_loss=0.0121\n",
      "Epoch [05/10] Step [135/161]: acc=0.5000 g_loss=0.5018 d_loss=0.7180 kd_loss=0.0072\n",
      "Epoch [05/10] Step [140/161]: acc=0.5000 g_loss=0.5044 d_loss=0.7179 kd_loss=0.0086\n",
      "Epoch [05/10] Step [145/161]: acc=0.5000 g_loss=0.5035 d_loss=0.7176 kd_loss=0.0112\n",
      "Epoch [05/10] Step [150/161]: acc=0.5000 g_loss=0.5006 d_loss=0.7185 kd_loss=0.0072\n",
      "Epoch [05/10] Step [155/161]: acc=0.5000 g_loss=0.5003 d_loss=0.7161 kd_loss=0.0153\n",
      "Epoch [05/10] Step [160/161]: acc=0.2558 g_loss=0.4911 d_loss=0.6126 kd_loss=0.0140\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.1570817232131958\n",
      "Accuracy: 0.4420368364030336\n",
      "F1 score (Macro): 0.43508844512222217\n",
      "F1 score (Per class): [0.38589981 0.51460362 0.4047619 ]\n",
      "Precision score (Per class): [0.26804124 0.70881226 0.43430657]\n",
      "Recall score (Per class): [0.68874172 0.40393013 0.37898089]\n",
      "Epoch: 6/10\n",
      "Epoch [06/10] Step [000/161]: acc=0.5000 g_loss=0.4933 d_loss=0.7219 kd_loss=0.0152\n",
      "Epoch [06/10] Step [005/161]: acc=0.5000 g_loss=0.4957 d_loss=0.7173 kd_loss=0.0141\n",
      "Epoch [06/10] Step [010/161]: acc=0.5000 g_loss=0.4919 d_loss=0.7190 kd_loss=0.0128\n",
      "Epoch [06/10] Step [015/161]: acc=0.5000 g_loss=0.4938 d_loss=0.7160 kd_loss=0.0148\n",
      "Epoch [06/10] Step [020/161]: acc=0.5000 g_loss=0.4931 d_loss=0.7167 kd_loss=0.0144\n",
      "Epoch [06/10] Step [025/161]: acc=0.5000 g_loss=0.4934 d_loss=0.7144 kd_loss=0.0062\n",
      "Epoch [06/10] Step [030/161]: acc=0.5000 g_loss=0.4943 d_loss=0.7139 kd_loss=0.0287\n",
      "Epoch [06/10] Step [035/161]: acc=0.5000 g_loss=0.4947 d_loss=0.7137 kd_loss=0.0098\n",
      "Epoch [06/10] Step [040/161]: acc=0.5000 g_loss=0.4893 d_loss=0.7147 kd_loss=0.0143\n",
      "Epoch [06/10] Step [045/161]: acc=0.5000 g_loss=0.4915 d_loss=0.7137 kd_loss=0.0107\n",
      "Epoch [06/10] Step [050/161]: acc=0.5000 g_loss=0.4776 d_loss=0.7220 kd_loss=0.0209\n",
      "Epoch [06/10] Step [055/161]: acc=0.5000 g_loss=0.4840 d_loss=0.7202 kd_loss=0.0131\n",
      "Epoch [06/10] Step [060/161]: acc=0.5000 g_loss=0.4853 d_loss=0.7180 kd_loss=0.0144\n",
      "Epoch [06/10] Step [065/161]: acc=0.5000 g_loss=0.4779 d_loss=0.7231 kd_loss=0.0103\n",
      "Epoch [06/10] Step [070/161]: acc=0.5000 g_loss=0.4774 d_loss=0.7260 kd_loss=0.0112\n",
      "Epoch [06/10] Step [075/161]: acc=0.5000 g_loss=0.4886 d_loss=0.7144 kd_loss=0.0093\n",
      "Epoch [06/10] Step [080/161]: acc=0.5000 g_loss=0.4871 d_loss=0.7191 kd_loss=0.0069\n",
      "Epoch [06/10] Step [085/161]: acc=0.5000 g_loss=0.4912 d_loss=0.7191 kd_loss=0.0169\n",
      "Epoch [06/10] Step [090/161]: acc=0.5000 g_loss=0.4968 d_loss=0.7173 kd_loss=0.0129\n",
      "Epoch [06/10] Step [095/161]: acc=0.5000 g_loss=0.5009 d_loss=0.7147 kd_loss=0.0062\n",
      "Epoch [06/10] Step [100/161]: acc=0.5000 g_loss=0.5064 d_loss=0.7144 kd_loss=0.0110\n",
      "Epoch [06/10] Step [105/161]: acc=0.5000 g_loss=0.5101 d_loss=0.7134 kd_loss=0.0136\n",
      "Epoch [06/10] Step [110/161]: acc=0.5000 g_loss=0.5148 d_loss=0.7119 kd_loss=0.0134\n",
      "Epoch [06/10] Step [115/161]: acc=0.5000 g_loss=0.5152 d_loss=0.7141 kd_loss=0.0154\n",
      "Epoch [06/10] Step [120/161]: acc=0.5000 g_loss=0.5142 d_loss=0.7168 kd_loss=0.0210\n",
      "Epoch [06/10] Step [125/161]: acc=0.5000 g_loss=0.5102 d_loss=0.7178 kd_loss=0.0229\n",
      "Epoch [06/10] Step [130/161]: acc=0.5000 g_loss=0.5160 d_loss=0.7147 kd_loss=0.0130\n",
      "Epoch [06/10] Step [135/161]: acc=0.5000 g_loss=0.5039 d_loss=0.7203 kd_loss=0.0263\n",
      "Epoch [06/10] Step [140/161]: acc=0.5000 g_loss=0.5067 d_loss=0.7183 kd_loss=0.0126\n",
      "Epoch [06/10] Step [145/161]: acc=0.5000 g_loss=0.5107 d_loss=0.7170 kd_loss=0.0241\n",
      "Epoch [06/10] Step [150/161]: acc=0.5000 g_loss=0.5040 d_loss=0.7192 kd_loss=0.0062\n",
      "Epoch [06/10] Step [155/161]: acc=0.5000 g_loss=0.5035 d_loss=0.7156 kd_loss=0.0194\n",
      "Epoch [06/10] Step [160/161]: acc=0.2558 g_loss=0.4955 d_loss=0.6159 kd_loss=0.0135\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.1261569261550903\n",
      "Accuracy: 0.4528710725893825\n",
      "F1 score (Macro): 0.4422632130224265\n",
      "F1 score (Per class): [0.36820084 0.5253078  0.433281  ]\n",
      "Precision score (Per class): [0.26911315 0.7032967  0.42724458]\n",
      "Recall score (Per class): [0.58278146 0.41921397 0.43949045]\n",
      "Epoch: 7/10\n",
      "Epoch [07/10] Step [000/161]: acc=0.5000 g_loss=0.4966 d_loss=0.7215 kd_loss=0.0114\n",
      "Epoch [07/10] Step [005/161]: acc=0.5000 g_loss=0.4977 d_loss=0.7175 kd_loss=0.0114\n",
      "Epoch [07/10] Step [010/161]: acc=0.5000 g_loss=0.4933 d_loss=0.7187 kd_loss=0.0078\n",
      "Epoch [07/10] Step [015/161]: acc=0.5000 g_loss=0.4943 d_loss=0.7169 kd_loss=0.0274\n",
      "Epoch [07/10] Step [020/161]: acc=0.5000 g_loss=0.4951 d_loss=0.7155 kd_loss=0.0080\n",
      "Epoch [07/10] Step [025/161]: acc=0.5000 g_loss=0.4959 d_loss=0.7124 kd_loss=0.0142\n",
      "Epoch [07/10] Step [030/161]: acc=0.5000 g_loss=0.4975 d_loss=0.7112 kd_loss=0.0096\n",
      "Epoch [07/10] Step [035/161]: acc=0.5000 g_loss=0.4989 d_loss=0.7113 kd_loss=0.0138\n",
      "Epoch [07/10] Step [040/161]: acc=0.5000 g_loss=0.4901 d_loss=0.7136 kd_loss=0.0126\n",
      "Epoch [07/10] Step [045/161]: acc=0.5000 g_loss=0.4905 d_loss=0.7150 kd_loss=0.0307\n",
      "Epoch [07/10] Step [050/161]: acc=0.5000 g_loss=0.4811 d_loss=0.7210 kd_loss=0.0115\n",
      "Epoch [07/10] Step [055/161]: acc=0.5000 g_loss=0.4840 d_loss=0.7208 kd_loss=0.0217\n",
      "Epoch [07/10] Step [060/161]: acc=0.5000 g_loss=0.4856 d_loss=0.7172 kd_loss=0.0383\n",
      "Epoch [07/10] Step [065/161]: acc=0.5000 g_loss=0.4840 d_loss=0.7211 kd_loss=0.0118\n",
      "Epoch [07/10] Step [070/161]: acc=0.5000 g_loss=0.4823 d_loss=0.7247 kd_loss=0.0159\n",
      "Epoch [07/10] Step [075/161]: acc=0.5000 g_loss=0.4941 d_loss=0.7139 kd_loss=0.0192\n",
      "Epoch [07/10] Step [080/161]: acc=0.5000 g_loss=0.4979 d_loss=0.7177 kd_loss=0.0169\n",
      "Epoch [07/10] Step [085/161]: acc=0.5000 g_loss=0.5057 d_loss=0.7142 kd_loss=0.0101\n",
      "Epoch [07/10] Step [090/161]: acc=0.5000 g_loss=0.5123 d_loss=0.7124 kd_loss=0.0205\n",
      "Epoch [07/10] Step [095/161]: acc=0.5000 g_loss=0.5128 d_loss=0.7137 kd_loss=0.0124\n",
      "Epoch [07/10] Step [100/161]: acc=0.5000 g_loss=0.5224 d_loss=0.7093 kd_loss=0.0147\n",
      "Epoch [07/10] Step [105/161]: acc=0.5000 g_loss=0.5229 d_loss=0.7104 kd_loss=0.0142\n",
      "Epoch [07/10] Step [110/161]: acc=0.5000 g_loss=0.5173 d_loss=0.7120 kd_loss=0.0252\n",
      "Epoch [07/10] Step [115/161]: acc=0.5000 g_loss=0.5161 d_loss=0.7158 kd_loss=0.0105\n",
      "Epoch [07/10] Step [120/161]: acc=0.5000 g_loss=0.5172 d_loss=0.7167 kd_loss=0.0107\n",
      "Epoch [07/10] Step [125/161]: acc=0.5000 g_loss=0.5063 d_loss=0.7198 kd_loss=0.0225\n",
      "Epoch [07/10] Step [130/161]: acc=0.5000 g_loss=0.5103 d_loss=0.7159 kd_loss=0.0139\n",
      "Epoch [07/10] Step [135/161]: acc=0.5000 g_loss=0.5045 d_loss=0.7192 kd_loss=0.0136\n",
      "Epoch [07/10] Step [140/161]: acc=0.5000 g_loss=0.5024 d_loss=0.7164 kd_loss=0.0157\n",
      "Epoch [07/10] Step [145/161]: acc=0.5000 g_loss=0.5070 d_loss=0.7135 kd_loss=0.0058\n",
      "Epoch [07/10] Step [150/161]: acc=0.5000 g_loss=0.5050 d_loss=0.7136 kd_loss=0.0112\n",
      "Epoch [07/10] Step [155/161]: acc=0.5000 g_loss=0.4982 d_loss=0.7172 kd_loss=0.0109\n",
      "Epoch [07/10] Step [160/161]: acc=0.2558 g_loss=0.4928 d_loss=0.6110 kd_loss=0.0114\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.207399606704712\n",
      "Accuracy: 0.42036836403033584\n",
      "F1 score (Macro): 0.41685065491359374\n",
      "F1 score (Per class): [0.37795276 0.43167702 0.44092219]\n",
      "Precision score (Per class): [0.26890756 0.74731183 0.40263158]\n",
      "Recall score (Per class): [0.63576159 0.30349345 0.48726115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10\n",
      "Epoch [08/10] Step [000/161]: acc=0.5000 g_loss=0.4909 d_loss=0.7211 kd_loss=0.0101\n",
      "Epoch [08/10] Step [005/161]: acc=0.5000 g_loss=0.4910 d_loss=0.7194 kd_loss=0.0147\n",
      "Epoch [08/10] Step [010/161]: acc=0.5000 g_loss=0.4890 d_loss=0.7185 kd_loss=0.0080\n",
      "Epoch [08/10] Step [015/161]: acc=0.5000 g_loss=0.4887 d_loss=0.7221 kd_loss=0.0100\n",
      "Epoch [08/10] Step [020/161]: acc=0.5000 g_loss=0.4929 d_loss=0.7183 kd_loss=0.0134\n",
      "Epoch [08/10] Step [025/161]: acc=0.5000 g_loss=0.5016 d_loss=0.7120 kd_loss=0.0150\n",
      "Epoch [08/10] Step [030/161]: acc=0.5000 g_loss=0.5008 d_loss=0.7128 kd_loss=0.0200\n",
      "Epoch [08/10] Step [035/161]: acc=0.5000 g_loss=0.5025 d_loss=0.7163 kd_loss=0.0082\n",
      "Epoch [08/10] Step [040/161]: acc=0.5000 g_loss=0.4962 d_loss=0.7204 kd_loss=0.0122\n",
      "Epoch [08/10] Step [045/161]: acc=0.5000 g_loss=0.4896 d_loss=0.7224 kd_loss=0.0199\n",
      "Epoch [08/10] Step [050/161]: acc=0.5000 g_loss=0.4956 d_loss=0.7157 kd_loss=0.0219\n",
      "Epoch [08/10] Step [055/161]: acc=0.5000 g_loss=0.4959 d_loss=0.7180 kd_loss=0.0164\n",
      "Epoch [08/10] Step [060/161]: acc=0.5000 g_loss=0.4975 d_loss=0.7166 kd_loss=0.0194\n",
      "Epoch [08/10] Step [065/161]: acc=0.5000 g_loss=0.4953 d_loss=0.7179 kd_loss=0.0195\n",
      "Epoch [08/10] Step [070/161]: acc=0.5000 g_loss=0.5011 d_loss=0.7158 kd_loss=0.0218\n",
      "Epoch [08/10] Step [075/161]: acc=0.5000 g_loss=0.5085 d_loss=0.7131 kd_loss=0.0120\n",
      "Epoch [08/10] Step [080/161]: acc=0.5000 g_loss=0.5082 d_loss=0.7171 kd_loss=0.0128\n",
      "Epoch [08/10] Step [085/161]: acc=0.5000 g_loss=0.5131 d_loss=0.7128 kd_loss=0.0172\n",
      "Epoch [08/10] Step [090/161]: acc=0.5000 g_loss=0.5149 d_loss=0.7134 kd_loss=0.0074\n",
      "Epoch [08/10] Step [095/161]: acc=0.5000 g_loss=0.5159 d_loss=0.7129 kd_loss=0.0102\n",
      "Epoch [08/10] Step [100/161]: acc=0.5000 g_loss=0.5156 d_loss=0.7138 kd_loss=0.0115\n",
      "Epoch [08/10] Step [105/161]: acc=0.5000 g_loss=0.5182 d_loss=0.7123 kd_loss=0.0193\n",
      "Epoch [08/10] Step [110/161]: acc=0.5000 g_loss=0.5153 d_loss=0.7121 kd_loss=0.0121\n",
      "Epoch [08/10] Step [115/161]: acc=0.5000 g_loss=0.5121 d_loss=0.7137 kd_loss=0.0250\n",
      "Epoch [08/10] Step [120/161]: acc=0.5000 g_loss=0.5104 d_loss=0.7157 kd_loss=0.0076\n",
      "Epoch [08/10] Step [125/161]: acc=0.5000 g_loss=0.5069 d_loss=0.7167 kd_loss=0.0071\n",
      "Epoch [08/10] Step [130/161]: acc=0.5000 g_loss=0.5018 d_loss=0.7178 kd_loss=0.0197\n",
      "Epoch [08/10] Step [135/161]: acc=0.5000 g_loss=0.5013 d_loss=0.7195 kd_loss=0.0068\n",
      "Epoch [08/10] Step [140/161]: acc=0.5000 g_loss=0.5001 d_loss=0.7144 kd_loss=0.0164\n",
      "Epoch [08/10] Step [145/161]: acc=0.5000 g_loss=0.5016 d_loss=0.7139 kd_loss=0.0224\n",
      "Epoch [08/10] Step [150/161]: acc=0.5000 g_loss=0.5049 d_loss=0.7117 kd_loss=0.0121\n",
      "Epoch [08/10] Step [155/161]: acc=0.5000 g_loss=0.4973 d_loss=0.7185 kd_loss=0.0305\n",
      "Epoch [08/10] Step [160/161]: acc=0.2558 g_loss=0.4995 d_loss=0.6078 kd_loss=0.0127\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.276151180267334\n",
      "Accuracy: 0.40411700975081255\n",
      "F1 score (Macro): 0.39357348270225856\n",
      "F1 score (Per class): [0.36403509 0.34505863 0.47162673]\n",
      "Precision score (Per class): [0.27213115 0.74100719 0.39039666]\n",
      "Recall score (Per class): [0.54966887 0.22489083 0.5955414 ]\n",
      "Epoch: 9/10\n",
      "Epoch [09/10] Step [000/161]: acc=0.5000 g_loss=0.4944 d_loss=0.7185 kd_loss=0.0113\n",
      "Epoch [09/10] Step [005/161]: acc=0.5000 g_loss=0.4955 d_loss=0.7184 kd_loss=0.0178\n",
      "Epoch [09/10] Step [010/161]: acc=0.5000 g_loss=0.5014 d_loss=0.7154 kd_loss=0.0175\n",
      "Epoch [09/10] Step [015/161]: acc=0.5000 g_loss=0.5154 d_loss=0.7089 kd_loss=0.0085\n",
      "Epoch [09/10] Step [020/161]: acc=0.5000 g_loss=0.5305 d_loss=0.7029 kd_loss=0.0111\n",
      "Epoch [09/10] Step [025/161]: acc=0.5000 g_loss=0.5237 d_loss=0.7110 kd_loss=0.0148\n",
      "Epoch [09/10] Step [030/161]: acc=0.5000 g_loss=0.4986 d_loss=0.7247 kd_loss=0.0212\n",
      "Epoch [09/10] Step [035/161]: acc=0.5000 g_loss=0.4946 d_loss=0.7312 kd_loss=0.0169\n",
      "Epoch [09/10] Step [040/161]: acc=0.5000 g_loss=0.4937 d_loss=0.7317 kd_loss=0.0160\n",
      "Epoch [09/10] Step [045/161]: acc=0.5000 g_loss=0.4930 d_loss=0.7257 kd_loss=0.0107\n",
      "Epoch [09/10] Step [050/161]: acc=0.5000 g_loss=0.5002 d_loss=0.7153 kd_loss=0.0199\n",
      "Epoch [09/10] Step [055/161]: acc=0.5000 g_loss=0.5006 d_loss=0.7164 kd_loss=0.0098\n",
      "Epoch [09/10] Step [060/161]: acc=0.5000 g_loss=0.5022 d_loss=0.7148 kd_loss=0.0207\n",
      "Epoch [09/10] Step [065/161]: acc=0.5000 g_loss=0.5013 d_loss=0.7139 kd_loss=0.0216\n",
      "Epoch [09/10] Step [070/161]: acc=0.5000 g_loss=0.5042 d_loss=0.7151 kd_loss=0.0236\n",
      "Epoch [09/10] Step [075/161]: acc=0.5000 g_loss=0.5048 d_loss=0.7152 kd_loss=0.0201\n",
      "Epoch [09/10] Step [080/161]: acc=0.5000 g_loss=0.5077 d_loss=0.7123 kd_loss=0.0149\n",
      "Epoch [09/10] Step [085/161]: acc=0.5000 g_loss=0.5089 d_loss=0.7116 kd_loss=0.0308\n",
      "Epoch [09/10] Step [090/161]: acc=0.5000 g_loss=0.5117 d_loss=0.7152 kd_loss=0.0200\n",
      "Epoch [09/10] Step [095/161]: acc=0.5000 g_loss=0.5089 d_loss=0.7118 kd_loss=0.0155\n",
      "Epoch [09/10] Step [100/161]: acc=0.5000 g_loss=0.5036 d_loss=0.7181 kd_loss=0.0165\n",
      "Epoch [09/10] Step [105/161]: acc=0.5000 g_loss=0.5098 d_loss=0.7146 kd_loss=0.0080\n",
      "Epoch [09/10] Step [110/161]: acc=0.5000 g_loss=0.5129 d_loss=0.7135 kd_loss=0.0191\n",
      "Epoch [09/10] Step [115/161]: acc=0.5000 g_loss=0.5090 d_loss=0.7140 kd_loss=0.0100\n",
      "Epoch [09/10] Step [120/161]: acc=0.5000 g_loss=0.5059 d_loss=0.7161 kd_loss=0.0132\n",
      "Epoch [09/10] Step [125/161]: acc=0.5000 g_loss=0.5070 d_loss=0.7161 kd_loss=0.0114\n",
      "Epoch [09/10] Step [130/161]: acc=0.5000 g_loss=0.5043 d_loss=0.7175 kd_loss=0.0162\n",
      "Epoch [09/10] Step [135/161]: acc=0.5000 g_loss=0.5034 d_loss=0.7183 kd_loss=0.0131\n",
      "Epoch [09/10] Step [140/161]: acc=0.5000 g_loss=0.5056 d_loss=0.7146 kd_loss=0.0127\n",
      "Epoch [09/10] Step [145/161]: acc=0.5000 g_loss=0.5077 d_loss=0.7130 kd_loss=0.0101\n",
      "Epoch [09/10] Step [150/161]: acc=0.5000 g_loss=0.5084 d_loss=0.7127 kd_loss=0.0143\n",
      "Epoch [09/10] Step [155/161]: acc=0.5000 g_loss=0.5061 d_loss=0.7139 kd_loss=0.0101\n",
      "Epoch [09/10] Step [160/161]: acc=0.2558 g_loss=0.5092 d_loss=0.6095 kd_loss=0.0139\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  1.2796149253845215\n",
      "Accuracy: 0.40303358613217766\n",
      "F1 score (Macro): 0.38727387312077416\n",
      "F1 score (Per class): [0.3443038  0.34257749 0.47494033]\n",
      "Precision score (Per class): [0.27868852 0.67741935 0.37977099]\n",
      "Recall score (Per class): [0.45033113 0.22925764 0.63375796]\n"
     ]
    }
   ],
   "source": [
    "# bilstm italian\n",
    "tgt_encoder = adapt(src_encoder, discriminator,\n",
    "                    src_classifier, train_dataloader, train_translated_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a985fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10\n",
      "Epoch [00/10] Step [000/127]: acc=0.5000 g_loss=0.5137 d_loss=0.7265 kd_loss=0.1420\n",
      "Epoch [00/10] Step [005/127]: acc=0.5000 g_loss=0.5138 d_loss=0.7220 kd_loss=0.0734\n",
      "Epoch [00/10] Step [010/127]: acc=0.5000 g_loss=0.5151 d_loss=0.7201 kd_loss=0.0795\n",
      "Epoch [00/10] Step [015/127]: acc=0.5000 g_loss=0.5157 d_loss=0.7167 kd_loss=0.1060\n",
      "Epoch [00/10] Step [020/127]: acc=0.5000 g_loss=0.5165 d_loss=0.7133 kd_loss=0.0770\n",
      "Epoch [00/10] Step [025/127]: acc=0.5000 g_loss=0.5100 d_loss=0.7138 kd_loss=0.0945\n",
      "Epoch [00/10] Step [030/127]: acc=0.5000 g_loss=0.5087 d_loss=0.7130 kd_loss=0.0648\n",
      "Epoch [00/10] Step [035/127]: acc=0.5000 g_loss=0.5100 d_loss=0.7122 kd_loss=0.1691\n",
      "Epoch [00/10] Step [040/127]: acc=0.5000 g_loss=0.5082 d_loss=0.7145 kd_loss=0.0583\n",
      "Epoch [00/10] Step [045/127]: acc=0.5000 g_loss=0.5012 d_loss=0.7211 kd_loss=0.1341\n",
      "Epoch [00/10] Step [050/127]: acc=0.5000 g_loss=0.5015 d_loss=0.7185 kd_loss=0.1420\n",
      "Epoch [00/10] Step [055/127]: acc=0.5000 g_loss=0.5052 d_loss=0.7188 kd_loss=0.1087\n",
      "Epoch [00/10] Step [060/127]: acc=0.5000 g_loss=0.5151 d_loss=0.7113 kd_loss=0.1141\n",
      "Epoch [00/10] Step [065/127]: acc=0.5000 g_loss=0.5151 d_loss=0.7113 kd_loss=0.0880\n",
      "Epoch [00/10] Step [070/127]: acc=0.5000 g_loss=0.5160 d_loss=0.7109 kd_loss=0.0746\n",
      "Epoch [00/10] Step [075/127]: acc=0.5000 g_loss=0.5172 d_loss=0.7111 kd_loss=0.0598\n",
      "Epoch [00/10] Step [080/127]: acc=0.5000 g_loss=0.5176 d_loss=0.7139 kd_loss=0.0650\n",
      "Epoch [00/10] Step [085/127]: acc=0.5000 g_loss=0.5205 d_loss=0.7110 kd_loss=0.1347\n",
      "Epoch [00/10] Step [090/127]: acc=0.5000 g_loss=0.5219 d_loss=0.7109 kd_loss=0.0645\n",
      "Epoch [00/10] Step [095/127]: acc=0.5000 g_loss=0.5239 d_loss=0.7130 kd_loss=0.1002\n",
      "Epoch [00/10] Step [100/127]: acc=0.5000 g_loss=0.5243 d_loss=0.7097 kd_loss=0.0833\n",
      "Epoch [00/10] Step [105/127]: acc=0.5000 g_loss=0.5220 d_loss=0.7126 kd_loss=0.0676\n",
      "Epoch [00/10] Step [110/127]: acc=0.5000 g_loss=0.5269 d_loss=0.7081 kd_loss=0.0789\n",
      "Epoch [00/10] Step [115/127]: acc=0.5000 g_loss=0.5261 d_loss=0.7098 kd_loss=0.0657\n",
      "Epoch [00/10] Step [120/127]: acc=0.5000 g_loss=0.5305 d_loss=0.7087 kd_loss=0.0576\n",
      "Epoch [00/10] Step [125/127]: acc=0.5000 g_loss=0.5289 d_loss=0.7099 kd_loss=0.0512\n",
      "French Test: \n",
      "\n",
      "Validation loss:  2.15982985496521\n",
      "Accuracy: 0.49339735894357745\n",
      "F1 score (Macro): 0.4619688906683495\n",
      "F1 score (Per class): [0.35897436 0.42494226 0.60199005]\n",
      "Precision score (Per class): [0.26190476 0.5974026  0.62857143]\n",
      "Recall score (Per class): [0.57037037 0.3297491  0.57756563]\n",
      "Epoch: 1/10\n",
      "Epoch [01/10] Step [000/127]: acc=0.5000 g_loss=0.5176 d_loss=0.7154 kd_loss=0.1076\n",
      "Epoch [01/10] Step [005/127]: acc=0.5000 g_loss=0.5236 d_loss=0.7115 kd_loss=0.0566\n",
      "Epoch [01/10] Step [010/127]: acc=0.5000 g_loss=0.5300 d_loss=0.7042 kd_loss=0.0497\n",
      "Epoch [01/10] Step [015/127]: acc=0.5000 g_loss=0.5349 d_loss=0.7054 kd_loss=0.0984\n",
      "Epoch [01/10] Step [020/127]: acc=0.5000 g_loss=0.5275 d_loss=0.6994 kd_loss=0.0296\n",
      "Epoch [01/10] Step [025/127]: acc=0.5000 g_loss=0.4978 d_loss=0.7149 kd_loss=0.0581\n",
      "Epoch [01/10] Step [030/127]: acc=0.5000 g_loss=0.4943 d_loss=0.7261 kd_loss=0.0559\n",
      "Epoch [01/10] Step [035/127]: acc=0.5000 g_loss=0.5075 d_loss=0.7253 kd_loss=0.0214\n",
      "Epoch [01/10] Step [040/127]: acc=0.5000 g_loss=0.5184 d_loss=0.7198 kd_loss=0.0465\n",
      "Epoch [01/10] Step [045/127]: acc=0.5000 g_loss=0.5137 d_loss=0.7247 kd_loss=0.0661\n",
      "Epoch [01/10] Step [050/127]: acc=0.5000 g_loss=0.5299 d_loss=0.7194 kd_loss=0.0377\n",
      "Epoch [01/10] Step [055/127]: acc=0.5000 g_loss=0.5287 d_loss=0.7134 kd_loss=0.0265\n",
      "Epoch [01/10] Step [060/127]: acc=0.5000 g_loss=0.5245 d_loss=0.7211 kd_loss=0.0285\n",
      "Epoch [01/10] Step [065/127]: acc=0.5000 g_loss=0.5352 d_loss=0.7242 kd_loss=0.0443\n",
      "Epoch [01/10] Step [070/127]: acc=0.5000 g_loss=0.5293 d_loss=0.7216 kd_loss=0.0183\n",
      "Epoch [01/10] Step [075/127]: acc=0.5000 g_loss=0.5306 d_loss=0.7209 kd_loss=0.0137\n",
      "Epoch [01/10] Step [080/127]: acc=0.5000 g_loss=0.5381 d_loss=0.7184 kd_loss=0.0242\n",
      "Epoch [01/10] Step [085/127]: acc=0.5000 g_loss=0.5338 d_loss=0.7176 kd_loss=0.0387\n",
      "Epoch [01/10] Step [090/127]: acc=0.5000 g_loss=0.5250 d_loss=0.7184 kd_loss=0.0336\n",
      "Epoch [01/10] Step [095/127]: acc=0.5000 g_loss=0.5262 d_loss=0.7213 kd_loss=0.0198\n",
      "Epoch [01/10] Step [100/127]: acc=0.5000 g_loss=0.5249 d_loss=0.7139 kd_loss=0.0316\n",
      "Epoch [01/10] Step [105/127]: acc=0.5000 g_loss=0.5149 d_loss=0.7188 kd_loss=0.0372\n",
      "Epoch [01/10] Step [110/127]: acc=0.5000 g_loss=0.5235 d_loss=0.7076 kd_loss=0.0294\n",
      "Epoch [01/10] Step [115/127]: acc=0.5000 g_loss=0.5236 d_loss=0.7050 kd_loss=0.0261\n",
      "Epoch [01/10] Step [120/127]: acc=0.5000 g_loss=0.5282 d_loss=0.7036 kd_loss=0.0338\n",
      "Epoch [01/10] Step [125/127]: acc=0.5000 g_loss=0.5154 d_loss=0.7110 kd_loss=0.0189\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.7883632183074951\n",
      "Accuracy: 0.4657863145258103\n",
      "F1 score (Macro): 0.4481796750233967\n",
      "F1 score (Per class): [0.35490605 0.43243243 0.55720054]\n",
      "Precision score (Per class): [0.24709302 0.58181818 0.63888889]\n",
      "Recall score (Per class): [0.62962963 0.34408602 0.49403341]\n",
      "Epoch: 2/10\n",
      "Epoch [02/10] Step [000/127]: acc=0.5000 g_loss=0.5038 d_loss=0.7179 kd_loss=0.0302\n",
      "Epoch [02/10] Step [005/127]: acc=0.5000 g_loss=0.5065 d_loss=0.7125 kd_loss=0.0533\n",
      "Epoch [02/10] Step [010/127]: acc=0.5000 g_loss=0.5089 d_loss=0.7066 kd_loss=0.0638\n",
      "Epoch [02/10] Step [015/127]: acc=0.5000 g_loss=0.5082 d_loss=0.7179 kd_loss=0.0518\n",
      "Epoch [02/10] Step [020/127]: acc=0.5000 g_loss=0.5031 d_loss=0.7136 kd_loss=0.0279\n",
      "Epoch [02/10] Step [025/127]: acc=0.5000 g_loss=0.4729 d_loss=0.7335 kd_loss=0.0279\n",
      "Epoch [02/10] Step [030/127]: acc=0.5000 g_loss=0.4743 d_loss=0.7377 kd_loss=0.0393\n",
      "Epoch [02/10] Step [035/127]: acc=0.5000 g_loss=0.4935 d_loss=0.7299 kd_loss=0.0375\n",
      "Epoch [02/10] Step [040/127]: acc=0.5000 g_loss=0.5196 d_loss=0.7160 kd_loss=0.0165\n",
      "Epoch [02/10] Step [045/127]: acc=0.5000 g_loss=0.5206 d_loss=0.7164 kd_loss=0.0651\n",
      "Epoch [02/10] Step [050/127]: acc=0.5000 g_loss=0.5306 d_loss=0.7116 kd_loss=0.0611\n",
      "Epoch [02/10] Step [055/127]: acc=0.5000 g_loss=0.5325 d_loss=0.7062 kd_loss=0.0349\n",
      "Epoch [02/10] Step [060/127]: acc=0.5000 g_loss=0.5360 d_loss=0.7133 kd_loss=0.0850\n",
      "Epoch [02/10] Step [065/127]: acc=0.5000 g_loss=0.5415 d_loss=0.7167 kd_loss=0.0187\n",
      "Epoch [02/10] Step [070/127]: acc=0.5000 g_loss=0.5287 d_loss=0.7208 kd_loss=0.0162\n",
      "Epoch [02/10] Step [075/127]: acc=0.5000 g_loss=0.5259 d_loss=0.7236 kd_loss=0.0406\n",
      "Epoch [02/10] Step [080/127]: acc=0.5000 g_loss=0.5349 d_loss=0.7225 kd_loss=0.0503\n",
      "Epoch [02/10] Step [085/127]: acc=0.5000 g_loss=0.5341 d_loss=0.7178 kd_loss=0.0459\n",
      "Epoch [02/10] Step [090/127]: acc=0.5000 g_loss=0.5278 d_loss=0.7162 kd_loss=0.0707\n",
      "Epoch [02/10] Step [095/127]: acc=0.5000 g_loss=0.5228 d_loss=0.7221 kd_loss=0.0459\n",
      "Epoch [02/10] Step [100/127]: acc=0.5000 g_loss=0.5239 d_loss=0.7140 kd_loss=0.0595\n",
      "Epoch [02/10] Step [105/127]: acc=0.5000 g_loss=0.5201 d_loss=0.7148 kd_loss=0.0612\n",
      "Epoch [02/10] Step [110/127]: acc=0.5000 g_loss=0.5254 d_loss=0.7072 kd_loss=0.0253\n",
      "Epoch [02/10] Step [115/127]: acc=0.5000 g_loss=0.5230 d_loss=0.7057 kd_loss=0.0456\n",
      "Epoch [02/10] Step [120/127]: acc=0.5000 g_loss=0.5222 d_loss=0.7066 kd_loss=0.0496\n",
      "Epoch [02/10] Step [125/127]: acc=0.5000 g_loss=0.5264 d_loss=0.7072 kd_loss=0.0581\n",
      "French Test: \n",
      "\n",
      "Validation loss:  2.0299201011657715\n",
      "Accuracy: 0.4417767106842737\n",
      "F1 score (Macro): 0.4310186768900777\n",
      "F1 score (Per class): [0.33962264 0.41797753 0.53545586]\n",
      "Precision score (Per class): [0.2278481  0.56024096 0.68014706]\n",
      "Recall score (Per class): [0.66666667 0.33333333 0.44152745]\n",
      "Epoch: 3/10\n",
      "Epoch [03/10] Step [000/127]: acc=0.5000 g_loss=0.5186 d_loss=0.7104 kd_loss=0.0324\n",
      "Epoch [03/10] Step [005/127]: acc=0.5000 g_loss=0.5109 d_loss=0.7147 kd_loss=0.0468\n",
      "Epoch [03/10] Step [010/127]: acc=0.5000 g_loss=0.5056 d_loss=0.7125 kd_loss=0.0611\n",
      "Epoch [03/10] Step [015/127]: acc=0.5000 g_loss=0.5041 d_loss=0.7269 kd_loss=0.0531\n",
      "Epoch [03/10] Step [020/127]: acc=0.5000 g_loss=0.5095 d_loss=0.7175 kd_loss=0.0351\n",
      "Epoch [03/10] Step [025/127]: acc=0.5000 g_loss=0.5184 d_loss=0.7142 kd_loss=0.0709\n",
      "Epoch [03/10] Step [030/127]: acc=0.5000 g_loss=0.5463 d_loss=0.7049 kd_loss=0.0351\n",
      "Epoch [03/10] Step [035/127]: acc=0.5000 g_loss=0.5798 d_loss=0.6914 kd_loss=0.0752\n",
      "Epoch [03/10] Step [040/127]: acc=0.5000 g_loss=0.6106 d_loss=0.6704 kd_loss=0.0690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [03/10] Step [045/127]: acc=0.5000 g_loss=0.5589 d_loss=0.7069 kd_loss=0.0593\n",
      "Epoch [03/10] Step [050/127]: acc=0.5000 g_loss=0.5552 d_loss=0.7102 kd_loss=0.0627\n",
      "Epoch [03/10] Step [055/127]: acc=0.5000 g_loss=0.5406 d_loss=0.7076 kd_loss=0.0339\n",
      "Epoch [03/10] Step [060/127]: acc=0.5000 g_loss=0.5281 d_loss=0.7271 kd_loss=0.0445\n",
      "Epoch [03/10] Step [065/127]: acc=0.5000 g_loss=0.5320 d_loss=0.7396 kd_loss=0.0255\n",
      "Epoch [03/10] Step [070/127]: acc=0.5000 g_loss=0.5142 d_loss=0.7431 kd_loss=0.0498\n",
      "Epoch [03/10] Step [075/127]: acc=0.5000 g_loss=0.5080 d_loss=0.7475 kd_loss=0.0196\n",
      "Epoch [03/10] Step [080/127]: acc=0.5000 g_loss=0.5132 d_loss=0.7497 kd_loss=0.0263\n",
      "Epoch [03/10] Step [085/127]: acc=0.5000 g_loss=0.5153 d_loss=0.7408 kd_loss=0.0494\n",
      "Epoch [03/10] Step [090/127]: acc=0.5000 g_loss=0.5186 d_loss=0.7322 kd_loss=0.0404\n",
      "Epoch [03/10] Step [095/127]: acc=0.5000 g_loss=0.5164 d_loss=0.7390 kd_loss=0.0192\n",
      "Epoch [03/10] Step [100/127]: acc=0.5000 g_loss=0.5156 d_loss=0.7325 kd_loss=0.0275\n",
      "Epoch [03/10] Step [105/127]: acc=0.5000 g_loss=0.5161 d_loss=0.7277 kd_loss=0.0689\n",
      "Epoch [03/10] Step [110/127]: acc=0.5000 g_loss=0.5238 d_loss=0.7189 kd_loss=0.0424\n",
      "Epoch [03/10] Step [115/127]: acc=0.5000 g_loss=0.5272 d_loss=0.7149 kd_loss=0.0395\n",
      "Epoch [03/10] Step [120/127]: acc=0.5000 g_loss=0.5258 d_loss=0.7122 kd_loss=0.0400\n",
      "Epoch [03/10] Step [125/127]: acc=0.5000 g_loss=0.5375 d_loss=0.7060 kd_loss=0.0467\n",
      "French Test: \n",
      "\n",
      "Validation loss:  2.0375733375549316\n",
      "Accuracy: 0.39255702280912363\n",
      "F1 score (Macro): 0.3934710732826936\n",
      "F1 score (Per class): [0.33450704 0.42714571 0.41876047]\n",
      "Precision score (Per class): [0.21939954 0.48198198 0.70224719]\n",
      "Recall score (Per class): [0.7037037  0.38351254 0.29832936]\n",
      "Epoch: 4/10\n",
      "Epoch [04/10] Step [000/127]: acc=0.5000 g_loss=0.5125 d_loss=0.7195 kd_loss=0.0445\n",
      "Epoch [04/10] Step [005/127]: acc=0.5000 g_loss=0.5072 d_loss=0.7236 kd_loss=0.0457\n",
      "Epoch [04/10] Step [010/127]: acc=0.5000 g_loss=0.5066 d_loss=0.7177 kd_loss=0.0405\n",
      "Epoch [04/10] Step [015/127]: acc=0.5000 g_loss=0.5059 d_loss=0.7288 kd_loss=0.0369\n",
      "Epoch [04/10] Step [020/127]: acc=0.5000 g_loss=0.5131 d_loss=0.7182 kd_loss=0.0900\n",
      "Epoch [04/10] Step [025/127]: acc=0.5000 g_loss=0.5253 d_loss=0.7117 kd_loss=0.0236\n",
      "Epoch [04/10] Step [030/127]: acc=0.5000 g_loss=0.5451 d_loss=0.7051 kd_loss=0.0601\n",
      "Epoch [04/10] Step [035/127]: acc=0.5000 g_loss=0.5673 d_loss=0.6945 kd_loss=0.0492\n",
      "Epoch [04/10] Step [040/127]: acc=0.5000 g_loss=0.5883 d_loss=0.6789 kd_loss=0.0475\n",
      "Epoch [04/10] Step [045/127]: acc=0.5000 g_loss=0.5199 d_loss=0.7266 kd_loss=0.0941\n",
      "Epoch [04/10] Step [050/127]: acc=0.5000 g_loss=0.5112 d_loss=0.7342 kd_loss=0.0395\n",
      "Epoch [04/10] Step [055/127]: acc=0.5000 g_loss=0.5006 d_loss=0.7348 kd_loss=0.0178\n",
      "Epoch [04/10] Step [060/127]: acc=0.5000 g_loss=0.5056 d_loss=0.7362 kd_loss=0.0417\n",
      "Epoch [04/10] Step [065/127]: acc=0.5000 g_loss=0.5060 d_loss=0.7425 kd_loss=0.0680\n",
      "Epoch [04/10] Step [070/127]: acc=0.5000 g_loss=0.5058 d_loss=0.7371 kd_loss=0.0257\n",
      "Epoch [04/10] Step [075/127]: acc=0.5000 g_loss=0.5098 d_loss=0.7330 kd_loss=0.0486\n",
      "Epoch [04/10] Step [080/127]: acc=0.5000 g_loss=0.5143 d_loss=0.7301 kd_loss=0.0534\n",
      "Epoch [04/10] Step [085/127]: acc=0.5000 g_loss=0.5143 d_loss=0.7272 kd_loss=0.0538\n",
      "Epoch [04/10] Step [090/127]: acc=0.5000 g_loss=0.5192 d_loss=0.7197 kd_loss=0.0268\n",
      "Epoch [04/10] Step [095/127]: acc=0.5000 g_loss=0.5179 d_loss=0.7189 kd_loss=0.0203\n",
      "Epoch [04/10] Step [100/127]: acc=0.5000 g_loss=0.5180 d_loss=0.7187 kd_loss=0.0323\n",
      "Epoch [04/10] Step [105/127]: acc=0.5000 g_loss=0.5309 d_loss=0.7094 kd_loss=0.0274\n",
      "Epoch [04/10] Step [110/127]: acc=0.5000 g_loss=0.5194 d_loss=0.7199 kd_loss=0.0377\n",
      "Epoch [04/10] Step [115/127]: acc=0.5000 g_loss=0.5190 d_loss=0.7122 kd_loss=0.0375\n",
      "Epoch [04/10] Step [120/127]: acc=0.5000 g_loss=0.5130 d_loss=0.7157 kd_loss=0.0239\n",
      "Epoch [04/10] Step [125/127]: acc=0.5000 g_loss=0.5208 d_loss=0.7164 kd_loss=0.0857\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.4390523433685303\n",
      "Accuracy: 0.5354141656662665\n",
      "F1 score (Macro): 0.48013417510627626\n",
      "F1 score (Per class): [0.36578171 0.42056075 0.65406007]\n",
      "Precision score (Per class): [0.30392157 0.60402685 0.6125    ]\n",
      "Recall score (Per class): [0.45925926 0.32258065 0.70167064]\n",
      "Epoch: 5/10\n",
      "Epoch [05/10] Step [000/127]: acc=0.5000 g_loss=0.4857 d_loss=0.7369 kd_loss=0.0261\n",
      "Epoch [05/10] Step [005/127]: acc=0.5000 g_loss=0.4879 d_loss=0.7357 kd_loss=0.0401\n",
      "Epoch [05/10] Step [010/127]: acc=0.5000 g_loss=0.4891 d_loss=0.7279 kd_loss=0.0523\n",
      "Epoch [05/10] Step [015/127]: acc=0.5000 g_loss=0.4963 d_loss=0.7362 kd_loss=0.1019\n",
      "Epoch [05/10] Step [020/127]: acc=0.5000 g_loss=0.5072 d_loss=0.7203 kd_loss=0.0446\n",
      "Epoch [05/10] Step [025/127]: acc=0.5000 g_loss=0.5232 d_loss=0.7100 kd_loss=0.0485\n",
      "Epoch [05/10] Step [030/127]: acc=0.5000 g_loss=0.5437 d_loss=0.7036 kd_loss=0.0516\n",
      "Epoch [05/10] Step [035/127]: acc=0.5000 g_loss=0.5637 d_loss=0.6944 kd_loss=0.0239\n",
      "Epoch [05/10] Step [040/127]: acc=0.5000 g_loss=0.5776 d_loss=0.6831 kd_loss=0.0225\n",
      "Epoch [05/10] Step [045/127]: acc=0.5000 g_loss=0.5222 d_loss=0.7221 kd_loss=0.0559\n",
      "Epoch [05/10] Step [050/127]: acc=0.5000 g_loss=0.5177 d_loss=0.7275 kd_loss=0.0370\n",
      "Epoch [05/10] Step [055/127]: acc=0.5000 g_loss=0.5028 d_loss=0.7300 kd_loss=0.0234\n",
      "Epoch [05/10] Step [060/127]: acc=0.5000 g_loss=0.5046 d_loss=0.7362 kd_loss=0.0494\n",
      "Epoch [05/10] Step [065/127]: acc=0.5000 g_loss=0.4986 d_loss=0.7464 kd_loss=0.0453\n",
      "Epoch [05/10] Step [070/127]: acc=0.5000 g_loss=0.4988 d_loss=0.7395 kd_loss=0.0411\n",
      "Epoch [05/10] Step [075/127]: acc=0.5000 g_loss=0.5038 d_loss=0.7353 kd_loss=0.0250\n",
      "Epoch [05/10] Step [080/127]: acc=0.5000 g_loss=0.5016 d_loss=0.7395 kd_loss=0.0404\n",
      "Epoch [05/10] Step [085/127]: acc=0.5000 g_loss=0.5003 d_loss=0.7344 kd_loss=0.0558\n",
      "Epoch [05/10] Step [090/127]: acc=0.5000 g_loss=0.5101 d_loss=0.7225 kd_loss=0.0328\n",
      "Epoch [05/10] Step [095/127]: acc=0.5000 g_loss=0.5078 d_loss=0.7269 kd_loss=0.0563\n",
      "Epoch [05/10] Step [100/127]: acc=0.5000 g_loss=0.5088 d_loss=0.7210 kd_loss=0.0113\n",
      "Epoch [05/10] Step [105/127]: acc=0.5000 g_loss=0.5150 d_loss=0.7169 kd_loss=0.0504\n",
      "Epoch [05/10] Step [110/127]: acc=0.5000 g_loss=0.5149 d_loss=0.7177 kd_loss=0.0279\n",
      "Epoch [05/10] Step [115/127]: acc=0.5000 g_loss=0.5199 d_loss=0.7111 kd_loss=0.0245\n",
      "Epoch [05/10] Step [120/127]: acc=0.5000 g_loss=0.5164 d_loss=0.7130 kd_loss=0.0260\n",
      "Epoch [05/10] Step [125/127]: acc=0.5000 g_loss=0.5291 d_loss=0.7087 kd_loss=0.0360\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.6736881732940674\n",
      "Accuracy: 0.460984393757503\n",
      "F1 score (Macro): 0.43711089494163424\n",
      "F1 score (Per class): [0.34583333 0.4        0.56549935]\n",
      "Precision score (Per class): [0.24057971 0.61029412 0.61931818]\n",
      "Recall score (Per class): [0.61481481 0.29749104 0.5202864 ]\n",
      "Epoch: 6/10\n",
      "Epoch [06/10] Step [000/127]: acc=0.5000 g_loss=0.4881 d_loss=0.7347 kd_loss=0.0334\n",
      "Epoch [06/10] Step [005/127]: acc=0.5000 g_loss=0.4890 d_loss=0.7337 kd_loss=0.0414\n",
      "Epoch [06/10] Step [010/127]: acc=0.5000 g_loss=0.4893 d_loss=0.7276 kd_loss=0.0227\n",
      "Epoch [06/10] Step [015/127]: acc=0.5000 g_loss=0.4952 d_loss=0.7337 kd_loss=0.0496\n",
      "Epoch [06/10] Step [020/127]: acc=0.5000 g_loss=0.5023 d_loss=0.7226 kd_loss=0.0387\n",
      "Epoch [06/10] Step [025/127]: acc=0.5000 g_loss=0.5058 d_loss=0.7191 kd_loss=0.0214\n",
      "Epoch [06/10] Step [030/127]: acc=0.5000 g_loss=0.5113 d_loss=0.7182 kd_loss=0.0349\n",
      "Epoch [06/10] Step [035/127]: acc=0.5000 g_loss=0.5121 d_loss=0.7196 kd_loss=0.0379\n",
      "Epoch [06/10] Step [040/127]: acc=0.5000 g_loss=0.5065 d_loss=0.7209 kd_loss=0.0239\n",
      "Epoch [06/10] Step [045/127]: acc=0.5000 g_loss=0.4980 d_loss=0.7265 kd_loss=0.0658\n",
      "Epoch [06/10] Step [050/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7244 kd_loss=0.0482\n",
      "Epoch [06/10] Step [055/127]: acc=0.5000 g_loss=0.4967 d_loss=0.7227 kd_loss=0.0264\n",
      "Epoch [06/10] Step [060/127]: acc=0.5000 g_loss=0.5021 d_loss=0.7213 kd_loss=0.0720\n",
      "Epoch [06/10] Step [065/127]: acc=0.5000 g_loss=0.4974 d_loss=0.7233 kd_loss=0.0411\n",
      "Epoch [06/10] Step [070/127]: acc=0.5000 g_loss=0.4980 d_loss=0.7216 kd_loss=0.0430\n",
      "Epoch [06/10] Step [075/127]: acc=0.5000 g_loss=0.5033 d_loss=0.7172 kd_loss=0.0588\n",
      "Epoch [06/10] Step [080/127]: acc=0.5000 g_loss=0.5013 d_loss=0.7206 kd_loss=0.0411\n",
      "Epoch [06/10] Step [085/127]: acc=0.5000 g_loss=0.5017 d_loss=0.7190 kd_loss=0.0493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [06/10] Step [090/127]: acc=0.5000 g_loss=0.5081 d_loss=0.7148 kd_loss=0.0333\n",
      "Epoch [06/10] Step [095/127]: acc=0.5000 g_loss=0.5080 d_loss=0.7149 kd_loss=0.0187\n",
      "Epoch [06/10] Step [100/127]: acc=0.5000 g_loss=0.5083 d_loss=0.7151 kd_loss=0.0194\n",
      "Epoch [06/10] Step [105/127]: acc=0.5000 g_loss=0.5181 d_loss=0.7097 kd_loss=0.0207\n",
      "Epoch [06/10] Step [110/127]: acc=0.5000 g_loss=0.5106 d_loss=0.7184 kd_loss=0.0275\n",
      "Epoch [06/10] Step [115/127]: acc=0.5000 g_loss=0.5102 d_loss=0.7134 kd_loss=0.0160\n",
      "Epoch [06/10] Step [120/127]: acc=0.5000 g_loss=0.5038 d_loss=0.7174 kd_loss=0.0222\n",
      "Epoch [06/10] Step [125/127]: acc=0.5000 g_loss=0.5135 d_loss=0.7167 kd_loss=0.0631\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.4308415651321411\n",
      "Accuracy: 0.5546218487394958\n",
      "F1 score (Macro): 0.48432030425102873\n",
      "F1 score (Per class): [0.34074074 0.4469526  0.66526758]\n",
      "Precision score (Per class): [0.34074074 0.60365854 0.59363296]\n",
      "Recall score (Per class): [0.34074074 0.35483871 0.75656325]\n",
      "Epoch: 7/10\n",
      "Epoch [07/10] Step [000/127]: acc=0.5000 g_loss=0.4819 d_loss=0.7361 kd_loss=0.0419\n",
      "Epoch [07/10] Step [005/127]: acc=0.5000 g_loss=0.4821 d_loss=0.7358 kd_loss=0.0247\n",
      "Epoch [07/10] Step [010/127]: acc=0.5000 g_loss=0.4866 d_loss=0.7284 kd_loss=0.0250\n",
      "Epoch [07/10] Step [015/127]: acc=0.5000 g_loss=0.4984 d_loss=0.7288 kd_loss=0.0405\n",
      "Epoch [07/10] Step [020/127]: acc=0.5000 g_loss=0.5148 d_loss=0.7146 kd_loss=0.0341\n",
      "Epoch [07/10] Step [025/127]: acc=0.5000 g_loss=0.4973 d_loss=0.7278 kd_loss=0.0331\n",
      "Epoch [07/10] Step [030/127]: acc=0.5000 g_loss=0.4954 d_loss=0.7301 kd_loss=0.0517\n",
      "Epoch [07/10] Step [035/127]: acc=0.5000 g_loss=0.4936 d_loss=0.7289 kd_loss=0.0310\n",
      "Epoch [07/10] Step [040/127]: acc=0.5000 g_loss=0.4890 d_loss=0.7288 kd_loss=0.0344\n",
      "Epoch [07/10] Step [045/127]: acc=0.5000 g_loss=0.5002 d_loss=0.7193 kd_loss=0.0605\n",
      "Epoch [07/10] Step [050/127]: acc=0.5000 g_loss=0.4978 d_loss=0.7177 kd_loss=0.0241\n",
      "Epoch [07/10] Step [055/127]: acc=0.5000 g_loss=0.4956 d_loss=0.7182 kd_loss=0.0272\n",
      "Epoch [07/10] Step [060/127]: acc=0.5000 g_loss=0.4997 d_loss=0.7174 kd_loss=0.0229\n",
      "Epoch [07/10] Step [065/127]: acc=0.5000 g_loss=0.4928 d_loss=0.7191 kd_loss=0.0174\n",
      "Epoch [07/10] Step [070/127]: acc=0.5000 g_loss=0.4929 d_loss=0.7198 kd_loss=0.0176\n",
      "Epoch [07/10] Step [075/127]: acc=0.5000 g_loss=0.4948 d_loss=0.7190 kd_loss=0.0255\n",
      "Epoch [07/10] Step [080/127]: acc=0.5000 g_loss=0.4921 d_loss=0.7244 kd_loss=0.0212\n",
      "Epoch [07/10] Step [085/127]: acc=0.5000 g_loss=0.4957 d_loss=0.7198 kd_loss=0.0219\n",
      "Epoch [07/10] Step [090/127]: acc=0.5000 g_loss=0.5012 d_loss=0.7155 kd_loss=0.0335\n",
      "Epoch [07/10] Step [095/127]: acc=0.5000 g_loss=0.5014 d_loss=0.7168 kd_loss=0.0241\n",
      "Epoch [07/10] Step [100/127]: acc=0.5000 g_loss=0.5045 d_loss=0.7140 kd_loss=0.0251\n",
      "Epoch [07/10] Step [105/127]: acc=0.5000 g_loss=0.5150 d_loss=0.7095 kd_loss=0.0335\n",
      "Epoch [07/10] Step [110/127]: acc=0.5000 g_loss=0.5105 d_loss=0.7165 kd_loss=0.0124\n",
      "Epoch [07/10] Step [115/127]: acc=0.5000 g_loss=0.5089 d_loss=0.7126 kd_loss=0.0272\n",
      "Epoch [07/10] Step [120/127]: acc=0.5000 g_loss=0.5002 d_loss=0.7192 kd_loss=0.0226\n",
      "Epoch [07/10] Step [125/127]: acc=0.5000 g_loss=0.5120 d_loss=0.7170 kd_loss=0.0175\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.6138744354248047\n",
      "Accuracy: 0.56062424969988\n",
      "F1 score (Macro): 0.4656251430308445\n",
      "F1 score (Per class): [0.3220339  0.39119804 0.68364349]\n",
      "Precision score (Per class): [0.37623762 0.61538462 0.57973422]\n",
      "Recall score (Per class): [0.28148148 0.28673835 0.83293556]\n",
      "Epoch: 8/10\n",
      "Epoch [08/10] Step [000/127]: acc=0.5000 g_loss=0.4737 d_loss=0.7430 kd_loss=0.0267\n",
      "Epoch [08/10] Step [005/127]: acc=0.5000 g_loss=0.4786 d_loss=0.7394 kd_loss=0.0337\n",
      "Epoch [08/10] Step [010/127]: acc=0.5000 g_loss=0.4875 d_loss=0.7295 kd_loss=0.0212\n",
      "Epoch [08/10] Step [015/127]: acc=0.5000 g_loss=0.5054 d_loss=0.7279 kd_loss=0.0288\n",
      "Epoch [08/10] Step [020/127]: acc=0.5000 g_loss=0.5264 d_loss=0.7112 kd_loss=0.0544\n",
      "Epoch [08/10] Step [025/127]: acc=0.5000 g_loss=0.4935 d_loss=0.7351 kd_loss=0.0192\n",
      "Epoch [08/10] Step [030/127]: acc=0.5000 g_loss=0.4897 d_loss=0.7399 kd_loss=0.0418\n",
      "Epoch [08/10] Step [035/127]: acc=0.5000 g_loss=0.4876 d_loss=0.7401 kd_loss=0.0343\n",
      "Epoch [08/10] Step [040/127]: acc=0.5000 g_loss=0.4866 d_loss=0.7335 kd_loss=0.0800\n",
      "Epoch [08/10] Step [045/127]: acc=0.5000 g_loss=0.5071 d_loss=0.7162 kd_loss=0.0449\n",
      "Epoch [08/10] Step [050/127]: acc=0.5000 g_loss=0.5052 d_loss=0.7158 kd_loss=0.0521\n",
      "Epoch [08/10] Step [055/127]: acc=0.5000 g_loss=0.5014 d_loss=0.7169 kd_loss=0.0196\n",
      "Epoch [08/10] Step [060/127]: acc=0.5000 g_loss=0.5040 d_loss=0.7182 kd_loss=0.0393\n",
      "Epoch [08/10] Step [065/127]: acc=0.5000 g_loss=0.4968 d_loss=0.7214 kd_loss=0.0217\n",
      "Epoch [08/10] Step [070/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7200 kd_loss=0.0219\n",
      "Epoch [08/10] Step [075/127]: acc=0.5000 g_loss=0.4986 d_loss=0.7207 kd_loss=0.0135\n",
      "Epoch [08/10] Step [080/127]: acc=0.5000 g_loss=0.4924 d_loss=0.7281 kd_loss=0.0326\n",
      "Epoch [08/10] Step [085/127]: acc=0.5000 g_loss=0.4951 d_loss=0.7228 kd_loss=0.0518\n",
      "Epoch [08/10] Step [090/127]: acc=0.5000 g_loss=0.5021 d_loss=0.7163 kd_loss=0.0273\n",
      "Epoch [08/10] Step [095/127]: acc=0.5000 g_loss=0.5001 d_loss=0.7191 kd_loss=0.0115\n",
      "Epoch [08/10] Step [100/127]: acc=0.5000 g_loss=0.5003 d_loss=0.7162 kd_loss=0.0208\n",
      "Epoch [08/10] Step [105/127]: acc=0.5000 g_loss=0.5064 d_loss=0.7152 kd_loss=0.0136\n",
      "Epoch [08/10] Step [110/127]: acc=0.5000 g_loss=0.5034 d_loss=0.7188 kd_loss=0.0205\n",
      "Epoch [08/10] Step [115/127]: acc=0.5000 g_loss=0.5037 d_loss=0.7147 kd_loss=0.0140\n",
      "Epoch [08/10] Step [120/127]: acc=0.5000 g_loss=0.4988 d_loss=0.7191 kd_loss=0.0169\n",
      "Epoch [08/10] Step [125/127]: acc=0.5000 g_loss=0.5024 d_loss=0.7206 kd_loss=0.0288\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.8057706356048584\n",
      "Accuracy: 0.56062424969988\n",
      "F1 score (Macro): 0.43893026616114406\n",
      "F1 score (Per class): [0.27802691 0.34408602 0.69467787]\n",
      "Precision score (Per class): [0.35227273 0.68817204 0.57055215]\n",
      "Recall score (Per class): [0.22962963 0.22939068 0.88782816]\n",
      "Epoch: 9/10\n",
      "Epoch [09/10] Step [000/127]: acc=0.5000 g_loss=0.4742 d_loss=0.7405 kd_loss=0.0113\n",
      "Epoch [09/10] Step [005/127]: acc=0.5000 g_loss=0.4783 d_loss=0.7362 kd_loss=0.0200\n",
      "Epoch [09/10] Step [010/127]: acc=0.5000 g_loss=0.4858 d_loss=0.7295 kd_loss=0.0339\n",
      "Epoch [09/10] Step [015/127]: acc=0.5000 g_loss=0.5053 d_loss=0.7226 kd_loss=0.0254\n",
      "Epoch [09/10] Step [020/127]: acc=0.5000 g_loss=0.5304 d_loss=0.7082 kd_loss=0.0491\n",
      "Epoch [09/10] Step [025/127]: acc=0.5000 g_loss=0.4898 d_loss=0.7392 kd_loss=0.0378\n",
      "Epoch [09/10] Step [030/127]: acc=0.5000 g_loss=0.4875 d_loss=0.7415 kd_loss=0.0425\n",
      "Epoch [09/10] Step [035/127]: acc=0.5000 g_loss=0.4867 d_loss=0.7389 kd_loss=0.0293\n",
      "Epoch [09/10] Step [040/127]: acc=0.5000 g_loss=0.4862 d_loss=0.7322 kd_loss=0.0342\n",
      "Epoch [09/10] Step [045/127]: acc=0.5000 g_loss=0.4973 d_loss=0.7213 kd_loss=0.0544\n",
      "Epoch [09/10] Step [050/127]: acc=0.5000 g_loss=0.4979 d_loss=0.7176 kd_loss=0.0284\n",
      "Epoch [09/10] Step [055/127]: acc=0.5000 g_loss=0.4948 d_loss=0.7183 kd_loss=0.0260\n",
      "Epoch [09/10] Step [060/127]: acc=0.5000 g_loss=0.4980 d_loss=0.7186 kd_loss=0.0374\n",
      "Epoch [09/10] Step [065/127]: acc=0.5000 g_loss=0.4897 d_loss=0.7235 kd_loss=0.0187\n",
      "Epoch [09/10] Step [070/127]: acc=0.5000 g_loss=0.4898 d_loss=0.7231 kd_loss=0.0245\n",
      "Epoch [09/10] Step [075/127]: acc=0.5000 g_loss=0.4936 d_loss=0.7200 kd_loss=0.0168\n",
      "Epoch [09/10] Step [080/127]: acc=0.5000 g_loss=0.4930 d_loss=0.7223 kd_loss=0.0427\n",
      "Epoch [09/10] Step [085/127]: acc=0.5000 g_loss=0.4931 d_loss=0.7208 kd_loss=0.0347\n",
      "Epoch [09/10] Step [090/127]: acc=0.5000 g_loss=0.4984 d_loss=0.7171 kd_loss=0.0305\n",
      "Epoch [09/10] Step [095/127]: acc=0.5000 g_loss=0.4998 d_loss=0.7149 kd_loss=0.0146\n",
      "Epoch [09/10] Step [100/127]: acc=0.5000 g_loss=0.5013 d_loss=0.7155 kd_loss=0.0247\n",
      "Epoch [09/10] Step [105/127]: acc=0.5000 g_loss=0.5074 d_loss=0.7134 kd_loss=0.0291\n",
      "Epoch [09/10] Step [110/127]: acc=0.5000 g_loss=0.5024 d_loss=0.7205 kd_loss=0.0133\n",
      "Epoch [09/10] Step [115/127]: acc=0.5000 g_loss=0.4994 d_loss=0.7175 kd_loss=0.0204\n",
      "Epoch [09/10] Step [120/127]: acc=0.5000 g_loss=0.4926 d_loss=0.7226 kd_loss=0.0216\n",
      "Epoch [09/10] Step [125/127]: acc=0.5000 g_loss=0.4966 d_loss=0.7248 kd_loss=0.0167\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.9805052280426025\n",
      "Accuracy: 0.5498199279711885\n",
      "F1 score (Macro): 0.3870340301643196\n",
      "F1 score (Per class): [0.15294118 0.32432432 0.68383659]\n",
      "Precision score (Per class): [0.37142857 0.65934066 0.54455446]\n",
      "Recall score (Per class): [0.0962963  0.21505376 0.91885442]\n"
     ]
    }
   ],
   "source": [
    "tgt_encoder = adapt(src_encoder, discriminator,\n",
    "                    src_classifier, train_dataloader, train_translated_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c73b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20\n",
      "Epoch [00/20] Step [000/127]: acc=0.5000 g_loss=0.4759 d_loss=0.7195 kd_loss=0.0972\n",
      "Epoch [00/20] Step [005/127]: acc=0.5000 g_loss=0.4762 d_loss=0.7231 kd_loss=0.0451\n",
      "Epoch [00/20] Step [010/127]: acc=0.5000 g_loss=0.4765 d_loss=0.7230 kd_loss=0.2035\n",
      "Epoch [00/20] Step [015/127]: acc=0.5000 g_loss=0.4771 d_loss=0.7224 kd_loss=0.0621\n",
      "Epoch [00/20] Step [020/127]: acc=0.5000 g_loss=0.4773 d_loss=0.7224 kd_loss=0.1038\n",
      "Epoch [00/20] Step [025/127]: acc=0.5000 g_loss=0.4773 d_loss=0.7223 kd_loss=0.1079\n",
      "Epoch [00/20] Step [030/127]: acc=0.5000 g_loss=0.4779 d_loss=0.7220 kd_loss=0.0744\n",
      "Epoch [00/20] Step [035/127]: acc=0.5000 g_loss=0.4776 d_loss=0.7221 kd_loss=0.0440\n",
      "Epoch [00/20] Step [040/127]: acc=0.5000 g_loss=0.4779 d_loss=0.7215 kd_loss=0.0323\n",
      "Epoch [00/20] Step [045/127]: acc=0.5000 g_loss=0.4778 d_loss=0.7216 kd_loss=0.0856\n",
      "Epoch [00/20] Step [050/127]: acc=0.5000 g_loss=0.4783 d_loss=0.7210 kd_loss=0.0212\n",
      "Epoch [00/20] Step [055/127]: acc=0.5000 g_loss=0.4786 d_loss=0.7208 kd_loss=0.1249\n",
      "Epoch [00/20] Step [060/127]: acc=0.5000 g_loss=0.4796 d_loss=0.7201 kd_loss=0.0350\n",
      "Epoch [00/20] Step [065/127]: acc=0.5000 g_loss=0.4796 d_loss=0.7203 kd_loss=0.0412\n",
      "Epoch [00/20] Step [070/127]: acc=0.5000 g_loss=0.4801 d_loss=0.7197 kd_loss=0.0648\n",
      "Epoch [00/20] Step [075/127]: acc=0.5000 g_loss=0.4800 d_loss=0.7196 kd_loss=0.0888\n",
      "Epoch [00/20] Step [080/127]: acc=0.5000 g_loss=0.4814 d_loss=0.7181 kd_loss=0.0355\n",
      "Epoch [00/20] Step [085/127]: acc=0.5000 g_loss=0.4807 d_loss=0.7191 kd_loss=0.0470\n",
      "Epoch [00/20] Step [090/127]: acc=0.5000 g_loss=0.4819 d_loss=0.7181 kd_loss=0.0291\n",
      "Epoch [00/20] Step [095/127]: acc=0.5000 g_loss=0.4825 d_loss=0.7177 kd_loss=0.1030\n",
      "Epoch [00/20] Step [100/127]: acc=0.5000 g_loss=0.4816 d_loss=0.7182 kd_loss=0.1077\n",
      "Epoch [00/20] Step [105/127]: acc=0.5000 g_loss=0.4818 d_loss=0.7188 kd_loss=0.1648\n",
      "Epoch [00/20] Step [110/127]: acc=0.5000 g_loss=0.4827 d_loss=0.7172 kd_loss=0.0474\n",
      "Epoch [00/20] Step [115/127]: acc=0.5000 g_loss=0.4815 d_loss=0.7171 kd_loss=0.0427\n",
      "Epoch [00/20] Step [120/127]: acc=0.5000 g_loss=0.4827 d_loss=0.7167 kd_loss=0.0310\n",
      "Epoch [00/20] Step [125/127]: acc=0.5000 g_loss=0.4815 d_loss=0.7181 kd_loss=0.0238\n",
      "German Test:\n",
      "Validation loss:  1.5354235172271729\n",
      "Accuracy: 0.6298543689320388\n",
      "F1 score (Macro): 0.5259503298598401\n",
      "F1 score (Per class): [0.35172414 0.47133758 0.75478927]\n",
      "Precision score (Per class): [0.28021978 0.51034483 0.79275654]\n",
      "Recall score (Per class): [0.47222222 0.43786982 0.7202925 ]\n",
      "Epoch: 1/20\n",
      "Epoch [01/20] Step [000/127]: acc=0.5000 g_loss=0.4795 d_loss=0.7194 kd_loss=0.1127\n",
      "Epoch [01/20] Step [005/127]: acc=0.5000 g_loss=0.4806 d_loss=0.7199 kd_loss=0.1122\n",
      "Epoch [01/20] Step [010/127]: acc=0.5000 g_loss=0.4848 d_loss=0.7176 kd_loss=0.1053\n",
      "Epoch [01/20] Step [015/127]: acc=0.5000 g_loss=0.4876 d_loss=0.7160 kd_loss=0.0609\n",
      "Epoch [01/20] Step [020/127]: acc=0.5000 g_loss=0.4910 d_loss=0.7158 kd_loss=0.0408\n",
      "Epoch [01/20] Step [025/127]: acc=0.5000 g_loss=0.4930 d_loss=0.7147 kd_loss=0.0750\n",
      "Epoch [01/20] Step [030/127]: acc=0.5000 g_loss=0.4952 d_loss=0.7140 kd_loss=0.1513\n",
      "Epoch [01/20] Step [035/127]: acc=0.5000 g_loss=0.4948 d_loss=0.7144 kd_loss=0.0516\n",
      "Epoch [01/20] Step [040/127]: acc=0.5000 g_loss=0.4960 d_loss=0.7121 kd_loss=0.0965\n",
      "Epoch [01/20] Step [045/127]: acc=0.5000 g_loss=0.4901 d_loss=0.7163 kd_loss=0.0103\n",
      "Epoch [01/20] Step [050/127]: acc=0.5000 g_loss=0.4905 d_loss=0.7164 kd_loss=0.0218\n",
      "Epoch [01/20] Step [055/127]: acc=0.5000 g_loss=0.4893 d_loss=0.7159 kd_loss=0.0958\n",
      "Epoch [01/20] Step [060/127]: acc=0.5000 g_loss=0.4895 d_loss=0.7171 kd_loss=0.0902\n",
      "Epoch [01/20] Step [065/127]: acc=0.5000 g_loss=0.4877 d_loss=0.7210 kd_loss=0.0402\n",
      "Epoch [01/20] Step [070/127]: acc=0.5000 g_loss=0.4886 d_loss=0.7187 kd_loss=0.0917\n",
      "Epoch [01/20] Step [075/127]: acc=0.5000 g_loss=0.4905 d_loss=0.7158 kd_loss=0.0415\n",
      "Epoch [01/20] Step [080/127]: acc=0.5000 g_loss=0.4935 d_loss=0.7121 kd_loss=0.0108\n",
      "Epoch [01/20] Step [085/127]: acc=0.5000 g_loss=0.4940 d_loss=0.7123 kd_loss=0.0437\n",
      "Epoch [01/20] Step [090/127]: acc=0.5000 g_loss=0.4951 d_loss=0.7123 kd_loss=0.0317\n",
      "Epoch [01/20] Step [095/127]: acc=0.5000 g_loss=0.4999 d_loss=0.7069 kd_loss=0.0332\n",
      "Epoch [01/20] Step [100/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7099 kd_loss=0.0415\n",
      "Epoch [01/20] Step [105/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7112 kd_loss=0.0647\n",
      "Epoch [01/20] Step [110/127]: acc=0.5000 g_loss=0.4981 d_loss=0.7140 kd_loss=0.0394\n",
      "Epoch [01/20] Step [115/127]: acc=0.5000 g_loss=0.4980 d_loss=0.7093 kd_loss=0.0790\n",
      "Epoch [01/20] Step [120/127]: acc=0.5000 g_loss=0.4957 d_loss=0.7116 kd_loss=0.0122\n",
      "Epoch [01/20] Step [125/127]: acc=0.5000 g_loss=0.4940 d_loss=0.7172 kd_loss=0.0113\n",
      "German Test:\n",
      "Validation loss:  1.38164484500885\n",
      "Accuracy: 0.6444174757281553\n",
      "F1 score (Macro): 0.5316905259089748\n",
      "F1 score (Per class): [0.33766234 0.49315068 0.76425856]\n",
      "Precision score (Per class): [0.31707317 0.45918367 0.7960396 ]\n",
      "Recall score (Per class): [0.36111111 0.53254438 0.73491773]\n",
      "Epoch: 2/20\n",
      "Epoch [02/20] Step [000/127]: acc=0.5000 g_loss=0.4762 d_loss=0.7289 kd_loss=0.0593\n",
      "Epoch [02/20] Step [005/127]: acc=0.5000 g_loss=0.4719 d_loss=0.7326 kd_loss=0.0823\n",
      "Epoch [02/20] Step [010/127]: acc=0.5000 g_loss=0.4762 d_loss=0.7290 kd_loss=0.0956\n",
      "Epoch [02/20] Step [015/127]: acc=0.5000 g_loss=0.4807 d_loss=0.7321 kd_loss=0.0465\n",
      "Epoch [02/20] Step [020/127]: acc=0.5000 g_loss=0.4914 d_loss=0.7227 kd_loss=0.0704\n",
      "Epoch [02/20] Step [025/127]: acc=0.5000 g_loss=0.5014 d_loss=0.7167 kd_loss=0.0488\n",
      "Epoch [02/20] Step [030/127]: acc=0.5000 g_loss=0.5048 d_loss=0.7158 kd_loss=0.0809\n",
      "Epoch [02/20] Step [035/127]: acc=0.5000 g_loss=0.5072 d_loss=0.7138 kd_loss=0.0716\n",
      "Epoch [02/20] Step [040/127]: acc=0.5000 g_loss=0.5095 d_loss=0.7138 kd_loss=0.0653\n",
      "Epoch [02/20] Step [045/127]: acc=0.5000 g_loss=0.5122 d_loss=0.7123 kd_loss=0.1709\n",
      "Epoch [02/20] Step [050/127]: acc=0.5000 g_loss=0.5162 d_loss=0.7095 kd_loss=0.0514\n",
      "Epoch [02/20] Step [055/127]: acc=0.5000 g_loss=0.5131 d_loss=0.7116 kd_loss=0.0621\n",
      "Epoch [02/20] Step [060/127]: acc=0.5000 g_loss=0.5127 d_loss=0.7118 kd_loss=0.0501\n",
      "Epoch [02/20] Step [065/127]: acc=0.5000 g_loss=0.5127 d_loss=0.7132 kd_loss=0.1016\n",
      "Epoch [02/20] Step [070/127]: acc=0.5000 g_loss=0.5112 d_loss=0.7133 kd_loss=0.1678\n",
      "Epoch [02/20] Step [075/127]: acc=0.5000 g_loss=0.5111 d_loss=0.7140 kd_loss=0.0519\n",
      "Epoch [02/20] Step [080/127]: acc=0.5000 g_loss=0.5095 d_loss=0.7172 kd_loss=0.0150\n",
      "Epoch [02/20] Step [085/127]: acc=0.5000 g_loss=0.5095 d_loss=0.7140 kd_loss=0.0314\n",
      "Epoch [02/20] Step [090/127]: acc=0.5000 g_loss=0.5075 d_loss=0.7130 kd_loss=0.0802\n",
      "Epoch [02/20] Step [095/127]: acc=0.5000 g_loss=0.5004 d_loss=0.7212 kd_loss=0.0755\n",
      "Epoch [02/20] Step [100/127]: acc=0.5000 g_loss=0.5019 d_loss=0.7149 kd_loss=0.0326\n",
      "Epoch [02/20] Step [105/127]: acc=0.5000 g_loss=0.4978 d_loss=0.7185 kd_loss=0.1220\n",
      "Epoch [02/20] Step [110/127]: acc=0.5000 g_loss=0.5004 d_loss=0.7144 kd_loss=0.1012\n",
      "Epoch [02/20] Step [115/127]: acc=0.5000 g_loss=0.5015 d_loss=0.7133 kd_loss=0.0238\n",
      "Epoch [02/20] Step [120/127]: acc=0.5000 g_loss=0.4989 d_loss=0.7153 kd_loss=0.0145\n",
      "Epoch [02/20] Step [125/127]: acc=0.5000 g_loss=0.4984 d_loss=0.7135 kd_loss=0.0269\n",
      "German Test:\n",
      "Validation loss:  1.964376449584961\n",
      "Accuracy: 0.4963592233009709\n",
      "F1 score (Macro): 0.43909833694070316\n",
      "F1 score (Per class): [0.30172414 0.38434164 0.63122924]\n",
      "Precision score (Per class): [0.19662921 0.48214286 0.8005618 ]\n",
      "Recall score (Per class): [0.64814815 0.31952663 0.52102377]\n",
      "Epoch: 3/20\n",
      "Epoch [03/20] Step [000/127]: acc=0.5000 g_loss=0.4918 d_loss=0.7190 kd_loss=0.0344\n",
      "Epoch [03/20] Step [005/127]: acc=0.5000 g_loss=0.4891 d_loss=0.7193 kd_loss=0.0742\n",
      "Epoch [03/20] Step [010/127]: acc=0.5000 g_loss=0.4898 d_loss=0.7203 kd_loss=0.1109\n",
      "Epoch [03/20] Step [015/127]: acc=0.5000 g_loss=0.4934 d_loss=0.7184 kd_loss=0.0215\n",
      "Epoch [03/20] Step [020/127]: acc=0.5000 g_loss=0.4964 d_loss=0.7194 kd_loss=0.0608\n",
      "Epoch [03/20] Step [025/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7183 kd_loss=0.0559\n",
      "Epoch [03/20] Step [030/127]: acc=0.5000 g_loss=0.5016 d_loss=0.7164 kd_loss=0.1095\n",
      "Epoch [03/20] Step [035/127]: acc=0.5000 g_loss=0.5037 d_loss=0.7145 kd_loss=0.0458\n",
      "Epoch [03/20] Step [040/127]: acc=0.5000 g_loss=0.5048 d_loss=0.7142 kd_loss=0.0597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [03/20] Step [045/127]: acc=0.5000 g_loss=0.5049 d_loss=0.7137 kd_loss=0.0474\n",
      "Epoch [03/20] Step [050/127]: acc=0.5000 g_loss=0.4997 d_loss=0.7170 kd_loss=0.0356\n",
      "Epoch [03/20] Step [055/127]: acc=0.5000 g_loss=0.5015 d_loss=0.7159 kd_loss=0.1408\n",
      "Epoch [03/20] Step [060/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7177 kd_loss=0.0300\n",
      "Epoch [03/20] Step [065/127]: acc=0.5000 g_loss=0.4964 d_loss=0.7169 kd_loss=0.0544\n",
      "Epoch [03/20] Step [070/127]: acc=0.5000 g_loss=0.4980 d_loss=0.7150 kd_loss=0.0443\n",
      "Epoch [03/20] Step [075/127]: acc=0.5000 g_loss=0.4958 d_loss=0.7145 kd_loss=0.0414\n",
      "Epoch [03/20] Step [080/127]: acc=0.5000 g_loss=0.4934 d_loss=0.7133 kd_loss=0.0045\n",
      "Epoch [03/20] Step [085/127]: acc=0.5000 g_loss=0.4913 d_loss=0.7175 kd_loss=0.0165\n",
      "Epoch [03/20] Step [090/127]: acc=0.5000 g_loss=0.4893 d_loss=0.7195 kd_loss=0.0201\n",
      "Epoch [03/20] Step [095/127]: acc=0.5000 g_loss=0.4892 d_loss=0.7181 kd_loss=0.0154\n",
      "Epoch [03/20] Step [100/127]: acc=0.5000 g_loss=0.4845 d_loss=0.7238 kd_loss=0.0742\n",
      "Epoch [03/20] Step [105/127]: acc=0.5000 g_loss=0.4858 d_loss=0.7232 kd_loss=0.0861\n",
      "Epoch [03/20] Step [110/127]: acc=0.5000 g_loss=0.4879 d_loss=0.7219 kd_loss=0.0374\n",
      "Epoch [03/20] Step [115/127]: acc=0.5000 g_loss=0.4894 d_loss=0.7207 kd_loss=0.0070\n",
      "Epoch [03/20] Step [120/127]: acc=0.5000 g_loss=0.4906 d_loss=0.7214 kd_loss=0.0084\n",
      "Epoch [03/20] Step [125/127]: acc=0.5000 g_loss=0.4949 d_loss=0.7191 kd_loss=0.0142\n",
      "German Test:\n",
      "Validation loss:  1.2792291641235352\n",
      "Accuracy: 0.6783980582524272\n",
      "F1 score (Macro): 0.5151141290271725\n",
      "F1 score (Per class): [0.34666667 0.39525692 0.8034188 ]\n",
      "Precision score (Per class): [0.33333333 0.5952381  0.75441413]\n",
      "Recall score (Per class): [0.36111111 0.29585799 0.85923218]\n",
      "Epoch: 4/20\n",
      "Epoch [04/20] Step [000/127]: acc=0.5000 g_loss=0.4990 d_loss=0.7174 kd_loss=0.0380\n",
      "Epoch [04/20] Step [005/127]: acc=0.5000 g_loss=0.5068 d_loss=0.7137 kd_loss=0.0151\n",
      "Epoch [04/20] Step [010/127]: acc=0.5000 g_loss=0.5104 d_loss=0.7133 kd_loss=0.0401\n",
      "Epoch [04/20] Step [015/127]: acc=0.5000 g_loss=0.5100 d_loss=0.7115 kd_loss=0.0557\n",
      "Epoch [04/20] Step [020/127]: acc=0.5000 g_loss=0.4968 d_loss=0.7186 kd_loss=0.0470\n",
      "Epoch [04/20] Step [025/127]: acc=0.5000 g_loss=0.4883 d_loss=0.7208 kd_loss=0.0420\n",
      "Epoch [04/20] Step [030/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7208 kd_loss=0.0306\n",
      "Epoch [04/20] Step [035/127]: acc=0.5000 g_loss=0.4851 d_loss=0.7216 kd_loss=0.0212\n",
      "Epoch [04/20] Step [040/127]: acc=0.5000 g_loss=0.4843 d_loss=0.7226 kd_loss=0.0354\n",
      "Epoch [04/20] Step [045/127]: acc=0.5000 g_loss=0.4844 d_loss=0.7213 kd_loss=0.0333\n",
      "Epoch [04/20] Step [050/127]: acc=0.5000 g_loss=0.4845 d_loss=0.7215 kd_loss=0.0144\n",
      "Epoch [04/20] Step [055/127]: acc=0.5000 g_loss=0.4848 d_loss=0.7212 kd_loss=0.0690\n",
      "Epoch [04/20] Step [060/127]: acc=0.5000 g_loss=0.4858 d_loss=0.7208 kd_loss=0.0112\n",
      "Epoch [04/20] Step [065/127]: acc=0.5000 g_loss=0.4867 d_loss=0.7204 kd_loss=0.0207\n",
      "Epoch [04/20] Step [070/127]: acc=0.5000 g_loss=0.4874 d_loss=0.7201 kd_loss=0.0750\n",
      "Epoch [04/20] Step [075/127]: acc=0.5000 g_loss=0.4882 d_loss=0.7199 kd_loss=0.0267\n",
      "Epoch [04/20] Step [080/127]: acc=0.5000 g_loss=0.4890 d_loss=0.7199 kd_loss=0.0053\n",
      "Epoch [04/20] Step [085/127]: acc=0.5000 g_loss=0.4900 d_loss=0.7194 kd_loss=0.0234\n",
      "Epoch [04/20] Step [090/127]: acc=0.5000 g_loss=0.4907 d_loss=0.7195 kd_loss=0.0171\n",
      "Epoch [04/20] Step [095/127]: acc=0.5000 g_loss=0.4917 d_loss=0.7191 kd_loss=0.0366\n",
      "Epoch [04/20] Step [100/127]: acc=0.5000 g_loss=0.4926 d_loss=0.7191 kd_loss=0.0552\n",
      "Epoch [04/20] Step [105/127]: acc=0.5000 g_loss=0.4935 d_loss=0.7186 kd_loss=0.0275\n",
      "Epoch [04/20] Step [110/127]: acc=0.5000 g_loss=0.4935 d_loss=0.7186 kd_loss=0.0181\n",
      "Epoch [04/20] Step [115/127]: acc=0.5000 g_loss=0.4929 d_loss=0.7194 kd_loss=0.0082\n",
      "Epoch [04/20] Step [120/127]: acc=0.5000 g_loss=0.4916 d_loss=0.7205 kd_loss=0.0129\n",
      "Epoch [04/20] Step [125/127]: acc=0.5000 g_loss=0.4907 d_loss=0.7196 kd_loss=0.0230\n",
      "German Test:\n",
      "Validation loss:  1.3225224018096924\n",
      "Accuracy: 0.662621359223301\n",
      "F1 score (Macro): 0.5409369779484139\n",
      "F1 score (Per class): [0.37956204 0.45714286 0.78610603]\n",
      "Precision score (Per class): [0.31325301 0.57657658 0.78610603]\n",
      "Recall score (Per class): [0.48148148 0.37869822 0.78610603]\n",
      "Epoch: 5/20\n",
      "Epoch [05/20] Step [000/127]: acc=0.5000 g_loss=0.4907 d_loss=0.7197 kd_loss=0.0134\n",
      "Epoch [05/20] Step [005/127]: acc=0.5000 g_loss=0.4909 d_loss=0.7196 kd_loss=0.0478\n",
      "Epoch [05/20] Step [010/127]: acc=0.5000 g_loss=0.4913 d_loss=0.7205 kd_loss=0.0247\n",
      "Epoch [05/20] Step [015/127]: acc=0.5000 g_loss=0.4915 d_loss=0.7196 kd_loss=0.0131\n",
      "Epoch [05/20] Step [020/127]: acc=0.5000 g_loss=0.4910 d_loss=0.7197 kd_loss=0.0177\n",
      "Epoch [05/20] Step [025/127]: acc=0.5000 g_loss=0.4905 d_loss=0.7199 kd_loss=0.0342\n",
      "Epoch [05/20] Step [030/127]: acc=0.5000 g_loss=0.4896 d_loss=0.7200 kd_loss=0.0473\n",
      "Epoch [05/20] Step [035/127]: acc=0.5000 g_loss=0.4890 d_loss=0.7206 kd_loss=0.0279\n",
      "Epoch [05/20] Step [040/127]: acc=0.5000 g_loss=0.4888 d_loss=0.7211 kd_loss=0.0260\n",
      "Epoch [05/20] Step [045/127]: acc=0.5000 g_loss=0.4886 d_loss=0.7204 kd_loss=0.0274\n",
      "Epoch [05/20] Step [050/127]: acc=0.5000 g_loss=0.4886 d_loss=0.7207 kd_loss=0.0268\n",
      "Epoch [05/20] Step [055/127]: acc=0.5000 g_loss=0.4885 d_loss=0.7206 kd_loss=0.0326\n",
      "Epoch [05/20] Step [060/127]: acc=0.5000 g_loss=0.4885 d_loss=0.7203 kd_loss=0.0221\n",
      "Epoch [05/20] Step [065/127]: acc=0.5000 g_loss=0.4881 d_loss=0.7203 kd_loss=0.0498\n",
      "Epoch [05/20] Step [070/127]: acc=0.5000 g_loss=0.4876 d_loss=0.7206 kd_loss=0.0827\n",
      "Epoch [05/20] Step [075/127]: acc=0.5000 g_loss=0.4871 d_loss=0.7207 kd_loss=0.0251\n",
      "Epoch [05/20] Step [080/127]: acc=0.5000 g_loss=0.4866 d_loss=0.7209 kd_loss=0.0208\n",
      "Epoch [05/20] Step [085/127]: acc=0.5000 g_loss=0.4862 d_loss=0.7210 kd_loss=0.0116\n",
      "Epoch [05/20] Step [090/127]: acc=0.5000 g_loss=0.4858 d_loss=0.7213 kd_loss=0.0320\n",
      "Epoch [05/20] Step [095/127]: acc=0.5000 g_loss=0.4857 d_loss=0.7211 kd_loss=0.0264\n",
      "Epoch [05/20] Step [100/127]: acc=0.5000 g_loss=0.4860 d_loss=0.7213 kd_loss=0.0091\n",
      "Epoch [05/20] Step [105/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7209 kd_loss=0.0265\n",
      "Epoch [05/20] Step [110/127]: acc=0.5000 g_loss=0.4862 d_loss=0.7207 kd_loss=0.0199\n",
      "Epoch [05/20] Step [115/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7213 kd_loss=0.0049\n",
      "Epoch [05/20] Step [120/127]: acc=0.5000 g_loss=0.4860 d_loss=0.7214 kd_loss=0.0096\n",
      "Epoch [05/20] Step [125/127]: acc=0.5000 g_loss=0.4857 d_loss=0.7211 kd_loss=0.0063\n",
      "German Test:\n",
      "Validation loss:  1.2980706691741943\n",
      "Accuracy: 0.6638349514563107\n",
      "F1 score (Macro): 0.5370740587850042\n",
      "F1 score (Per class): [0.38345865 0.44043321 0.78733032]\n",
      "Precision score (Per class): [0.32278481 0.56481481 0.77956989]\n",
      "Recall score (Per class): [0.47222222 0.36094675 0.7952468 ]\n",
      "Epoch: 6/20\n",
      "Epoch [06/20] Step [000/127]: acc=0.5000 g_loss=0.4857 d_loss=0.7211 kd_loss=0.0581\n",
      "Epoch [06/20] Step [005/127]: acc=0.5000 g_loss=0.4860 d_loss=0.7210 kd_loss=0.0396\n",
      "Epoch [06/20] Step [010/127]: acc=0.5000 g_loss=0.4862 d_loss=0.7215 kd_loss=0.0444\n",
      "Epoch [06/20] Step [015/127]: acc=0.5000 g_loss=0.4867 d_loss=0.7210 kd_loss=0.0088\n",
      "Epoch [06/20] Step [020/127]: acc=0.5000 g_loss=0.4868 d_loss=0.7207 kd_loss=0.0126\n",
      "Epoch [06/20] Step [025/127]: acc=0.5000 g_loss=0.4867 d_loss=0.7206 kd_loss=0.0495\n",
      "Epoch [06/20] Step [030/127]: acc=0.5000 g_loss=0.4867 d_loss=0.7206 kd_loss=0.0262\n",
      "Epoch [06/20] Step [035/127]: acc=0.5000 g_loss=0.4866 d_loss=0.7208 kd_loss=0.0123\n",
      "Epoch [06/20] Step [040/127]: acc=0.5000 g_loss=0.4862 d_loss=0.7210 kd_loss=0.0152\n",
      "Epoch [06/20] Step [045/127]: acc=0.5000 g_loss=0.4858 d_loss=0.7209 kd_loss=0.0214\n",
      "Epoch [06/20] Step [050/127]: acc=0.5000 g_loss=0.4856 d_loss=0.7214 kd_loss=0.0080\n",
      "Epoch [06/20] Step [055/127]: acc=0.5000 g_loss=0.4855 d_loss=0.7211 kd_loss=0.0390\n",
      "Epoch [06/20] Step [060/127]: acc=0.5000 g_loss=0.4858 d_loss=0.7208 kd_loss=0.0366\n",
      "Epoch [06/20] Step [065/127]: acc=0.5000 g_loss=0.4861 d_loss=0.7208 kd_loss=0.0232\n",
      "Epoch [06/20] Step [070/127]: acc=0.5000 g_loss=0.4862 d_loss=0.7207 kd_loss=0.0376\n",
      "Epoch [06/20] Step [075/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7207 kd_loss=0.0256\n",
      "Epoch [06/20] Step [080/127]: acc=0.5000 g_loss=0.4864 d_loss=0.7207 kd_loss=0.0119\n",
      "Epoch [06/20] Step [085/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7207 kd_loss=0.0147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [06/20] Step [090/127]: acc=0.5000 g_loss=0.4860 d_loss=0.7209 kd_loss=0.0137\n",
      "Epoch [06/20] Step [095/127]: acc=0.5000 g_loss=0.4858 d_loss=0.7208 kd_loss=0.0214\n",
      "Epoch [06/20] Step [100/127]: acc=0.5000 g_loss=0.4858 d_loss=0.7211 kd_loss=0.0593\n",
      "Epoch [06/20] Step [105/127]: acc=0.5000 g_loss=0.4857 d_loss=0.7208 kd_loss=0.0177\n",
      "Epoch [06/20] Step [110/127]: acc=0.5000 g_loss=0.4854 d_loss=0.7209 kd_loss=0.0065\n",
      "Epoch [06/20] Step [115/127]: acc=0.5000 g_loss=0.4853 d_loss=0.7211 kd_loss=0.0076\n",
      "Epoch [06/20] Step [120/127]: acc=0.5000 g_loss=0.4849 d_loss=0.7213 kd_loss=0.0067\n",
      "Epoch [06/20] Step [125/127]: acc=0.5000 g_loss=0.4848 d_loss=0.7211 kd_loss=0.0107\n",
      "German Test:\n",
      "Validation loss:  1.2111209630966187\n",
      "Accuracy: 0.6783980582524272\n",
      "F1 score (Macro): 0.5412046821196905\n",
      "F1 score (Per class): [0.36363636 0.46315789 0.79681979]\n",
      "Precision score (Per class): [0.34146341 0.56896552 0.77094017]\n",
      "Recall score (Per class): [0.38888889 0.39053254 0.82449726]\n",
      "Epoch: 7/20\n",
      "Epoch [07/20] Step [000/127]: acc=0.5000 g_loss=0.4848 d_loss=0.7211 kd_loss=0.0248\n",
      "Epoch [07/20] Step [005/127]: acc=0.5000 g_loss=0.4849 d_loss=0.7211 kd_loss=0.0383\n",
      "Epoch [07/20] Step [010/127]: acc=0.5000 g_loss=0.4853 d_loss=0.7214 kd_loss=0.0189\n",
      "Epoch [07/20] Step [015/127]: acc=0.5000 g_loss=0.4857 d_loss=0.7209 kd_loss=0.0106\n",
      "Epoch [07/20] Step [020/127]: acc=0.5000 g_loss=0.4859 d_loss=0.7208 kd_loss=0.0130\n",
      "Epoch [07/20] Step [025/127]: acc=0.5000 g_loss=0.4862 d_loss=0.7206 kd_loss=0.0182\n",
      "Epoch [07/20] Step [030/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7206 kd_loss=0.0166\n",
      "Epoch [07/20] Step [035/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7206 kd_loss=0.0124\n",
      "Epoch [07/20] Step [040/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7206 kd_loss=0.0112\n",
      "Epoch [07/20] Step [045/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7206 kd_loss=0.0173\n",
      "Epoch [07/20] Step [050/127]: acc=0.5000 g_loss=0.4862 d_loss=0.7208 kd_loss=0.0179\n",
      "Epoch [07/20] Step [055/127]: acc=0.5000 g_loss=0.4862 d_loss=0.7206 kd_loss=0.0260\n",
      "Epoch [07/20] Step [060/127]: acc=0.5000 g_loss=0.4864 d_loss=0.7205 kd_loss=0.0304\n",
      "Epoch [07/20] Step [065/127]: acc=0.5000 g_loss=0.4867 d_loss=0.7204 kd_loss=0.0342\n",
      "Epoch [07/20] Step [070/127]: acc=0.5000 g_loss=0.4869 d_loss=0.7203 kd_loss=0.0876\n",
      "Epoch [07/20] Step [075/127]: acc=0.5000 g_loss=0.4870 d_loss=0.7203 kd_loss=0.0285\n",
      "Epoch [07/20] Step [080/127]: acc=0.5000 g_loss=0.4872 d_loss=0.7204 kd_loss=0.0147\n",
      "Epoch [07/20] Step [085/127]: acc=0.5000 g_loss=0.4872 d_loss=0.7203 kd_loss=0.0183\n",
      "Epoch [07/20] Step [090/127]: acc=0.5000 g_loss=0.4872 d_loss=0.7203 kd_loss=0.0128\n",
      "Epoch [07/20] Step [095/127]: acc=0.5000 g_loss=0.4871 d_loss=0.7203 kd_loss=0.0240\n",
      "Epoch [07/20] Step [100/127]: acc=0.5000 g_loss=0.4870 d_loss=0.7205 kd_loss=0.0385\n",
      "Epoch [07/20] Step [105/127]: acc=0.5000 g_loss=0.4869 d_loss=0.7204 kd_loss=0.0194\n",
      "Epoch [07/20] Step [110/127]: acc=0.5000 g_loss=0.4867 d_loss=0.7204 kd_loss=0.0154\n",
      "Epoch [07/20] Step [115/127]: acc=0.5000 g_loss=0.4866 d_loss=0.7205 kd_loss=0.0040\n",
      "Epoch [07/20] Step [120/127]: acc=0.5000 g_loss=0.4865 d_loss=0.7206 kd_loss=0.0100\n",
      "Epoch [07/20] Step [125/127]: acc=0.5000 g_loss=0.4865 d_loss=0.7205 kd_loss=0.0062\n",
      "German Test:\n",
      "Validation loss:  1.2192224264144897\n",
      "Accuracy: 0.6747572815533981\n",
      "F1 score (Macro): 0.5345309068619594\n",
      "F1 score (Per class): [0.35497835 0.45390071 0.79471366]\n",
      "Precision score (Per class): [0.33333333 0.56637168 0.7670068 ]\n",
      "Recall score (Per class): [0.37962963 0.37869822 0.82449726]\n",
      "Epoch: 8/20\n",
      "Epoch [08/20] Step [000/127]: acc=0.5000 g_loss=0.4866 d_loss=0.7204 kd_loss=0.0215\n",
      "Epoch [08/20] Step [005/127]: acc=0.5000 g_loss=0.4869 d_loss=0.7204 kd_loss=0.0120\n",
      "Epoch [08/20] Step [010/127]: acc=0.5000 g_loss=0.4870 d_loss=0.7207 kd_loss=0.0344\n",
      "Epoch [08/20] Step [015/127]: acc=0.5000 g_loss=0.4873 d_loss=0.7203 kd_loss=0.0087\n",
      "Epoch [08/20] Step [020/127]: acc=0.5000 g_loss=0.4875 d_loss=0.7203 kd_loss=0.0264\n",
      "Epoch [08/20] Step [025/127]: acc=0.5000 g_loss=0.4877 d_loss=0.7201 kd_loss=0.0225\n",
      "Epoch [08/20] Step [030/127]: acc=0.5000 g_loss=0.4880 d_loss=0.7201 kd_loss=0.0231\n",
      "Epoch [08/20] Step [035/127]: acc=0.5000 g_loss=0.4880 d_loss=0.7201 kd_loss=0.0166\n",
      "Epoch [08/20] Step [040/127]: acc=0.5000 g_loss=0.4880 d_loss=0.7200 kd_loss=0.0097\n",
      "Epoch [08/20] Step [045/127]: acc=0.5000 g_loss=0.4879 d_loss=0.7201 kd_loss=0.0060\n",
      "Epoch [08/20] Step [050/127]: acc=0.5000 g_loss=0.4877 d_loss=0.7203 kd_loss=0.0227\n",
      "Epoch [08/20] Step [055/127]: acc=0.5000 g_loss=0.4876 d_loss=0.7202 kd_loss=0.0216\n",
      "Epoch [08/20] Step [060/127]: acc=0.5000 g_loss=0.4877 d_loss=0.7201 kd_loss=0.0171\n",
      "Epoch [08/20] Step [065/127]: acc=0.5000 g_loss=0.4877 d_loss=0.7201 kd_loss=0.0249\n",
      "Epoch [08/20] Step [070/127]: acc=0.5000 g_loss=0.4877 d_loss=0.7201 kd_loss=0.0166\n",
      "Epoch [08/20] Step [075/127]: acc=0.5000 g_loss=0.4876 d_loss=0.7201 kd_loss=0.0150\n",
      "Epoch [08/20] Step [080/127]: acc=0.5000 g_loss=0.4877 d_loss=0.7202 kd_loss=0.0138\n",
      "Epoch [08/20] Step [085/127]: acc=0.5000 g_loss=0.4877 d_loss=0.7201 kd_loss=0.0222\n",
      "Epoch [08/20] Step [090/127]: acc=0.5000 g_loss=0.4879 d_loss=0.7202 kd_loss=0.0409\n",
      "Epoch [08/20] Step [095/127]: acc=0.5000 g_loss=0.4883 d_loss=0.7199 kd_loss=0.0215\n",
      "Epoch [08/20] Step [100/127]: acc=0.5000 g_loss=0.4886 d_loss=0.7200 kd_loss=0.0376\n",
      "Epoch [08/20] Step [105/127]: acc=0.5000 g_loss=0.4889 d_loss=0.7197 kd_loss=0.0309\n",
      "Epoch [08/20] Step [110/127]: acc=0.5000 g_loss=0.4892 d_loss=0.7197 kd_loss=0.0052\n",
      "Epoch [08/20] Step [115/127]: acc=0.5000 g_loss=0.4893 d_loss=0.7197 kd_loss=0.0051\n",
      "Epoch [08/20] Step [120/127]: acc=0.5000 g_loss=0.4890 d_loss=0.7199 kd_loss=0.0099\n",
      "Epoch [08/20] Step [125/127]: acc=0.5000 g_loss=0.4889 d_loss=0.7198 kd_loss=0.0133\n",
      "German Test:\n",
      "Validation loss:  1.283142328262329\n",
      "Accuracy: 0.6735436893203883\n",
      "F1 score (Macro): 0.5478228355128028\n",
      "F1 score (Per class): [0.4        0.44745763 0.79601088]\n",
      "Precision score (Per class): [0.35211268 0.52380952 0.78956835]\n",
      "Recall score (Per class): [0.46296296 0.39053254 0.80255941]\n",
      "Epoch: 9/20\n",
      "Epoch [09/20] Step [000/127]: acc=0.5000 g_loss=0.4890 d_loss=0.7198 kd_loss=0.0316\n",
      "Epoch [09/20] Step [005/127]: acc=0.5000 g_loss=0.4890 d_loss=0.7198 kd_loss=0.0125\n",
      "Epoch [09/20] Step [010/127]: acc=0.5000 g_loss=0.4889 d_loss=0.7200 kd_loss=0.0385\n",
      "Epoch [09/20] Step [015/127]: acc=0.5000 g_loss=0.4891 d_loss=0.7197 kd_loss=0.0119\n",
      "Epoch [09/20] Step [020/127]: acc=0.5000 g_loss=0.4891 d_loss=0.7197 kd_loss=0.0121\n",
      "Epoch [09/20] Step [025/127]: acc=0.5000 g_loss=0.4891 d_loss=0.7197 kd_loss=0.0269\n",
      "Epoch [09/20] Step [030/127]: acc=0.5000 g_loss=0.4892 d_loss=0.7196 kd_loss=0.0326\n",
      "Epoch [09/20] Step [035/127]: acc=0.5000 g_loss=0.4894 d_loss=0.7196 kd_loss=0.0287\n",
      "Epoch [09/20] Step [040/127]: acc=0.5000 g_loss=0.4895 d_loss=0.7196 kd_loss=0.0080\n",
      "Epoch [09/20] Step [045/127]: acc=0.5000 g_loss=0.4896 d_loss=0.7195 kd_loss=0.0066\n",
      "Epoch [09/20] Step [050/127]: acc=0.5000 g_loss=0.4895 d_loss=0.7196 kd_loss=0.0074\n",
      "Epoch [09/20] Step [055/127]: acc=0.5000 g_loss=0.4894 d_loss=0.7196 kd_loss=0.0131\n",
      "Epoch [09/20] Step [060/127]: acc=0.5000 g_loss=0.4894 d_loss=0.7196 kd_loss=0.0155\n",
      "Epoch [09/20] Step [065/127]: acc=0.5000 g_loss=0.4894 d_loss=0.7196 kd_loss=0.0323\n",
      "Epoch [09/20] Step [070/127]: acc=0.5000 g_loss=0.4894 d_loss=0.7195 kd_loss=0.0509\n",
      "Epoch [09/20] Step [075/127]: acc=0.5000 g_loss=0.4894 d_loss=0.7195 kd_loss=0.0249\n",
      "Epoch [09/20] Step [080/127]: acc=0.5000 g_loss=0.4897 d_loss=0.7195 kd_loss=0.0160\n",
      "Epoch [09/20] Step [085/127]: acc=0.5000 g_loss=0.4900 d_loss=0.7194 kd_loss=0.0209\n",
      "Epoch [09/20] Step [090/127]: acc=0.5000 g_loss=0.4901 d_loss=0.7194 kd_loss=0.0138\n",
      "Epoch [09/20] Step [095/127]: acc=0.5000 g_loss=0.4902 d_loss=0.7193 kd_loss=0.0353\n",
      "Epoch [09/20] Step [100/127]: acc=0.5000 g_loss=0.4906 d_loss=0.7193 kd_loss=0.0357\n",
      "Epoch [09/20] Step [105/127]: acc=0.5000 g_loss=0.4908 d_loss=0.7192 kd_loss=0.0256\n",
      "Epoch [09/20] Step [110/127]: acc=0.5000 g_loss=0.4909 d_loss=0.7191 kd_loss=0.0131\n",
      "Epoch [09/20] Step [115/127]: acc=0.5000 g_loss=0.4911 d_loss=0.7191 kd_loss=0.0233\n",
      "Epoch [09/20] Step [120/127]: acc=0.5000 g_loss=0.4910 d_loss=0.7192 kd_loss=0.0075\n",
      "Epoch [09/20] Step [125/127]: acc=0.5000 g_loss=0.4907 d_loss=0.7192 kd_loss=0.0055\n",
      "German Test:\n",
      "Validation loss:  1.2611358165740967\n",
      "Accuracy: 0.6759708737864077\n",
      "F1 score (Macro): 0.5344734051231849\n",
      "F1 score (Per class): [0.35555556 0.45138889 0.79647577]\n",
      "Precision score (Per class): [0.34188034 0.54621849 0.76870748]\n",
      "Recall score (Per class): [0.37037037 0.38461538 0.82632541]\n",
      "Epoch: 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Step [000/127]: acc=0.5000 g_loss=0.4907 d_loss=0.7192 kd_loss=0.0129\n",
      "Epoch [10/20] Step [005/127]: acc=0.5000 g_loss=0.4908 d_loss=0.7192 kd_loss=0.0143\n",
      "Epoch [10/20] Step [010/127]: acc=0.5000 g_loss=0.4908 d_loss=0.7194 kd_loss=0.0369\n",
      "Epoch [10/20] Step [015/127]: acc=0.5000 g_loss=0.4910 d_loss=0.7191 kd_loss=0.0087\n",
      "Epoch [10/20] Step [020/127]: acc=0.5000 g_loss=0.4910 d_loss=0.7192 kd_loss=0.0144\n",
      "Epoch [10/20] Step [025/127]: acc=0.5000 g_loss=0.4910 d_loss=0.7191 kd_loss=0.0130\n",
      "Epoch [10/20] Step [030/127]: acc=0.5000 g_loss=0.4910 d_loss=0.7191 kd_loss=0.0288\n",
      "Epoch [10/20] Step [035/127]: acc=0.5000 g_loss=0.4912 d_loss=0.7191 kd_loss=0.0168\n",
      "Epoch [10/20] Step [040/127]: acc=0.5000 g_loss=0.4912 d_loss=0.7191 kd_loss=0.0103\n",
      "Epoch [10/20] Step [045/127]: acc=0.5000 g_loss=0.4911 d_loss=0.7191 kd_loss=0.0083\n",
      "Epoch [10/20] Step [050/127]: acc=0.5000 g_loss=0.4910 d_loss=0.7192 kd_loss=0.0084\n",
      "Epoch [10/20] Step [055/127]: acc=0.5000 g_loss=0.4910 d_loss=0.7192 kd_loss=0.0392\n",
      "Epoch [10/20] Step [060/127]: acc=0.5000 g_loss=0.4911 d_loss=0.7191 kd_loss=0.0347\n",
      "Epoch [10/20] Step [065/127]: acc=0.5000 g_loss=0.4911 d_loss=0.7191 kd_loss=0.0101\n",
      "Epoch [10/20] Step [070/127]: acc=0.5000 g_loss=0.4911 d_loss=0.7191 kd_loss=0.0219\n",
      "Epoch [10/20] Step [075/127]: acc=0.5000 g_loss=0.4912 d_loss=0.7190 kd_loss=0.0090\n",
      "Epoch [10/20] Step [080/127]: acc=0.5000 g_loss=0.4913 d_loss=0.7190 kd_loss=0.0059\n",
      "Epoch [10/20] Step [085/127]: acc=0.5000 g_loss=0.4913 d_loss=0.7190 kd_loss=0.0157\n",
      "Epoch [10/20] Step [090/127]: acc=0.5000 g_loss=0.4917 d_loss=0.7189 kd_loss=0.0380\n",
      "Epoch [10/20] Step [095/127]: acc=0.5000 g_loss=0.4923 d_loss=0.7187 kd_loss=0.0112\n",
      "Epoch [10/20] Step [100/127]: acc=0.5000 g_loss=0.4924 d_loss=0.7187 kd_loss=0.0288\n",
      "Epoch [10/20] Step [105/127]: acc=0.5000 g_loss=0.4925 d_loss=0.7186 kd_loss=0.0356\n",
      "Epoch [10/20] Step [110/127]: acc=0.5000 g_loss=0.4924 d_loss=0.7186 kd_loss=0.0111\n",
      "Epoch [10/20] Step [115/127]: acc=0.5000 g_loss=0.4921 d_loss=0.7188 kd_loss=0.0093\n",
      "Epoch [10/20] Step [120/127]: acc=0.5000 g_loss=0.4917 d_loss=0.7190 kd_loss=0.0080\n",
      "Epoch [10/20] Step [125/127]: acc=0.5000 g_loss=0.4913 d_loss=0.7190 kd_loss=0.0193\n",
      "German Test:\n",
      "Validation loss:  1.225843906402588\n",
      "Accuracy: 0.6747572815533981\n",
      "F1 score (Macro): 0.5362306229255498\n",
      "F1 score (Per class): [0.36363636 0.45070423 0.79435128]\n",
      "Precision score (Per class): [0.34146341 0.55652174 0.76791809]\n",
      "Recall score (Per class): [0.38888889 0.37869822 0.8226691 ]\n",
      "Epoch: 11/20\n",
      "Epoch [11/20] Step [000/127]: acc=0.5000 g_loss=0.4915 d_loss=0.7190 kd_loss=0.0209\n",
      "Epoch [11/20] Step [005/127]: acc=0.5000 g_loss=0.4920 d_loss=0.7188 kd_loss=0.0395\n",
      "Epoch [11/20] Step [010/127]: acc=0.5000 g_loss=0.4923 d_loss=0.7189 kd_loss=0.0285\n",
      "Epoch [11/20] Step [015/127]: acc=0.5000 g_loss=0.4928 d_loss=0.7186 kd_loss=0.0071\n",
      "Epoch [11/20] Step [020/127]: acc=0.5000 g_loss=0.4929 d_loss=0.7186 kd_loss=0.0107\n",
      "Epoch [11/20] Step [025/127]: acc=0.5000 g_loss=0.4928 d_loss=0.7186 kd_loss=0.0102\n",
      "Epoch [11/20] Step [030/127]: acc=0.5000 g_loss=0.4927 d_loss=0.7187 kd_loss=0.0134\n",
      "Epoch [11/20] Step [035/127]: acc=0.5000 g_loss=0.4924 d_loss=0.7188 kd_loss=0.0140\n",
      "Epoch [11/20] Step [040/127]: acc=0.5000 g_loss=0.4921 d_loss=0.7188 kd_loss=0.0146\n",
      "Epoch [11/20] Step [045/127]: acc=0.5000 g_loss=0.4919 d_loss=0.7190 kd_loss=0.0085\n",
      "Epoch [11/20] Step [050/127]: acc=0.5000 g_loss=0.4917 d_loss=0.7191 kd_loss=0.0076\n",
      "Epoch [11/20] Step [055/127]: acc=0.5000 g_loss=0.4917 d_loss=0.7190 kd_loss=0.0158\n",
      "Epoch [11/20] Step [060/127]: acc=0.5000 g_loss=0.4922 d_loss=0.7188 kd_loss=0.0199\n",
      "Epoch [11/20] Step [065/127]: acc=0.5000 g_loss=0.4926 d_loss=0.7187 kd_loss=0.0226\n",
      "Epoch [11/20] Step [070/127]: acc=0.5000 g_loss=0.4931 d_loss=0.7186 kd_loss=0.0544\n",
      "Epoch [11/20] Step [075/127]: acc=0.5000 g_loss=0.4936 d_loss=0.7184 kd_loss=0.0095\n",
      "Epoch [11/20] Step [080/127]: acc=0.5000 g_loss=0.4939 d_loss=0.7183 kd_loss=0.0077\n",
      "Epoch [11/20] Step [085/127]: acc=0.5000 g_loss=0.4942 d_loss=0.7182 kd_loss=0.0052\n",
      "Epoch [11/20] Step [090/127]: acc=0.5000 g_loss=0.4942 d_loss=0.7182 kd_loss=0.0083\n",
      "Epoch [11/20] Step [095/127]: acc=0.5000 g_loss=0.4943 d_loss=0.7182 kd_loss=0.0198\n",
      "Epoch [11/20] Step [100/127]: acc=0.5000 g_loss=0.4943 d_loss=0.7182 kd_loss=0.0150\n",
      "Epoch [11/20] Step [105/127]: acc=0.5000 g_loss=0.4942 d_loss=0.7181 kd_loss=0.0610\n",
      "Epoch [11/20] Step [110/127]: acc=0.5000 g_loss=0.4943 d_loss=0.7181 kd_loss=0.0191\n",
      "Epoch [11/20] Step [115/127]: acc=0.5000 g_loss=0.4942 d_loss=0.7182 kd_loss=0.0070\n",
      "Epoch [11/20] Step [120/127]: acc=0.5000 g_loss=0.4940 d_loss=0.7182 kd_loss=0.0141\n",
      "Epoch [11/20] Step [125/127]: acc=0.5000 g_loss=0.4937 d_loss=0.7183 kd_loss=0.0039\n",
      "German Test:\n",
      "Validation loss:  1.1905534267425537\n",
      "Accuracy: 0.6747572815533981\n",
      "F1 score (Macro): 0.5579739819827778\n",
      "F1 score (Per class): [0.39837398 0.48253968 0.79300828]\n",
      "Precision score (Per class): [0.35507246 0.52054795 0.79814815]\n",
      "Recall score (Per class): [0.4537037  0.44970414 0.78793419]\n",
      "Epoch: 12/20\n",
      "Epoch [12/20] Step [000/127]: acc=0.5000 g_loss=0.4938 d_loss=0.7183 kd_loss=0.0121\n",
      "Epoch [12/20] Step [005/127]: acc=0.5000 g_loss=0.4942 d_loss=0.7182 kd_loss=0.0180\n",
      "Epoch [12/20] Step [010/127]: acc=0.5000 g_loss=0.4944 d_loss=0.7182 kd_loss=0.0513\n",
      "Epoch [12/20] Step [015/127]: acc=0.5000 g_loss=0.4949 d_loss=0.7180 kd_loss=0.0188\n",
      "Epoch [12/20] Step [020/127]: acc=0.5000 g_loss=0.4950 d_loss=0.7180 kd_loss=0.0175\n",
      "Epoch [12/20] Step [025/127]: acc=0.5000 g_loss=0.4948 d_loss=0.7181 kd_loss=0.0332\n",
      "Epoch [12/20] Step [030/127]: acc=0.5000 g_loss=0.4947 d_loss=0.7181 kd_loss=0.0361\n",
      "Epoch [12/20] Step [035/127]: acc=0.5000 g_loss=0.4945 d_loss=0.7182 kd_loss=0.0103\n",
      "Epoch [12/20] Step [040/127]: acc=0.5000 g_loss=0.4938 d_loss=0.7183 kd_loss=0.0089\n",
      "Epoch [12/20] Step [045/127]: acc=0.5000 g_loss=0.4933 d_loss=0.7185 kd_loss=0.0060\n",
      "Epoch [12/20] Step [050/127]: acc=0.5000 g_loss=0.4929 d_loss=0.7187 kd_loss=0.0067\n",
      "Epoch [12/20] Step [055/127]: acc=0.5000 g_loss=0.4926 d_loss=0.7187 kd_loss=0.0142\n",
      "Epoch [12/20] Step [060/127]: acc=0.5000 g_loss=0.4927 d_loss=0.7187 kd_loss=0.0192\n",
      "Epoch [12/20] Step [065/127]: acc=0.5000 g_loss=0.4931 d_loss=0.7186 kd_loss=0.0243\n",
      "Epoch [12/20] Step [070/127]: acc=0.5000 g_loss=0.4934 d_loss=0.7185 kd_loss=0.0234\n",
      "Epoch [12/20] Step [075/127]: acc=0.5000 g_loss=0.4939 d_loss=0.7183 kd_loss=0.0196\n",
      "Epoch [12/20] Step [080/127]: acc=0.5000 g_loss=0.4943 d_loss=0.7182 kd_loss=0.0037\n",
      "Epoch [12/20] Step [085/127]: acc=0.5000 g_loss=0.4946 d_loss=0.7182 kd_loss=0.0156\n",
      "Epoch [12/20] Step [090/127]: acc=0.5000 g_loss=0.4948 d_loss=0.7181 kd_loss=0.0157\n",
      "Epoch [12/20] Step [095/127]: acc=0.5000 g_loss=0.4951 d_loss=0.7180 kd_loss=0.0089\n",
      "Epoch [12/20] Step [100/127]: acc=0.5000 g_loss=0.4954 d_loss=0.7179 kd_loss=0.0595\n",
      "Epoch [12/20] Step [105/127]: acc=0.5000 g_loss=0.4963 d_loss=0.7176 kd_loss=0.0325\n",
      "Epoch [12/20] Step [110/127]: acc=0.5000 g_loss=0.4969 d_loss=0.7174 kd_loss=0.0068\n",
      "Epoch [12/20] Step [115/127]: acc=0.5000 g_loss=0.4970 d_loss=0.7174 kd_loss=0.0049\n",
      "Epoch [12/20] Step [120/127]: acc=0.5000 g_loss=0.4966 d_loss=0.7176 kd_loss=0.0052\n",
      "Epoch [12/20] Step [125/127]: acc=0.5000 g_loss=0.4960 d_loss=0.7177 kd_loss=0.0083\n",
      "German Test:\n",
      "Validation loss:  1.2052733898162842\n",
      "Accuracy: 0.6723300970873787\n",
      "F1 score (Macro): 0.545227269651822\n",
      "F1 score (Per class): [0.37815126 0.46405229 0.79347826]\n",
      "Precision score (Per class): [0.34615385 0.51824818 0.78635548]\n",
      "Recall score (Per class): [0.41666667 0.42011834 0.80073126]\n",
      "Epoch: 13/20\n",
      "Epoch [13/20] Step [000/127]: acc=0.5000 g_loss=0.4960 d_loss=0.7177 kd_loss=0.0106\n",
      "Epoch [13/20] Step [005/127]: acc=0.5000 g_loss=0.4960 d_loss=0.7177 kd_loss=0.0215\n",
      "Epoch [13/20] Step [010/127]: acc=0.5000 g_loss=0.4959 d_loss=0.7178 kd_loss=0.0137\n",
      "Epoch [13/20] Step [015/127]: acc=0.5000 g_loss=0.4955 d_loss=0.7178 kd_loss=0.0121\n",
      "Epoch [13/20] Step [020/127]: acc=0.5000 g_loss=0.4952 d_loss=0.7179 kd_loss=0.0198\n",
      "Epoch [13/20] Step [025/127]: acc=0.5000 g_loss=0.4949 d_loss=0.7180 kd_loss=0.0105\n",
      "Epoch [13/20] Step [030/127]: acc=0.5000 g_loss=0.4949 d_loss=0.7180 kd_loss=0.0204\n",
      "Epoch [13/20] Step [035/127]: acc=0.5000 g_loss=0.4950 d_loss=0.7180 kd_loss=0.0087\n",
      "Epoch [13/20] Step [040/127]: acc=0.5000 g_loss=0.4951 d_loss=0.7180 kd_loss=0.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Step [045/127]: acc=0.5000 g_loss=0.4953 d_loss=0.7179 kd_loss=0.0085\n",
      "Epoch [13/20] Step [050/127]: acc=0.5000 g_loss=0.4957 d_loss=0.7180 kd_loss=0.0049\n",
      "Epoch [13/20] Step [055/127]: acc=0.5000 g_loss=0.4960 d_loss=0.7178 kd_loss=0.0214\n",
      "Epoch [13/20] Step [060/127]: acc=0.5000 g_loss=0.4966 d_loss=0.7176 kd_loss=0.0111\n",
      "Epoch [13/20] Step [065/127]: acc=0.5000 g_loss=0.4968 d_loss=0.7176 kd_loss=0.0143\n",
      "Epoch [13/20] Step [070/127]: acc=0.5000 g_loss=0.4969 d_loss=0.7175 kd_loss=0.0830\n",
      "Epoch [13/20] Step [075/127]: acc=0.5000 g_loss=0.4971 d_loss=0.7175 kd_loss=0.0099\n",
      "Epoch [13/20] Step [080/127]: acc=0.5000 g_loss=0.4971 d_loss=0.7175 kd_loss=0.0077\n",
      "Epoch [13/20] Step [085/127]: acc=0.5000 g_loss=0.4968 d_loss=0.7176 kd_loss=0.0119\n",
      "Epoch [13/20] Step [090/127]: acc=0.5000 g_loss=0.4963 d_loss=0.7177 kd_loss=0.0086\n",
      "Epoch [13/20] Step [095/127]: acc=0.5000 g_loss=0.4959 d_loss=0.7178 kd_loss=0.0172\n",
      "Epoch [13/20] Step [100/127]: acc=0.5000 g_loss=0.4959 d_loss=0.7178 kd_loss=0.0061\n",
      "Epoch [13/20] Step [105/127]: acc=0.5000 g_loss=0.4958 d_loss=0.7178 kd_loss=0.0247\n",
      "Epoch [13/20] Step [110/127]: acc=0.5000 g_loss=0.4960 d_loss=0.7177 kd_loss=0.0065\n",
      "Epoch [13/20] Step [115/127]: acc=0.5000 g_loss=0.4963 d_loss=0.7176 kd_loss=0.0044\n",
      "Epoch [13/20] Step [120/127]: acc=0.5000 g_loss=0.4964 d_loss=0.7177 kd_loss=0.0103\n",
      "Epoch [13/20] Step [125/127]: acc=0.5000 g_loss=0.4963 d_loss=0.7176 kd_loss=0.0083\n",
      "German Test:\n",
      "Validation loss:  1.1824818849563599\n",
      "Accuracy: 0.683252427184466\n",
      "F1 score (Macro): 0.5253033168455491\n",
      "F1 score (Per class): [0.32195122 0.44912281 0.80483592]\n",
      "Precision score (Per class): [0.34020619 0.55172414 0.76268412]\n",
      "Recall score (Per class): [0.30555556 0.37869822 0.85191956]\n",
      "Epoch: 14/20\n",
      "Epoch [14/20] Step [000/127]: acc=0.5000 g_loss=0.4965 d_loss=0.7176 kd_loss=0.0061\n",
      "Epoch [14/20] Step [005/127]: acc=0.5000 g_loss=0.4968 d_loss=0.7175 kd_loss=0.0158\n",
      "Epoch [14/20] Step [010/127]: acc=0.5000 g_loss=0.4967 d_loss=0.7176 kd_loss=0.0131\n",
      "Epoch [14/20] Step [015/127]: acc=0.5000 g_loss=0.4967 d_loss=0.7175 kd_loss=0.0076\n",
      "Epoch [14/20] Step [020/127]: acc=0.5000 g_loss=0.4965 d_loss=0.7176 kd_loss=0.0147\n",
      "Epoch [14/20] Step [025/127]: acc=0.5000 g_loss=0.4964 d_loss=0.7176 kd_loss=0.0237\n",
      "Epoch [14/20] Step [030/127]: acc=0.5000 g_loss=0.4969 d_loss=0.7174 kd_loss=0.0318\n",
      "Epoch [14/20] Step [035/127]: acc=0.5000 g_loss=0.4973 d_loss=0.7173 kd_loss=0.0107\n",
      "Epoch [14/20] Step [040/127]: acc=0.5000 g_loss=0.4974 d_loss=0.7173 kd_loss=0.0117\n",
      "Epoch [14/20] Step [045/127]: acc=0.5000 g_loss=0.4972 d_loss=0.7174 kd_loss=0.0183\n",
      "Epoch [14/20] Step [050/127]: acc=0.5000 g_loss=0.4970 d_loss=0.7175 kd_loss=0.0052\n",
      "Epoch [14/20] Step [055/127]: acc=0.5000 g_loss=0.4969 d_loss=0.7175 kd_loss=0.0147\n",
      "Epoch [14/20] Step [060/127]: acc=0.5000 g_loss=0.4971 d_loss=0.7174 kd_loss=0.0073\n",
      "Epoch [14/20] Step [065/127]: acc=0.5000 g_loss=0.4974 d_loss=0.7173 kd_loss=0.0104\n",
      "Epoch [14/20] Step [070/127]: acc=0.5000 g_loss=0.4976 d_loss=0.7173 kd_loss=0.0233\n",
      "Epoch [14/20] Step [075/127]: acc=0.5000 g_loss=0.4979 d_loss=0.7172 kd_loss=0.0084\n",
      "Epoch [14/20] Step [080/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7171 kd_loss=0.0061\n",
      "Epoch [14/20] Step [085/127]: acc=0.5000 g_loss=0.4985 d_loss=0.7171 kd_loss=0.0073\n",
      "Epoch [14/20] Step [090/127]: acc=0.5000 g_loss=0.4985 d_loss=0.7171 kd_loss=0.0115\n",
      "Epoch [14/20] Step [095/127]: acc=0.5000 g_loss=0.4986 d_loss=0.7170 kd_loss=0.0128\n",
      "Epoch [14/20] Step [100/127]: acc=0.5000 g_loss=0.4986 d_loss=0.7171 kd_loss=0.0073\n",
      "Epoch [14/20] Step [105/127]: acc=0.5000 g_loss=0.4984 d_loss=0.7171 kd_loss=0.0269\n",
      "Epoch [14/20] Step [110/127]: acc=0.5000 g_loss=0.4982 d_loss=0.7171 kd_loss=0.0075\n",
      "Epoch [14/20] Step [115/127]: acc=0.5000 g_loss=0.4979 d_loss=0.7172 kd_loss=0.0053\n",
      "Epoch [14/20] Step [120/127]: acc=0.5000 g_loss=0.4974 d_loss=0.7174 kd_loss=0.0078\n",
      "Epoch [14/20] Step [125/127]: acc=0.5000 g_loss=0.4972 d_loss=0.7174 kd_loss=0.0105\n",
      "German Test:\n",
      "Validation loss:  1.2339057922363281\n",
      "Accuracy: 0.654126213592233\n",
      "F1 score (Macro): 0.5391737371277013\n",
      "F1 score (Per class): [0.36862745 0.47204969 0.77684407]\n",
      "Precision score (Per class): [0.31972789 0.49673203 0.79389313]\n",
      "Recall score (Per class): [0.43518519 0.44970414 0.76051188]\n",
      "Epoch: 15/20\n",
      "Epoch [15/20] Step [000/127]: acc=0.5000 g_loss=0.4976 d_loss=0.7174 kd_loss=0.0143\n",
      "Epoch [15/20] Step [005/127]: acc=0.5000 g_loss=0.4979 d_loss=0.7173 kd_loss=0.0122\n",
      "Epoch [15/20] Step [010/127]: acc=0.5000 g_loss=0.4977 d_loss=0.7174 kd_loss=0.0150\n",
      "Epoch [15/20] Step [015/127]: acc=0.5000 g_loss=0.4976 d_loss=0.7173 kd_loss=0.0032\n",
      "Epoch [15/20] Step [020/127]: acc=0.5000 g_loss=0.4973 d_loss=0.7173 kd_loss=0.0064\n",
      "Epoch [15/20] Step [025/127]: acc=0.5000 g_loss=0.4968 d_loss=0.7175 kd_loss=0.0068\n",
      "Epoch [15/20] Step [030/127]: acc=0.5000 g_loss=0.4964 d_loss=0.7176 kd_loss=0.0107\n",
      "Epoch [15/20] Step [035/127]: acc=0.5000 g_loss=0.4963 d_loss=0.7177 kd_loss=0.0060\n",
      "Epoch [15/20] Step [040/127]: acc=0.5000 g_loss=0.4963 d_loss=0.7176 kd_loss=0.0033\n",
      "Epoch [15/20] Step [045/127]: acc=0.5000 g_loss=0.4965 d_loss=0.7175 kd_loss=0.0108\n",
      "Epoch [15/20] Step [050/127]: acc=0.5000 g_loss=0.4969 d_loss=0.7175 kd_loss=0.0104\n",
      "Epoch [15/20] Step [055/127]: acc=0.5000 g_loss=0.4973 d_loss=0.7174 kd_loss=0.0145\n",
      "Epoch [15/20] Step [060/127]: acc=0.5000 g_loss=0.4977 d_loss=0.7172 kd_loss=0.0056\n",
      "Epoch [15/20] Step [065/127]: acc=0.5000 g_loss=0.4979 d_loss=0.7173 kd_loss=0.0096\n",
      "Epoch [15/20] Step [070/127]: acc=0.5000 g_loss=0.4980 d_loss=0.7172 kd_loss=0.0367\n",
      "Epoch [15/20] Step [075/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7171 kd_loss=0.0078\n",
      "Epoch [15/20] Step [080/127]: acc=0.5000 g_loss=0.4984 d_loss=0.7171 kd_loss=0.0066\n",
      "Epoch [15/20] Step [085/127]: acc=0.5000 g_loss=0.4984 d_loss=0.7171 kd_loss=0.0041\n",
      "Epoch [15/20] Step [090/127]: acc=0.5000 g_loss=0.4986 d_loss=0.7171 kd_loss=0.0134\n",
      "Epoch [15/20] Step [095/127]: acc=0.5000 g_loss=0.4987 d_loss=0.7171 kd_loss=0.0114\n",
      "Epoch [15/20] Step [100/127]: acc=0.5000 g_loss=0.4989 d_loss=0.7170 kd_loss=0.0148\n",
      "Epoch [15/20] Step [105/127]: acc=0.5000 g_loss=0.4994 d_loss=0.7168 kd_loss=0.0222\n",
      "Epoch [15/20] Step [110/127]: acc=0.5000 g_loss=0.4998 d_loss=0.7167 kd_loss=0.0083\n",
      "Epoch [15/20] Step [115/127]: acc=0.5000 g_loss=0.4998 d_loss=0.7167 kd_loss=0.0066\n",
      "Epoch [15/20] Step [120/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7168 kd_loss=0.0100\n",
      "Epoch [15/20] Step [125/127]: acc=0.5000 g_loss=0.4992 d_loss=0.7169 kd_loss=0.0041\n",
      "German Test:\n",
      "Validation loss:  1.245785117149353\n",
      "Accuracy: 0.6553398058252428\n",
      "F1 score (Macro): 0.5500135608184873\n",
      "F1 score (Per class): [0.40433213 0.47096774 0.77474081]\n",
      "Precision score (Per class): [0.33136095 0.5177305  0.79961089]\n",
      "Recall score (Per class): [0.51851852 0.43195266 0.75137112]\n",
      "Epoch: 16/20\n",
      "Epoch [16/20] Step [000/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7168 kd_loss=0.0093\n",
      "Epoch [16/20] Step [005/127]: acc=0.5000 g_loss=0.5000 d_loss=0.7167 kd_loss=0.0120\n",
      "Epoch [16/20] Step [010/127]: acc=0.5000 g_loss=0.4999 d_loss=0.7168 kd_loss=0.0097\n",
      "Epoch [16/20] Step [015/127]: acc=0.5000 g_loss=0.4994 d_loss=0.7169 kd_loss=0.0114\n",
      "Epoch [16/20] Step [020/127]: acc=0.5000 g_loss=0.4989 d_loss=0.7170 kd_loss=0.0082\n",
      "Epoch [16/20] Step [025/127]: acc=0.5000 g_loss=0.4984 d_loss=0.7171 kd_loss=0.0079\n",
      "Epoch [16/20] Step [030/127]: acc=0.5000 g_loss=0.4980 d_loss=0.7172 kd_loss=0.0301\n",
      "Epoch [16/20] Step [035/127]: acc=0.5000 g_loss=0.4981 d_loss=0.7172 kd_loss=0.0044\n",
      "Epoch [16/20] Step [040/127]: acc=0.5000 g_loss=0.4978 d_loss=0.7172 kd_loss=0.0058\n",
      "Epoch [16/20] Step [045/127]: acc=0.5000 g_loss=0.4975 d_loss=0.7173 kd_loss=0.0041\n",
      "Epoch [16/20] Step [050/127]: acc=0.5000 g_loss=0.4972 d_loss=0.7176 kd_loss=0.0051\n",
      "Epoch [16/20] Step [055/127]: acc=0.5000 g_loss=0.4972 d_loss=0.7175 kd_loss=0.0156\n",
      "Epoch [16/20] Step [060/127]: acc=0.5000 g_loss=0.4979 d_loss=0.7173 kd_loss=0.0065\n",
      "Epoch [16/20] Step [065/127]: acc=0.5000 g_loss=0.4985 d_loss=0.7171 kd_loss=0.0108\n",
      "Epoch [16/20] Step [070/127]: acc=0.5000 g_loss=0.4989 d_loss=0.7170 kd_loss=0.0325\n",
      "Epoch [16/20] Step [075/127]: acc=0.5000 g_loss=0.4991 d_loss=0.7169 kd_loss=0.0126\n",
      "Epoch [16/20] Step [080/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7168 kd_loss=0.0072\n",
      "Epoch [16/20] Step [085/127]: acc=0.5000 g_loss=0.4997 d_loss=0.7168 kd_loss=0.0093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Step [090/127]: acc=0.5000 g_loss=0.4997 d_loss=0.7168 kd_loss=0.0073\n",
      "Epoch [16/20] Step [095/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7168 kd_loss=0.0060\n",
      "Epoch [16/20] Step [100/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7168 kd_loss=0.0081\n",
      "Epoch [16/20] Step [105/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7168 kd_loss=0.0107\n",
      "Epoch [16/20] Step [110/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7168 kd_loss=0.0030\n",
      "Epoch [16/20] Step [115/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7168 kd_loss=0.0039\n",
      "Epoch [16/20] Step [120/127]: acc=0.5000 g_loss=0.4992 d_loss=0.7169 kd_loss=0.0059\n",
      "Epoch [16/20] Step [125/127]: acc=0.5000 g_loss=0.4989 d_loss=0.7170 kd_loss=0.0080\n",
      "German Test:\n",
      "Validation loss:  1.218925952911377\n",
      "Accuracy: 0.6735436893203883\n",
      "F1 score (Macro): 0.5440293690539146\n",
      "F1 score (Per class): [0.3776824  0.46052632 0.79387939]\n",
      "Precision score (Per class): [0.352      0.51851852 0.78191489]\n",
      "Recall score (Per class): [0.40740741 0.41420118 0.80621572]\n",
      "Epoch: 17/20\n",
      "Epoch [17/20] Step [000/127]: acc=0.5000 g_loss=0.4992 d_loss=0.7169 kd_loss=0.0098\n",
      "Epoch [17/20] Step [005/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7168 kd_loss=0.0055\n",
      "Epoch [17/20] Step [010/127]: acc=0.5000 g_loss=0.4997 d_loss=0.7168 kd_loss=0.0216\n",
      "Epoch [17/20] Step [015/127]: acc=0.5000 g_loss=0.4999 d_loss=0.7167 kd_loss=0.0062\n",
      "Epoch [17/20] Step [020/127]: acc=0.5000 g_loss=0.4998 d_loss=0.7167 kd_loss=0.0068\n",
      "Epoch [17/20] Step [025/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7168 kd_loss=0.0091\n",
      "Epoch [17/20] Step [030/127]: acc=0.5000 g_loss=0.4993 d_loss=0.7169 kd_loss=0.0207\n",
      "Epoch [17/20] Step [035/127]: acc=0.5000 g_loss=0.4993 d_loss=0.7169 kd_loss=0.0058\n",
      "Epoch [17/20] Step [040/127]: acc=0.5000 g_loss=0.4991 d_loss=0.7169 kd_loss=0.0070\n",
      "Epoch [17/20] Step [045/127]: acc=0.5000 g_loss=0.4989 d_loss=0.7170 kd_loss=0.0064\n",
      "Epoch [17/20] Step [050/127]: acc=0.5000 g_loss=0.4986 d_loss=0.7171 kd_loss=0.0084\n",
      "Epoch [17/20] Step [055/127]: acc=0.5000 g_loss=0.4985 d_loss=0.7171 kd_loss=0.0041\n",
      "Epoch [17/20] Step [060/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7171 kd_loss=0.0033\n",
      "Epoch [17/20] Step [065/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7171 kd_loss=0.0064\n",
      "Epoch [17/20] Step [070/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7172 kd_loss=0.0083\n",
      "Epoch [17/20] Step [075/127]: acc=0.5000 g_loss=0.4985 d_loss=0.7171 kd_loss=0.0061\n",
      "Epoch [17/20] Step [080/127]: acc=0.5000 g_loss=0.4991 d_loss=0.7169 kd_loss=0.0089\n",
      "Epoch [17/20] Step [085/127]: acc=0.5000 g_loss=0.4997 d_loss=0.7168 kd_loss=0.0039\n",
      "Epoch [17/20] Step [090/127]: acc=0.5000 g_loss=0.5001 d_loss=0.7167 kd_loss=0.0047\n",
      "Epoch [17/20] Step [095/127]: acc=0.5000 g_loss=0.5004 d_loss=0.7166 kd_loss=0.0080\n",
      "Epoch [17/20] Step [100/127]: acc=0.5000 g_loss=0.5003 d_loss=0.7166 kd_loss=0.0054\n",
      "Epoch [17/20] Step [105/127]: acc=0.5000 g_loss=0.5000 d_loss=0.7166 kd_loss=0.0074\n",
      "Epoch [17/20] Step [110/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7167 kd_loss=0.0049\n",
      "Epoch [17/20] Step [115/127]: acc=0.5000 g_loss=0.4992 d_loss=0.7169 kd_loss=0.0065\n",
      "Epoch [17/20] Step [120/127]: acc=0.5000 g_loss=0.4990 d_loss=0.7169 kd_loss=0.0039\n",
      "Epoch [17/20] Step [125/127]: acc=0.5000 g_loss=0.4988 d_loss=0.7170 kd_loss=0.0057\n",
      "German Test:\n",
      "Validation loss:  1.2306538820266724\n",
      "Accuracy: 0.6796116504854369\n",
      "F1 score (Macro): 0.5437217256143559\n",
      "F1 score (Per class): [0.36842105 0.46416382 0.7985803 ]\n",
      "Precision score (Per class): [0.35       0.5483871  0.77586207]\n",
      "Recall score (Per class): [0.38888889 0.40236686 0.8226691 ]\n",
      "Epoch: 18/20\n",
      "Epoch [18/20] Step [000/127]: acc=0.5000 g_loss=0.4991 d_loss=0.7169 kd_loss=0.0056\n",
      "Epoch [18/20] Step [005/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7168 kd_loss=0.0062\n",
      "Epoch [18/20] Step [010/127]: acc=0.5000 g_loss=0.4998 d_loss=0.7168 kd_loss=0.0099\n",
      "Epoch [18/20] Step [015/127]: acc=0.5000 g_loss=0.4998 d_loss=0.7167 kd_loss=0.0064\n",
      "Epoch [18/20] Step [020/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7167 kd_loss=0.0099\n",
      "Epoch [18/20] Step [025/127]: acc=0.5000 g_loss=0.4994 d_loss=0.7168 kd_loss=0.0120\n",
      "Epoch [18/20] Step [030/127]: acc=0.5000 g_loss=0.4993 d_loss=0.7168 kd_loss=0.0189\n",
      "Epoch [18/20] Step [035/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7168 kd_loss=0.0059\n",
      "Epoch [18/20] Step [040/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7168 kd_loss=0.0067\n",
      "Epoch [18/20] Step [045/127]: acc=0.5000 g_loss=0.4999 d_loss=0.7167 kd_loss=0.0152\n",
      "Epoch [18/20] Step [050/127]: acc=0.5000 g_loss=0.5004 d_loss=0.7166 kd_loss=0.0044\n",
      "Epoch [18/20] Step [055/127]: acc=0.5000 g_loss=0.5009 d_loss=0.7164 kd_loss=0.0044\n",
      "Epoch [18/20] Step [060/127]: acc=0.5000 g_loss=0.5016 d_loss=0.7162 kd_loss=0.0047\n",
      "Epoch [18/20] Step [065/127]: acc=0.5000 g_loss=0.5018 d_loss=0.7162 kd_loss=0.0071\n",
      "Epoch [18/20] Step [070/127]: acc=0.5000 g_loss=0.5016 d_loss=0.7163 kd_loss=0.0107\n",
      "Epoch [18/20] Step [075/127]: acc=0.5000 g_loss=0.5012 d_loss=0.7164 kd_loss=0.0063\n",
      "Epoch [18/20] Step [080/127]: acc=0.5000 g_loss=0.5007 d_loss=0.7164 kd_loss=0.0059\n",
      "Epoch [18/20] Step [085/127]: acc=0.5000 g_loss=0.5001 d_loss=0.7166 kd_loss=0.0070\n",
      "Epoch [18/20] Step [090/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7168 kd_loss=0.0066\n",
      "Epoch [18/20] Step [095/127]: acc=0.5000 g_loss=0.4990 d_loss=0.7169 kd_loss=0.0061\n",
      "Epoch [18/20] Step [100/127]: acc=0.5000 g_loss=0.4985 d_loss=0.7170 kd_loss=0.0095\n",
      "Epoch [18/20] Step [105/127]: acc=0.5000 g_loss=0.4982 d_loss=0.7171 kd_loss=0.0031\n",
      "Epoch [18/20] Step [110/127]: acc=0.5000 g_loss=0.4982 d_loss=0.7171 kd_loss=0.0031\n",
      "Epoch [18/20] Step [115/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7171 kd_loss=0.0032\n",
      "Epoch [18/20] Step [120/127]: acc=0.5000 g_loss=0.4984 d_loss=0.7171 kd_loss=0.0056\n",
      "Epoch [18/20] Step [125/127]: acc=0.5000 g_loss=0.4985 d_loss=0.7171 kd_loss=0.0031\n",
      "German Test:\n",
      "Validation loss:  1.2272369861602783\n",
      "Accuracy: 0.6686893203883495\n",
      "F1 score (Macro): 0.5418408909457503\n",
      "F1 score (Per class): [0.37815126 0.45751634 0.78985507]\n",
      "Precision score (Per class): [0.34615385 0.51094891 0.78276481]\n",
      "Recall score (Per class): [0.41666667 0.41420118 0.79707495]\n",
      "Epoch: 19/20\n",
      "Epoch [19/20] Step [000/127]: acc=0.5000 g_loss=0.4990 d_loss=0.7170 kd_loss=0.0026\n",
      "Epoch [19/20] Step [005/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7168 kd_loss=0.0033\n",
      "Epoch [19/20] Step [010/127]: acc=0.5000 g_loss=0.5001 d_loss=0.7167 kd_loss=0.0064\n",
      "Epoch [19/20] Step [015/127]: acc=0.5000 g_loss=0.5003 d_loss=0.7166 kd_loss=0.0039\n",
      "Epoch [19/20] Step [020/127]: acc=0.5000 g_loss=0.5003 d_loss=0.7166 kd_loss=0.0028\n",
      "Epoch [19/20] Step [025/127]: acc=0.5000 g_loss=0.5000 d_loss=0.7167 kd_loss=0.0094\n",
      "Epoch [19/20] Step [030/127]: acc=0.5000 g_loss=0.4998 d_loss=0.7167 kd_loss=0.0096\n",
      "Epoch [19/20] Step [035/127]: acc=0.5000 g_loss=0.4997 d_loss=0.7168 kd_loss=0.0061\n",
      "Epoch [19/20] Step [040/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7168 kd_loss=0.0070\n",
      "Epoch [19/20] Step [045/127]: acc=0.5000 g_loss=0.4994 d_loss=0.7169 kd_loss=0.0045\n",
      "Epoch [19/20] Step [050/127]: acc=0.5000 g_loss=0.4992 d_loss=0.7170 kd_loss=0.0046\n",
      "Epoch [19/20] Step [055/127]: acc=0.5000 g_loss=0.4991 d_loss=0.7169 kd_loss=0.0044\n",
      "Epoch [19/20] Step [060/127]: acc=0.5000 g_loss=0.4992 d_loss=0.7169 kd_loss=0.0059\n",
      "Epoch [19/20] Step [065/127]: acc=0.5000 g_loss=0.4992 d_loss=0.7169 kd_loss=0.0069\n",
      "Epoch [19/20] Step [070/127]: acc=0.5000 g_loss=0.4991 d_loss=0.7171 kd_loss=0.0217\n",
      "Epoch [19/20] Step [075/127]: acc=0.5000 g_loss=0.4989 d_loss=0.7170 kd_loss=0.0053\n",
      "Epoch [19/20] Step [080/127]: acc=0.5000 g_loss=0.4988 d_loss=0.7170 kd_loss=0.0033\n",
      "Epoch [19/20] Step [085/127]: acc=0.5000 g_loss=0.4987 d_loss=0.7171 kd_loss=0.0051\n",
      "Epoch [19/20] Step [090/127]: acc=0.5000 g_loss=0.4990 d_loss=0.7170 kd_loss=0.0049\n",
      "Epoch [19/20] Step [095/127]: acc=0.5000 g_loss=0.4994 d_loss=0.7169 kd_loss=0.0065\n",
      "Epoch [19/20] Step [100/127]: acc=0.5000 g_loss=0.4999 d_loss=0.7168 kd_loss=0.0140\n",
      "Epoch [19/20] Step [105/127]: acc=0.5000 g_loss=0.5004 d_loss=0.7166 kd_loss=0.0035\n",
      "Epoch [19/20] Step [110/127]: acc=0.5000 g_loss=0.5006 d_loss=0.7165 kd_loss=0.0022\n",
      "Epoch [19/20] Step [115/127]: acc=0.5000 g_loss=0.5007 d_loss=0.7165 kd_loss=0.0025\n",
      "Epoch [19/20] Step [120/127]: acc=0.5000 g_loss=0.5005 d_loss=0.7166 kd_loss=0.0024\n",
      "Epoch [19/20] Step [125/127]: acc=0.5000 g_loss=0.5003 d_loss=0.7166 kd_loss=0.0022\n",
      "German Test:\n",
      "Validation loss:  1.210853099822998\n",
      "Accuracy: 0.6686893203883495\n",
      "F1 score (Macro): 0.5483406862816097\n",
      "F1 score (Per class): [0.38842975 0.46687697 0.78971534]\n",
      "Precision score (Per class): [0.35074627 0.5        0.79335793]\n",
      "Recall score (Per class): [0.43518519 0.43786982 0.78610603]\n"
     ]
    }
   ],
   "source": [
    "# bilstm german\n",
    "tgt_encoder = adapt(src_encoder, discriminator,\n",
    "                    bi_lstm_classifier, train_dataloader, train_translated_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ac08edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10\n",
      "Epoch [00/10] Step [000/127]: acc=0.5000 g_loss=0.5002 d_loss=0.7170 kd_loss=0.0350\n",
      "Epoch [00/10] Step [005/127]: acc=0.5000 g_loss=0.4990 d_loss=0.7181 kd_loss=0.0651\n",
      "Epoch [00/10] Step [010/127]: acc=0.5000 g_loss=0.5015 d_loss=0.7163 kd_loss=0.0548\n",
      "Epoch [00/10] Step [015/127]: acc=0.5000 g_loss=0.5041 d_loss=0.7150 kd_loss=0.0203\n",
      "Epoch [00/10] Step [020/127]: acc=0.5000 g_loss=0.5032 d_loss=0.7132 kd_loss=0.0276\n",
      "Epoch [00/10] Step [025/127]: acc=0.5000 g_loss=0.5040 d_loss=0.7117 kd_loss=0.0560\n",
      "Epoch [00/10] Step [030/127]: acc=0.5000 g_loss=0.5046 d_loss=0.7135 kd_loss=0.0270\n",
      "Epoch [00/10] Step [035/127]: acc=0.5000 g_loss=0.5022 d_loss=0.7147 kd_loss=0.0333\n",
      "Epoch [00/10] Step [040/127]: acc=0.5000 g_loss=0.5003 d_loss=0.7155 kd_loss=0.0091\n",
      "Epoch [00/10] Step [045/127]: acc=0.5000 g_loss=0.4879 d_loss=0.7244 kd_loss=0.0291\n",
      "Epoch [00/10] Step [050/127]: acc=0.5000 g_loss=0.4892 d_loss=0.7219 kd_loss=0.0198\n",
      "Epoch [00/10] Step [055/127]: acc=0.5000 g_loss=0.4887 d_loss=0.7235 kd_loss=0.0466\n",
      "Epoch [00/10] Step [060/127]: acc=0.5000 g_loss=0.4936 d_loss=0.7186 kd_loss=0.0411\n",
      "Epoch [00/10] Step [065/127]: acc=0.5000 g_loss=0.4955 d_loss=0.7171 kd_loss=0.0277\n",
      "Epoch [00/10] Step [070/127]: acc=0.5000 g_loss=0.4977 d_loss=0.7163 kd_loss=0.0680\n",
      "Epoch [00/10] Step [075/127]: acc=0.5000 g_loss=0.5006 d_loss=0.7154 kd_loss=0.0314\n",
      "Epoch [00/10] Step [080/127]: acc=0.5000 g_loss=0.5030 d_loss=0.7156 kd_loss=0.0134\n",
      "Epoch [00/10] Step [085/127]: acc=0.5000 g_loss=0.5060 d_loss=0.7140 kd_loss=0.0105\n",
      "Epoch [00/10] Step [090/127]: acc=0.5000 g_loss=0.5108 d_loss=0.7116 kd_loss=0.0154\n",
      "Epoch [00/10] Step [095/127]: acc=0.5000 g_loss=0.5168 d_loss=0.7111 kd_loss=0.0147\n",
      "Epoch [00/10] Step [100/127]: acc=0.5000 g_loss=0.5137 d_loss=0.7099 kd_loss=0.0598\n",
      "Epoch [00/10] Step [105/127]: acc=0.5000 g_loss=0.5095 d_loss=0.7130 kd_loss=0.0321\n",
      "Epoch [00/10] Step [110/127]: acc=0.5000 g_loss=0.5168 d_loss=0.7074 kd_loss=0.0104\n",
      "Epoch [00/10] Step [115/127]: acc=0.5000 g_loss=0.5089 d_loss=0.7186 kd_loss=0.0164\n",
      "Epoch [00/10] Step [120/127]: acc=0.5000 g_loss=0.5086 d_loss=0.7190 kd_loss=0.0114\n",
      "Epoch [00/10] Step [125/127]: acc=0.5000 g_loss=0.5007 d_loss=0.7192 kd_loss=0.0250\n",
      "French Test: \n",
      "\n",
      "Validation loss:  2.393829107284546\n",
      "Accuracy: 0.4597839135654262\n",
      "F1 score (Macro): 0.4499176873717878\n",
      "F1 score (Per class): [0.36032389 0.45454545 0.53488372]\n",
      "Precision score (Per class): [0.24791086 0.53658537 0.68401487]\n",
      "Recall score (Per class): [0.65925926 0.39426523 0.43914081]\n",
      "Epoch: 1/10\n",
      "Epoch [01/10] Step [000/127]: acc=0.5000 g_loss=0.4891 d_loss=0.7290 kd_loss=0.0441\n",
      "Epoch [01/10] Step [005/127]: acc=0.5000 g_loss=0.4852 d_loss=0.7302 kd_loss=0.0196\n",
      "Epoch [01/10] Step [010/127]: acc=0.5000 g_loss=0.4849 d_loss=0.7311 kd_loss=0.0222\n",
      "Epoch [01/10] Step [015/127]: acc=0.5000 g_loss=0.4874 d_loss=0.7219 kd_loss=0.0164\n",
      "Epoch [01/10] Step [020/127]: acc=0.5000 g_loss=0.4907 d_loss=0.7204 kd_loss=0.0082\n",
      "Epoch [01/10] Step [025/127]: acc=0.5000 g_loss=0.4988 d_loss=0.7122 kd_loss=0.0114\n",
      "Epoch [01/10] Step [030/127]: acc=0.5000 g_loss=0.5091 d_loss=0.7082 kd_loss=0.0168\n",
      "Epoch [01/10] Step [035/127]: acc=0.5000 g_loss=0.5200 d_loss=0.7023 kd_loss=0.0158\n",
      "Epoch [01/10] Step [040/127]: acc=0.5000 g_loss=0.5076 d_loss=0.7125 kd_loss=0.0186\n",
      "Epoch [01/10] Step [045/127]: acc=0.5000 g_loss=0.4838 d_loss=0.7286 kd_loss=0.0059\n",
      "Epoch [01/10] Step [050/127]: acc=0.5000 g_loss=0.4855 d_loss=0.7279 kd_loss=0.0084\n",
      "Epoch [01/10] Step [055/127]: acc=0.5000 g_loss=0.4768 d_loss=0.7304 kd_loss=0.0134\n",
      "Epoch [01/10] Step [060/127]: acc=0.5000 g_loss=0.4792 d_loss=0.7266 kd_loss=0.0106\n",
      "Epoch [01/10] Step [065/127]: acc=0.5000 g_loss=0.4787 d_loss=0.7290 kd_loss=0.0294\n",
      "Epoch [01/10] Step [070/127]: acc=0.5000 g_loss=0.4817 d_loss=0.7231 kd_loss=0.0240\n",
      "Epoch [01/10] Step [075/127]: acc=0.5000 g_loss=0.4797 d_loss=0.7230 kd_loss=0.0129\n",
      "Epoch [01/10] Step [080/127]: acc=0.5000 g_loss=0.4845 d_loss=0.7172 kd_loss=0.0038\n",
      "Epoch [01/10] Step [085/127]: acc=0.5000 g_loss=0.4831 d_loss=0.7221 kd_loss=0.0185\n",
      "Epoch [01/10] Step [090/127]: acc=0.5000 g_loss=0.4843 d_loss=0.7234 kd_loss=0.0090\n",
      "Epoch [01/10] Step [095/127]: acc=0.5000 g_loss=0.4872 d_loss=0.7202 kd_loss=0.0101\n",
      "Epoch [01/10] Step [100/127]: acc=0.5000 g_loss=0.4907 d_loss=0.7223 kd_loss=0.0173\n",
      "Epoch [01/10] Step [105/127]: acc=0.5000 g_loss=0.4965 d_loss=0.7186 kd_loss=0.0086\n",
      "Epoch [01/10] Step [110/127]: acc=0.5000 g_loss=0.4993 d_loss=0.7187 kd_loss=0.0081\n",
      "Epoch [01/10] Step [115/127]: acc=0.5000 g_loss=0.5027 d_loss=0.7173 kd_loss=0.0064\n",
      "Epoch [01/10] Step [120/127]: acc=0.5000 g_loss=0.5032 d_loss=0.7189 kd_loss=0.0097\n",
      "Epoch [01/10] Step [125/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7220 kd_loss=0.0034\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.9126802682876587\n",
      "Accuracy: 0.5162064825930373\n",
      "F1 score (Macro): 0.482434271653115\n",
      "F1 score (Per class): [0.34715026 0.47657841 0.62357414]\n",
      "Precision score (Per class): [0.26693227 0.55188679 0.66486486]\n",
      "Recall score (Per class): [0.4962963  0.41935484 0.58711217]\n",
      "Epoch: 2/10\n",
      "Epoch [02/10] Step [000/127]: acc=0.5000 g_loss=0.4992 d_loss=0.7213 kd_loss=0.0127\n",
      "Epoch [02/10] Step [005/127]: acc=0.5000 g_loss=0.5001 d_loss=0.7199 kd_loss=0.0209\n",
      "Epoch [02/10] Step [010/127]: acc=0.5000 g_loss=0.4988 d_loss=0.7209 kd_loss=0.0234\n",
      "Epoch [02/10] Step [015/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7179 kd_loss=0.0037\n",
      "Epoch [02/10] Step [020/127]: acc=0.5000 g_loss=0.4982 d_loss=0.7177 kd_loss=0.0094\n",
      "Epoch [02/10] Step [025/127]: acc=0.5000 g_loss=0.4971 d_loss=0.7159 kd_loss=0.0174\n",
      "Epoch [02/10] Step [030/127]: acc=0.5000 g_loss=0.4950 d_loss=0.7160 kd_loss=0.0191\n",
      "Epoch [02/10] Step [035/127]: acc=0.5000 g_loss=0.4918 d_loss=0.7160 kd_loss=0.0074\n",
      "Epoch [02/10] Step [040/127]: acc=0.5000 g_loss=0.4893 d_loss=0.7138 kd_loss=0.0054\n",
      "Epoch [02/10] Step [045/127]: acc=0.5000 g_loss=0.4894 d_loss=0.7117 kd_loss=0.0043\n",
      "Epoch [02/10] Step [050/127]: acc=0.5000 g_loss=0.4827 d_loss=0.7163 kd_loss=0.0044\n",
      "Epoch [02/10] Step [055/127]: acc=0.5000 g_loss=0.4745 d_loss=0.7232 kd_loss=0.0156\n",
      "Epoch [02/10] Step [060/127]: acc=0.5000 g_loss=0.4814 d_loss=0.7190 kd_loss=0.0162\n",
      "Epoch [02/10] Step [065/127]: acc=0.5000 g_loss=0.4787 d_loss=0.7236 kd_loss=0.0116\n",
      "Epoch [02/10] Step [070/127]: acc=0.5000 g_loss=0.4855 d_loss=0.7191 kd_loss=0.0616\n",
      "Epoch [02/10] Step [075/127]: acc=0.5000 g_loss=0.4869 d_loss=0.7188 kd_loss=0.0166\n",
      "Epoch [02/10] Step [080/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7083 kd_loss=0.0056\n",
      "Epoch [02/10] Step [085/127]: acc=0.5000 g_loss=0.4945 d_loss=0.7160 kd_loss=0.0070\n",
      "Epoch [02/10] Step [090/127]: acc=0.5000 g_loss=0.5009 d_loss=0.7157 kd_loss=0.0172\n",
      "Epoch [02/10] Step [095/127]: acc=0.5000 g_loss=0.5008 d_loss=0.7149 kd_loss=0.0066\n",
      "Epoch [02/10] Step [100/127]: acc=0.5000 g_loss=0.5081 d_loss=0.7167 kd_loss=0.0197\n",
      "Epoch [02/10] Step [105/127]: acc=0.5000 g_loss=0.5184 d_loss=0.7119 kd_loss=0.0106\n",
      "Epoch [02/10] Step [110/127]: acc=0.5000 g_loss=0.5139 d_loss=0.7188 kd_loss=0.0097\n",
      "Epoch [02/10] Step [115/127]: acc=0.5000 g_loss=0.5210 d_loss=0.7104 kd_loss=0.0103\n",
      "Epoch [02/10] Step [120/127]: acc=0.5000 g_loss=0.5147 d_loss=0.7162 kd_loss=0.0083\n",
      "Epoch [02/10] Step [125/127]: acc=0.5000 g_loss=0.5064 d_loss=0.7264 kd_loss=0.0069\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.79427170753479\n",
      "Accuracy: 0.5522208883553421\n",
      "F1 score (Macro): 0.4886533897712286\n",
      "F1 score (Per class): [0.34210526 0.45132743 0.67252747]\n",
      "Precision score (Per class): [0.30769231 0.58959538 0.62321792]\n",
      "Recall score (Per class): [0.38518519 0.3655914  0.73031026]\n",
      "Epoch: 3/10\n",
      "Epoch [03/10] Step [000/127]: acc=0.5000 g_loss=0.5059 d_loss=0.7234 kd_loss=0.0083\n",
      "Epoch [03/10] Step [005/127]: acc=0.5000 g_loss=0.5062 d_loss=0.7215 kd_loss=0.0159\n",
      "Epoch [03/10] Step [010/127]: acc=0.5000 g_loss=0.5018 d_loss=0.7215 kd_loss=0.0295\n",
      "Epoch [03/10] Step [015/127]: acc=0.5000 g_loss=0.5018 d_loss=0.7234 kd_loss=0.0062\n",
      "Epoch [03/10] Step [020/127]: acc=0.5000 g_loss=0.5031 d_loss=0.7209 kd_loss=0.0187\n",
      "Epoch [03/10] Step [025/127]: acc=0.5000 g_loss=0.5021 d_loss=0.7190 kd_loss=0.0142\n",
      "Epoch [03/10] Step [030/127]: acc=0.5000 g_loss=0.5017 d_loss=0.7166 kd_loss=0.0323\n",
      "Epoch [03/10] Step [035/127]: acc=0.5000 g_loss=0.5006 d_loss=0.7146 kd_loss=0.0091\n",
      "Epoch [03/10] Step [040/127]: acc=0.5000 g_loss=0.4993 d_loss=0.7135 kd_loss=0.0056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [03/10] Step [045/127]: acc=0.5000 g_loss=0.4889 d_loss=0.7181 kd_loss=0.0053\n",
      "Epoch [03/10] Step [050/127]: acc=0.5000 g_loss=0.4888 d_loss=0.7164 kd_loss=0.0044\n",
      "Epoch [03/10] Step [055/127]: acc=0.5000 g_loss=0.4836 d_loss=0.7209 kd_loss=0.0285\n",
      "Epoch [03/10] Step [060/127]: acc=0.5000 g_loss=0.4864 d_loss=0.7177 kd_loss=0.0113\n",
      "Epoch [03/10] Step [065/127]: acc=0.5000 g_loss=0.4856 d_loss=0.7193 kd_loss=0.0078\n",
      "Epoch [03/10] Step [070/127]: acc=0.5000 g_loss=0.4878 d_loss=0.7170 kd_loss=0.0385\n",
      "Epoch [03/10] Step [075/127]: acc=0.5000 g_loss=0.4847 d_loss=0.7193 kd_loss=0.0089\n",
      "Epoch [03/10] Step [080/127]: acc=0.5000 g_loss=0.4885 d_loss=0.7163 kd_loss=0.0056\n",
      "Epoch [03/10] Step [085/127]: acc=0.5000 g_loss=0.4863 d_loss=0.7183 kd_loss=0.0107\n",
      "Epoch [03/10] Step [090/127]: acc=0.5000 g_loss=0.4883 d_loss=0.7180 kd_loss=0.0068\n",
      "Epoch [03/10] Step [095/127]: acc=0.5000 g_loss=0.4898 d_loss=0.7191 kd_loss=0.0063\n",
      "Epoch [03/10] Step [100/127]: acc=0.5000 g_loss=0.4905 d_loss=0.7174 kd_loss=0.0088\n",
      "Epoch [03/10] Step [105/127]: acc=0.5000 g_loss=0.4945 d_loss=0.7168 kd_loss=0.0226\n",
      "Epoch [03/10] Step [110/127]: acc=0.5000 g_loss=0.4993 d_loss=0.7140 kd_loss=0.0068\n",
      "Epoch [03/10] Step [115/127]: acc=0.5000 g_loss=0.5004 d_loss=0.7158 kd_loss=0.0048\n",
      "Epoch [03/10] Step [120/127]: acc=0.5000 g_loss=0.5013 d_loss=0.7176 kd_loss=0.0061\n",
      "Epoch [03/10] Step [125/127]: acc=0.5000 g_loss=0.5018 d_loss=0.7168 kd_loss=0.0072\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.8114490509033203\n",
      "Accuracy: 0.5486194477791116\n",
      "F1 score (Macro): 0.49756783112624264\n",
      "F1 score (Per class): [0.35928144 0.46982759 0.66359447]\n",
      "Precision score (Per class): [0.30150754 0.58918919 0.64142539]\n",
      "Recall score (Per class): [0.44444444 0.390681   0.68735084]\n",
      "Epoch: 4/10\n",
      "Epoch [04/10] Step [000/127]: acc=0.5000 g_loss=0.5036 d_loss=0.7177 kd_loss=0.0070\n",
      "Epoch [04/10] Step [005/127]: acc=0.5000 g_loss=0.5075 d_loss=0.7170 kd_loss=0.0100\n",
      "Epoch [04/10] Step [010/127]: acc=0.5000 g_loss=0.5126 d_loss=0.7154 kd_loss=0.0278\n",
      "Epoch [04/10] Step [015/127]: acc=0.5000 g_loss=0.5170 d_loss=0.7139 kd_loss=0.0066\n",
      "Epoch [04/10] Step [020/127]: acc=0.5000 g_loss=0.5150 d_loss=0.7110 kd_loss=0.0114\n",
      "Epoch [04/10] Step [025/127]: acc=0.5000 g_loss=0.5057 d_loss=0.7141 kd_loss=0.0187\n",
      "Epoch [04/10] Step [030/127]: acc=0.5000 g_loss=0.5019 d_loss=0.7176 kd_loss=0.0607\n",
      "Epoch [04/10] Step [035/127]: acc=0.5000 g_loss=0.4979 d_loss=0.7176 kd_loss=0.0133\n",
      "Epoch [04/10] Step [040/127]: acc=0.5000 g_loss=0.4947 d_loss=0.7183 kd_loss=0.0045\n",
      "Epoch [04/10] Step [045/127]: acc=0.5000 g_loss=0.4781 d_loss=0.7267 kd_loss=0.0157\n",
      "Epoch [04/10] Step [050/127]: acc=0.5000 g_loss=0.4812 d_loss=0.7215 kd_loss=0.0072\n",
      "Epoch [04/10] Step [055/127]: acc=0.5000 g_loss=0.4745 d_loss=0.7268 kd_loss=0.0177\n",
      "Epoch [04/10] Step [060/127]: acc=0.5000 g_loss=0.4796 d_loss=0.7214 kd_loss=0.0085\n",
      "Epoch [04/10] Step [065/127]: acc=0.5000 g_loss=0.4807 d_loss=0.7189 kd_loss=0.0236\n",
      "Epoch [04/10] Step [070/127]: acc=0.5000 g_loss=0.4843 d_loss=0.7173 kd_loss=0.0135\n",
      "Epoch [04/10] Step [075/127]: acc=0.5000 g_loss=0.4857 d_loss=0.7168 kd_loss=0.0118\n",
      "Epoch [04/10] Step [080/127]: acc=0.5000 g_loss=0.4899 d_loss=0.7142 kd_loss=0.0045\n",
      "Epoch [04/10] Step [085/127]: acc=0.5000 g_loss=0.4888 d_loss=0.7167 kd_loss=0.0153\n",
      "Epoch [04/10] Step [090/127]: acc=0.5000 g_loss=0.4923 d_loss=0.7160 kd_loss=0.0086\n",
      "Epoch [04/10] Step [095/127]: acc=0.5000 g_loss=0.4960 d_loss=0.7172 kd_loss=0.0098\n",
      "Epoch [04/10] Step [100/127]: acc=0.5000 g_loss=0.4983 d_loss=0.7143 kd_loss=0.0106\n",
      "Epoch [04/10] Step [105/127]: acc=0.5000 g_loss=0.5042 d_loss=0.7112 kd_loss=0.0200\n",
      "Epoch [04/10] Step [110/127]: acc=0.5000 g_loss=0.5086 d_loss=0.7076 kd_loss=0.0098\n",
      "Epoch [04/10] Step [115/127]: acc=0.5000 g_loss=0.5071 d_loss=0.7137 kd_loss=0.0075\n",
      "Epoch [04/10] Step [120/127]: acc=0.5000 g_loss=0.5097 d_loss=0.7143 kd_loss=0.0045\n",
      "Epoch [04/10] Step [125/127]: acc=0.5000 g_loss=0.5081 d_loss=0.7123 kd_loss=0.0090\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.8531358242034912\n",
      "Accuracy: 0.5498199279711885\n",
      "F1 score (Macro): 0.504346484914097\n",
      "F1 score (Per class): [0.34965035 0.52249135 0.64089776]\n",
      "Precision score (Per class): [0.33112583 0.50501672 0.67101828]\n",
      "Recall score (Per class): [0.37037037 0.54121864 0.61336516]\n",
      "Epoch: 5/10\n",
      "Epoch [05/10] Step [000/127]: acc=0.5000 g_loss=0.5060 d_loss=0.7174 kd_loss=0.0063\n",
      "Epoch [05/10] Step [005/127]: acc=0.5000 g_loss=0.5010 d_loss=0.7202 kd_loss=0.0105\n",
      "Epoch [05/10] Step [010/127]: acc=0.5000 g_loss=0.4963 d_loss=0.7269 kd_loss=0.0400\n",
      "Epoch [05/10] Step [015/127]: acc=0.5000 g_loss=0.4968 d_loss=0.7164 kd_loss=0.0078\n",
      "Epoch [05/10] Step [020/127]: acc=0.5000 g_loss=0.4908 d_loss=0.7252 kd_loss=0.0106\n",
      "Epoch [05/10] Step [025/127]: acc=0.5000 g_loss=0.4936 d_loss=0.7219 kd_loss=0.0156\n",
      "Epoch [05/10] Step [030/127]: acc=0.5000 g_loss=0.4999 d_loss=0.7164 kd_loss=0.0204\n",
      "Epoch [05/10] Step [035/127]: acc=0.5000 g_loss=0.5143 d_loss=0.7077 kd_loss=0.0074\n",
      "Epoch [05/10] Step [040/127]: acc=0.5000 g_loss=0.5267 d_loss=0.7035 kd_loss=0.0062\n",
      "Epoch [05/10] Step [045/127]: acc=0.5000 g_loss=0.5053 d_loss=0.7206 kd_loss=0.0145\n",
      "Epoch [05/10] Step [050/127]: acc=0.5000 g_loss=0.5155 d_loss=0.7189 kd_loss=0.0109\n",
      "Epoch [05/10] Step [055/127]: acc=0.5000 g_loss=0.4992 d_loss=0.7296 kd_loss=0.0165\n",
      "Epoch [05/10] Step [060/127]: acc=0.5000 g_loss=0.5020 d_loss=0.7273 kd_loss=0.0188\n",
      "Epoch [05/10] Step [065/127]: acc=0.5000 g_loss=0.5083 d_loss=0.7253 kd_loss=0.0186\n",
      "Epoch [05/10] Step [070/127]: acc=0.5000 g_loss=0.5114 d_loss=0.7218 kd_loss=0.0370\n",
      "Epoch [05/10] Step [075/127]: acc=0.5000 g_loss=0.5143 d_loss=0.7191 kd_loss=0.0049\n",
      "Epoch [05/10] Step [080/127]: acc=0.5000 g_loss=0.5143 d_loss=0.7141 kd_loss=0.0078\n",
      "Epoch [05/10] Step [085/127]: acc=0.5000 g_loss=0.5131 d_loss=0.7177 kd_loss=0.0083\n",
      "Epoch [05/10] Step [090/127]: acc=0.5000 g_loss=0.5062 d_loss=0.7225 kd_loss=0.0124\n",
      "Epoch [05/10] Step [095/127]: acc=0.5000 g_loss=0.5090 d_loss=0.7177 kd_loss=0.0090\n",
      "Epoch [05/10] Step [100/127]: acc=0.5000 g_loss=0.5092 d_loss=0.7190 kd_loss=0.0241\n",
      "Epoch [05/10] Step [105/127]: acc=0.5000 g_loss=0.5057 d_loss=0.7202 kd_loss=0.0470\n",
      "Epoch [05/10] Step [110/127]: acc=0.5000 g_loss=0.5087 d_loss=0.7176 kd_loss=0.0052\n",
      "Epoch [05/10] Step [115/127]: acc=0.5000 g_loss=0.5115 d_loss=0.7168 kd_loss=0.0128\n",
      "Epoch [05/10] Step [120/127]: acc=0.5000 g_loss=0.5121 d_loss=0.7187 kd_loss=0.0059\n",
      "Epoch [05/10] Step [125/127]: acc=0.5000 g_loss=0.5020 d_loss=0.7241 kd_loss=0.0145\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.633959174156189\n",
      "Accuracy: 0.5546218487394958\n",
      "F1 score (Macro): 0.493424455482054\n",
      "F1 score (Per class): [0.30588235 0.52877698 0.64561404]\n",
      "Precision score (Per class): [0.325      0.53068592 0.63302752]\n",
      "Recall score (Per class): [0.28888889 0.52688172 0.65871122]\n",
      "Epoch: 6/10\n",
      "Epoch [06/10] Step [000/127]: acc=0.5000 g_loss=0.4977 d_loss=0.7249 kd_loss=0.0077\n",
      "Epoch [06/10] Step [005/127]: acc=0.5000 g_loss=0.4968 d_loss=0.7247 kd_loss=0.0214\n",
      "Epoch [06/10] Step [010/127]: acc=0.5000 g_loss=0.4963 d_loss=0.7220 kd_loss=0.0077\n",
      "Epoch [06/10] Step [015/127]: acc=0.5000 g_loss=0.4951 d_loss=0.7196 kd_loss=0.0089\n",
      "Epoch [06/10] Step [020/127]: acc=0.5000 g_loss=0.4948 d_loss=0.7179 kd_loss=0.0066\n",
      "Epoch [06/10] Step [025/127]: acc=0.5000 g_loss=0.4939 d_loss=0.7162 kd_loss=0.0125\n",
      "Epoch [06/10] Step [030/127]: acc=0.5000 g_loss=0.4926 d_loss=0.7148 kd_loss=0.0248\n",
      "Epoch [06/10] Step [035/127]: acc=0.5000 g_loss=0.4919 d_loss=0.7140 kd_loss=0.0151\n",
      "Epoch [06/10] Step [040/127]: acc=0.5000 g_loss=0.4905 d_loss=0.7148 kd_loss=0.0071\n",
      "Epoch [06/10] Step [045/127]: acc=0.5000 g_loss=0.4848 d_loss=0.7174 kd_loss=0.0069\n",
      "Epoch [06/10] Step [050/127]: acc=0.5000 g_loss=0.4850 d_loss=0.7163 kd_loss=0.0137\n",
      "Epoch [06/10] Step [055/127]: acc=0.5000 g_loss=0.4830 d_loss=0.7201 kd_loss=0.0152\n",
      "Epoch [06/10] Step [060/127]: acc=0.5000 g_loss=0.4899 d_loss=0.7171 kd_loss=0.0161\n",
      "Epoch [06/10] Step [065/127]: acc=0.5000 g_loss=0.4959 d_loss=0.7179 kd_loss=0.0104\n",
      "Epoch [06/10] Step [070/127]: acc=0.5000 g_loss=0.5050 d_loss=0.7129 kd_loss=0.0105\n",
      "Epoch [06/10] Step [075/127]: acc=0.5000 g_loss=0.5011 d_loss=0.7183 kd_loss=0.0095\n",
      "Epoch [06/10] Step [080/127]: acc=0.5000 g_loss=0.5123 d_loss=0.7133 kd_loss=0.0039\n",
      "Epoch [06/10] Step [085/127]: acc=0.5000 g_loss=0.5079 d_loss=0.7153 kd_loss=0.0079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [06/10] Step [090/127]: acc=0.5000 g_loss=0.5018 d_loss=0.7205 kd_loss=0.0275\n",
      "Epoch [06/10] Step [095/127]: acc=0.5000 g_loss=0.4991 d_loss=0.7201 kd_loss=0.0110\n",
      "Epoch [06/10] Step [100/127]: acc=0.5000 g_loss=0.4953 d_loss=0.7194 kd_loss=0.0215\n",
      "Epoch [06/10] Step [105/127]: acc=0.5000 g_loss=0.4953 d_loss=0.7190 kd_loss=0.0206\n",
      "Epoch [06/10] Step [110/127]: acc=0.5000 g_loss=0.4947 d_loss=0.7191 kd_loss=0.0032\n",
      "Epoch [06/10] Step [115/127]: acc=0.5000 g_loss=0.4956 d_loss=0.7176 kd_loss=0.0039\n",
      "Epoch [06/10] Step [120/127]: acc=0.5000 g_loss=0.4974 d_loss=0.7193 kd_loss=0.0069\n",
      "Epoch [06/10] Step [125/127]: acc=0.5000 g_loss=0.5017 d_loss=0.7197 kd_loss=0.0097\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.763452172279358\n",
      "Accuracy: 0.5438175270108043\n",
      "F1 score (Macro): 0.49945463429548415\n",
      "F1 score (Per class): [0.34640523 0.51086957 0.64108911]\n",
      "Precision score (Per class): [0.30994152 0.51648352 0.66580977]\n",
      "Recall score (Per class): [0.39259259 0.50537634 0.61813842]\n",
      "Epoch: 7/10\n",
      "Epoch [07/10] Step [000/127]: acc=0.5000 g_loss=0.5077 d_loss=0.7170 kd_loss=0.0085\n",
      "Epoch [07/10] Step [005/127]: acc=0.5000 g_loss=0.5179 d_loss=0.7130 kd_loss=0.0080\n",
      "Epoch [07/10] Step [010/127]: acc=0.5000 g_loss=0.5236 d_loss=0.7093 kd_loss=0.0151\n",
      "Epoch [07/10] Step [015/127]: acc=0.5000 g_loss=0.5246 d_loss=0.7169 kd_loss=0.0059\n",
      "Epoch [07/10] Step [020/127]: acc=0.5000 g_loss=0.5225 d_loss=0.7106 kd_loss=0.0279\n",
      "Epoch [07/10] Step [025/127]: acc=0.5000 g_loss=0.5132 d_loss=0.7142 kd_loss=0.0110\n",
      "Epoch [07/10] Step [030/127]: acc=0.5000 g_loss=0.5115 d_loss=0.7187 kd_loss=0.0148\n",
      "Epoch [07/10] Step [035/127]: acc=0.5000 g_loss=0.5099 d_loss=0.7192 kd_loss=0.0179\n",
      "Epoch [07/10] Step [040/127]: acc=0.5000 g_loss=0.5082 d_loss=0.7208 kd_loss=0.0079\n",
      "Epoch [07/10] Step [045/127]: acc=0.5000 g_loss=0.4982 d_loss=0.7274 kd_loss=0.0090\n",
      "Epoch [07/10] Step [050/127]: acc=0.5000 g_loss=0.4993 d_loss=0.7263 kd_loss=0.0111\n",
      "Epoch [07/10] Step [055/127]: acc=0.5000 g_loss=0.4935 d_loss=0.7276 kd_loss=0.0103\n",
      "Epoch [07/10] Step [060/127]: acc=0.5000 g_loss=0.4971 d_loss=0.7236 kd_loss=0.0117\n",
      "Epoch [07/10] Step [065/127]: acc=0.5000 g_loss=0.5007 d_loss=0.7223 kd_loss=0.0148\n",
      "Epoch [07/10] Step [070/127]: acc=0.5000 g_loss=0.5050 d_loss=0.7163 kd_loss=0.0167\n",
      "Epoch [07/10] Step [075/127]: acc=0.5000 g_loss=0.5032 d_loss=0.7178 kd_loss=0.0090\n",
      "Epoch [07/10] Step [080/127]: acc=0.5000 g_loss=0.5084 d_loss=0.7127 kd_loss=0.0099\n",
      "Epoch [07/10] Step [085/127]: acc=0.5000 g_loss=0.5031 d_loss=0.7169 kd_loss=0.0054\n",
      "Epoch [07/10] Step [090/127]: acc=0.5000 g_loss=0.5064 d_loss=0.7149 kd_loss=0.0061\n",
      "Epoch [07/10] Step [095/127]: acc=0.5000 g_loss=0.5046 d_loss=0.7167 kd_loss=0.0102\n",
      "Epoch [07/10] Step [100/127]: acc=0.5000 g_loss=0.5017 d_loss=0.7159 kd_loss=0.0099\n",
      "Epoch [07/10] Step [105/127]: acc=0.5000 g_loss=0.5019 d_loss=0.7175 kd_loss=0.0167\n",
      "Epoch [07/10] Step [110/127]: acc=0.5000 g_loss=0.4995 d_loss=0.7183 kd_loss=0.0031\n",
      "Epoch [07/10] Step [115/127]: acc=0.5000 g_loss=0.4998 d_loss=0.7157 kd_loss=0.0047\n",
      "Epoch [07/10] Step [120/127]: acc=0.5000 g_loss=0.4966 d_loss=0.7192 kd_loss=0.0045\n",
      "Epoch [07/10] Step [125/127]: acc=0.5000 g_loss=0.4921 d_loss=0.7195 kd_loss=0.0087\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.7527246475219727\n",
      "Accuracy: 0.5726290516206483\n",
      "F1 score (Macro): 0.49997521797267647\n",
      "F1 score (Per class): [0.31896552 0.50478011 0.67618002]\n",
      "Precision score (Per class): [0.3814433  0.54098361 0.62601626]\n",
      "Recall score (Per class): [0.27407407 0.47311828 0.73508353]\n",
      "Epoch: 8/10\n",
      "Epoch [08/10] Step [000/127]: acc=0.5000 g_loss=0.4961 d_loss=0.7178 kd_loss=0.0115\n",
      "Epoch [08/10] Step [005/127]: acc=0.5000 g_loss=0.4973 d_loss=0.7139 kd_loss=0.0087\n",
      "Epoch [08/10] Step [010/127]: acc=0.5000 g_loss=0.4957 d_loss=0.7172 kd_loss=0.0195\n",
      "Epoch [08/10] Step [015/127]: acc=0.5000 g_loss=0.4958 d_loss=0.7144 kd_loss=0.0061\n",
      "Epoch [08/10] Step [020/127]: acc=0.5000 g_loss=0.4987 d_loss=0.7148 kd_loss=0.0134\n",
      "Epoch [08/10] Step [025/127]: acc=0.5000 g_loss=0.5001 d_loss=0.7144 kd_loss=0.0103\n",
      "Epoch [08/10] Step [030/127]: acc=0.5000 g_loss=0.5005 d_loss=0.7134 kd_loss=0.0227\n",
      "Epoch [08/10] Step [035/127]: acc=0.5000 g_loss=0.4994 d_loss=0.7146 kd_loss=0.0148\n",
      "Epoch [08/10] Step [040/127]: acc=0.5000 g_loss=0.4993 d_loss=0.7135 kd_loss=0.0072\n",
      "Epoch [08/10] Step [045/127]: acc=0.5000 g_loss=0.4890 d_loss=0.7197 kd_loss=0.0044\n",
      "Epoch [08/10] Step [050/127]: acc=0.5000 g_loss=0.4927 d_loss=0.7175 kd_loss=0.0074\n",
      "Epoch [08/10] Step [055/127]: acc=0.5000 g_loss=0.4859 d_loss=0.7231 kd_loss=0.0070\n",
      "Epoch [08/10] Step [060/127]: acc=0.5000 g_loss=0.4895 d_loss=0.7211 kd_loss=0.0116\n",
      "Epoch [08/10] Step [065/127]: acc=0.5000 g_loss=0.4919 d_loss=0.7202 kd_loss=0.0113\n",
      "Epoch [08/10] Step [070/127]: acc=0.5000 g_loss=0.4932 d_loss=0.7195 kd_loss=0.0264\n",
      "Epoch [08/10] Step [075/127]: acc=0.5000 g_loss=0.4962 d_loss=0.7175 kd_loss=0.0123\n",
      "Epoch [08/10] Step [080/127]: acc=0.5000 g_loss=0.5006 d_loss=0.7144 kd_loss=0.0068\n",
      "Epoch [08/10] Step [085/127]: acc=0.5000 g_loss=0.4964 d_loss=0.7190 kd_loss=0.0123\n",
      "Epoch [08/10] Step [090/127]: acc=0.5000 g_loss=0.5048 d_loss=0.7143 kd_loss=0.0093\n",
      "Epoch [08/10] Step [095/127]: acc=0.5000 g_loss=0.5008 d_loss=0.7187 kd_loss=0.0099\n",
      "Epoch [08/10] Step [100/127]: acc=0.5000 g_loss=0.5035 d_loss=0.7138 kd_loss=0.0258\n",
      "Epoch [08/10] Step [105/127]: acc=0.5000 g_loss=0.5030 d_loss=0.7187 kd_loss=0.0070\n",
      "Epoch [08/10] Step [110/127]: acc=0.5000 g_loss=0.5046 d_loss=0.7166 kd_loss=0.0159\n",
      "Epoch [08/10] Step [115/127]: acc=0.5000 g_loss=0.5042 d_loss=0.7149 kd_loss=0.0041\n",
      "Epoch [08/10] Step [120/127]: acc=0.5000 g_loss=0.5008 d_loss=0.7201 kd_loss=0.0055\n",
      "Epoch [08/10] Step [125/127]: acc=0.5000 g_loss=0.4971 d_loss=0.7207 kd_loss=0.0049\n",
      "French Test: \n",
      "\n",
      "Validation loss:  1.928431749343872\n",
      "Accuracy: 0.5642256902761105\n",
      "F1 score (Macro): 0.47250136845792595\n",
      "F1 score (Per class): [0.31147541 0.41588785 0.69014085]\n",
      "Precision score (Per class): [0.34862385 0.59731544 0.59652174]\n",
      "Recall score (Per class): [0.28148148 0.31899642 0.81861575]\n",
      "Epoch: 9/10\n",
      "Epoch [09/10] Step [000/127]: acc=0.5000 g_loss=0.5016 d_loss=0.7194 kd_loss=0.0049\n",
      "Epoch [09/10] Step [005/127]: acc=0.5000 g_loss=0.5091 d_loss=0.7136 kd_loss=0.0058\n",
      "Epoch [09/10] Step [010/127]: acc=0.5000 g_loss=0.5204 d_loss=0.7133 kd_loss=0.1204\n",
      "Epoch [09/10] Step [015/127]: acc=0.5000 g_loss=0.5360 d_loss=0.7018 kd_loss=0.0047\n",
      "Epoch [09/10] Step [020/127]: acc=0.5000 g_loss=0.5359 d_loss=0.7129 kd_loss=0.0072\n",
      "Epoch [09/10] Step [025/127]: acc=0.5000 g_loss=0.5236 d_loss=0.7222 kd_loss=0.0091\n",
      "Epoch [09/10] Step [030/127]: acc=0.5000 g_loss=0.5150 d_loss=0.7264 kd_loss=0.0140\n",
      "Epoch [09/10] Step [035/127]: acc=0.5000 g_loss=0.5144 d_loss=0.7202 kd_loss=0.0092\n",
      "Epoch [09/10] Step [040/127]: acc=0.5000 g_loss=0.5152 d_loss=0.7123 kd_loss=0.0059\n",
      "Epoch [09/10] Step [045/127]: acc=0.5000 g_loss=0.4969 d_loss=0.7183 kd_loss=0.0040\n",
      "Epoch [09/10] Step [050/127]: acc=0.5000 g_loss=0.4996 d_loss=0.7133 kd_loss=0.0065\n",
      "Epoch [09/10] Step [055/127]: acc=0.5000 g_loss=0.4913 d_loss=0.7210 kd_loss=0.0092\n",
      "Epoch [09/10] Step [060/127]: acc=0.5000 g_loss=0.4962 d_loss=0.7141 kd_loss=0.0112\n",
      "Epoch [09/10] Step [065/127]: acc=0.5000 g_loss=0.4884 d_loss=0.7139 kd_loss=0.0115\n",
      "Epoch [09/10] Step [070/127]: acc=0.5000 g_loss=0.4825 d_loss=0.7212 kd_loss=0.0682\n",
      "Epoch [09/10] Step [075/127]: acc=0.5000 g_loss=0.4845 d_loss=0.7198 kd_loss=0.0127\n",
      "Epoch [09/10] Step [080/127]: acc=0.5000 g_loss=0.4803 d_loss=0.7255 kd_loss=0.0292\n",
      "Epoch [09/10] Step [085/127]: acc=0.5000 g_loss=0.4846 d_loss=0.7223 kd_loss=0.0144\n",
      "Epoch [09/10] Step [090/127]: acc=0.5000 g_loss=0.4924 d_loss=0.7176 kd_loss=0.0183\n",
      "Epoch [09/10] Step [095/127]: acc=0.5000 g_loss=0.4905 d_loss=0.7236 kd_loss=0.0186\n",
      "Epoch [09/10] Step [100/127]: acc=0.5000 g_loss=0.4943 d_loss=0.7177 kd_loss=0.0135\n",
      "Epoch [09/10] Step [105/127]: acc=0.5000 g_loss=0.4959 d_loss=0.7210 kd_loss=0.0241\n",
      "Epoch [09/10] Step [110/127]: acc=0.5000 g_loss=0.5003 d_loss=0.7190 kd_loss=0.0184\n",
      "Epoch [09/10] Step [115/127]: acc=0.5000 g_loss=0.5046 d_loss=0.7157 kd_loss=0.0072\n",
      "Epoch [09/10] Step [120/127]: acc=0.5000 g_loss=0.5049 d_loss=0.7179 kd_loss=0.0103\n",
      "Epoch [09/10] Step [125/127]: acc=0.5000 g_loss=0.5075 d_loss=0.7179 kd_loss=0.0061\n",
      "French Test: \n",
      "\n",
      "Validation loss:  2.094109058380127\n",
      "Accuracy: 0.5426170468187275\n",
      "F1 score (Macro): 0.4798497520299692\n",
      "F1 score (Per class): [0.38040346 0.38518519 0.67396061]\n",
      "Precision score (Per class): [0.31132075 0.61904762 0.62222222]\n",
      "Recall score (Per class): [0.48888889 0.27956989 0.73508353]\n"
     ]
    }
   ],
   "source": [
    "tgt_encoder = adapt(src_encoder, discriminator,\n",
    "                    bi_lstm_classifier, train_dataloader, train_translated_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62e167dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15\n",
      "Epoch [00/15] Step [000/146]: acc=0.5000 g_loss=0.7005 d_loss=0.6907 kd_loss=0.3532\n",
      "Epoch [00/15] Step [005/146]: acc=0.5000 g_loss=0.7021 d_loss=0.6922 kd_loss=0.1627\n",
      "Epoch [00/15] Step [010/146]: acc=0.5000 g_loss=0.7035 d_loss=0.6921 kd_loss=0.3191\n",
      "Epoch [00/15] Step [015/146]: acc=0.5000 g_loss=0.7039 d_loss=0.6897 kd_loss=0.1806\n",
      "Epoch [00/15] Step [020/146]: acc=0.5000 g_loss=0.7065 d_loss=0.6895 kd_loss=0.0698\n",
      "Epoch [00/15] Step [025/146]: acc=0.5000 g_loss=0.7020 d_loss=0.6930 kd_loss=0.0240\n",
      "Epoch [00/15] Step [030/146]: acc=0.5000 g_loss=0.7039 d_loss=0.6898 kd_loss=0.1064\n",
      "Epoch [00/15] Step [035/146]: acc=0.5000 g_loss=0.6994 d_loss=0.6919 kd_loss=0.1433\n",
      "Epoch [00/15] Step [040/146]: acc=0.5000 g_loss=0.6982 d_loss=0.6918 kd_loss=0.2200\n",
      "Epoch [00/15] Step [045/146]: acc=0.5000 g_loss=0.6928 d_loss=0.6933 kd_loss=0.1643\n",
      "Epoch [00/15] Step [050/146]: acc=0.5000 g_loss=0.6909 d_loss=0.6916 kd_loss=0.3013\n",
      "Epoch [00/15] Step [055/146]: acc=0.5000 g_loss=0.6973 d_loss=0.6871 kd_loss=0.1706\n",
      "Epoch [00/15] Step [060/146]: acc=0.5000 g_loss=0.6944 d_loss=0.6880 kd_loss=0.1590\n",
      "Epoch [00/15] Step [065/146]: acc=0.5000 g_loss=0.6908 d_loss=0.6888 kd_loss=0.1592\n",
      "Epoch [00/15] Step [070/146]: acc=0.5000 g_loss=0.6918 d_loss=0.6872 kd_loss=0.0845\n",
      "Epoch [00/15] Step [075/146]: acc=0.5000 g_loss=0.6917 d_loss=0.6866 kd_loss=0.2020\n",
      "Epoch [00/15] Step [080/146]: acc=0.5000 g_loss=0.6931 d_loss=0.6857 kd_loss=0.2872\n",
      "Epoch [00/15] Step [085/146]: acc=0.5000 g_loss=0.6892 d_loss=0.6864 kd_loss=0.1490\n",
      "Epoch [00/15] Step [090/146]: acc=0.5000 g_loss=0.6931 d_loss=0.6850 kd_loss=0.1473\n",
      "Epoch [00/15] Step [095/146]: acc=0.5000 g_loss=0.6933 d_loss=0.6807 kd_loss=0.0760\n",
      "Epoch [00/15] Step [100/146]: acc=0.5000 g_loss=0.6882 d_loss=0.6825 kd_loss=0.1944\n",
      "Epoch [00/15] Step [105/146]: acc=0.5000 g_loss=0.6951 d_loss=0.6786 kd_loss=0.2661\n",
      "Epoch [00/15] Step [110/146]: acc=0.5000 g_loss=0.6895 d_loss=0.6822 kd_loss=0.0497\n",
      "Epoch [00/15] Step [115/146]: acc=0.5000 g_loss=0.6888 d_loss=0.6797 kd_loss=0.1209\n",
      "Epoch [00/15] Step [120/146]: acc=0.5000 g_loss=0.6907 d_loss=0.6763 kd_loss=0.1921\n",
      "Epoch [00/15] Step [125/146]: acc=0.5000 g_loss=0.6853 d_loss=0.6802 kd_loss=0.1733\n",
      "Epoch [00/15] Step [130/146]: acc=0.5000 g_loss=0.6894 d_loss=0.6798 kd_loss=0.2991\n",
      "Epoch [00/15] Step [135/146]: acc=0.5000 g_loss=0.6945 d_loss=0.6787 kd_loss=0.1277\n",
      "Epoch [00/15] Step [140/146]: acc=0.5000 g_loss=0.6875 d_loss=0.6714 kd_loss=0.1663\n",
      "Epoch [00/15] Step [145/146]: acc=0.2727 g_loss=0.6873 d_loss=0.6669 kd_loss=0.2377\n",
      "German Test:\n",
      "Validation loss:  2.1525228023529053\n",
      "Accuracy: 0.5946601941747572\n",
      "F1 score (Macro): 0.5047146934552506\n",
      "F1 score (Per class): [0.330033   0.47094801 0.71316306]\n",
      "Precision score (Per class): [0.25641026 0.48734177 0.77070064]\n",
      "Recall score (Per class): [0.46296296 0.4556213  0.66361974]\n",
      "Epoch: 1/15\n",
      "Epoch [01/15] Step [000/146]: acc=0.5000 g_loss=0.6432 d_loss=0.6929 kd_loss=0.2442\n",
      "Epoch [01/15] Step [005/146]: acc=0.5000 g_loss=0.6318 d_loss=0.6981 kd_loss=0.1245\n",
      "Epoch [01/15] Step [010/146]: acc=0.5000 g_loss=0.6429 d_loss=0.6886 kd_loss=0.0829\n",
      "Epoch [01/15] Step [015/146]: acc=0.5000 g_loss=0.6547 d_loss=0.6890 kd_loss=0.0389\n",
      "Epoch [01/15] Step [020/146]: acc=0.5000 g_loss=0.6630 d_loss=0.6831 kd_loss=0.0223\n",
      "Epoch [01/15] Step [025/146]: acc=0.5000 g_loss=0.6927 d_loss=0.6751 kd_loss=0.0158\n",
      "Epoch [01/15] Step [030/146]: acc=0.5000 g_loss=0.7007 d_loss=0.6711 kd_loss=0.0497\n",
      "Epoch [01/15] Step [035/146]: acc=0.5000 g_loss=0.7042 d_loss=0.6718 kd_loss=0.0483\n",
      "Epoch [01/15] Step [040/146]: acc=0.5000 g_loss=0.6970 d_loss=0.6776 kd_loss=0.1161\n",
      "Epoch [01/15] Step [045/146]: acc=0.5000 g_loss=0.6801 d_loss=0.6790 kd_loss=0.0743\n",
      "Epoch [01/15] Step [050/146]: acc=0.5000 g_loss=0.6672 d_loss=0.6772 kd_loss=0.0768\n",
      "Epoch [01/15] Step [055/146]: acc=0.5000 g_loss=0.6845 d_loss=0.6686 kd_loss=0.0582\n",
      "Epoch [01/15] Step [060/146]: acc=0.5000 g_loss=0.6735 d_loss=0.6746 kd_loss=0.0648\n",
      "Epoch [01/15] Step [065/146]: acc=0.5000 g_loss=0.6514 d_loss=0.6761 kd_loss=0.0766\n",
      "Epoch [01/15] Step [070/146]: acc=0.5000 g_loss=0.6494 d_loss=0.6750 kd_loss=0.0671\n",
      "Epoch [01/15] Step [075/146]: acc=0.5000 g_loss=0.6475 d_loss=0.6836 kd_loss=0.2307\n",
      "Epoch [01/15] Step [080/146]: acc=0.5000 g_loss=0.6549 d_loss=0.6771 kd_loss=0.1379\n",
      "Epoch [01/15] Step [085/146]: acc=0.5000 g_loss=0.6420 d_loss=0.6759 kd_loss=0.0948\n",
      "Epoch [01/15] Step [090/146]: acc=0.5000 g_loss=0.6552 d_loss=0.6757 kd_loss=0.2131\n",
      "Epoch [01/15] Step [095/146]: acc=0.5000 g_loss=0.6523 d_loss=0.6698 kd_loss=0.1146\n",
      "Epoch [01/15] Step [100/146]: acc=0.5000 g_loss=0.6477 d_loss=0.6716 kd_loss=0.0703\n",
      "Epoch [01/15] Step [105/146]: acc=0.5000 g_loss=0.6643 d_loss=0.6670 kd_loss=0.2451\n",
      "Epoch [01/15] Step [110/146]: acc=0.5000 g_loss=0.6554 d_loss=0.6734 kd_loss=0.1261\n",
      "Epoch [01/15] Step [115/146]: acc=0.5000 g_loss=0.6634 d_loss=0.6636 kd_loss=0.0481\n",
      "Epoch [01/15] Step [120/146]: acc=0.5000 g_loss=0.6492 d_loss=0.6767 kd_loss=0.1234\n",
      "Epoch [01/15] Step [125/146]: acc=0.5000 g_loss=0.6498 d_loss=0.6736 kd_loss=0.0996\n",
      "Epoch [01/15] Step [130/146]: acc=0.5000 g_loss=0.6555 d_loss=0.6706 kd_loss=0.1373\n",
      "Epoch [01/15] Step [135/146]: acc=0.5000 g_loss=0.6615 d_loss=0.6762 kd_loss=0.0739\n",
      "Epoch [01/15] Step [140/146]: acc=0.5000 g_loss=0.6645 d_loss=0.6572 kd_loss=0.0534\n",
      "Epoch [01/15] Step [145/146]: acc=0.2727 g_loss=0.6381 d_loss=0.6494 kd_loss=0.0454\n",
      "German Test:\n",
      "Validation loss:  1.924775242805481\n",
      "Accuracy: 0.6322815533980582\n",
      "F1 score (Macro): 0.5257480901792909\n",
      "F1 score (Per class): [0.35587189 0.47019868 0.75117371]\n",
      "Precision score (Per class): [0.28901734 0.53383459 0.77220077]\n",
      "Recall score (Per class): [0.46296296 0.42011834 0.73126143]\n",
      "Epoch: 2/15\n",
      "Epoch [02/15] Step [000/146]: acc=0.5000 g_loss=0.5829 d_loss=0.7137 kd_loss=0.1712\n",
      "Epoch [02/15] Step [005/146]: acc=0.5000 g_loss=0.5757 d_loss=0.7186 kd_loss=0.1137\n",
      "Epoch [02/15] Step [010/146]: acc=0.5000 g_loss=0.6032 d_loss=0.6973 kd_loss=0.2071\n",
      "Epoch [02/15] Step [015/146]: acc=0.5000 g_loss=0.6388 d_loss=0.6803 kd_loss=0.1583\n",
      "Epoch [02/15] Step [020/146]: acc=0.5000 g_loss=0.6756 d_loss=0.6694 kd_loss=0.0812\n",
      "Epoch [02/15] Step [025/146]: acc=0.5000 g_loss=0.7003 d_loss=0.6838 kd_loss=0.0446\n",
      "Epoch [02/15] Step [030/146]: acc=0.5000 g_loss=0.7486 d_loss=0.6594 kd_loss=0.1278\n",
      "Epoch [02/15] Step [035/146]: acc=0.5000 g_loss=0.6850 d_loss=0.7014 kd_loss=0.1123\n",
      "Epoch [02/15] Step [040/146]: acc=0.5000 g_loss=0.6720 d_loss=0.7144 kd_loss=0.1439\n",
      "Epoch [02/15] Step [045/146]: acc=0.5000 g_loss=0.6509 d_loss=0.7084 kd_loss=0.1301\n",
      "Epoch [02/15] Step [050/146]: acc=0.5000 g_loss=0.6472 d_loss=0.6932 kd_loss=0.1222\n",
      "Epoch [02/15] Step [055/146]: acc=0.5000 g_loss=0.6793 d_loss=0.6723 kd_loss=0.5006\n",
      "Epoch [02/15] Step [060/146]: acc=0.5000 g_loss=0.6744 d_loss=0.6812 kd_loss=0.4313\n",
      "Epoch [02/15] Step [065/146]: acc=0.5000 g_loss=0.6577 d_loss=0.6925 kd_loss=0.2172\n",
      "Epoch [02/15] Step [070/146]: acc=0.5000 g_loss=0.6614 d_loss=0.6867 kd_loss=0.0958\n",
      "Epoch [02/15] Step [075/146]: acc=0.5000 g_loss=0.6520 d_loss=0.6848 kd_loss=0.1731\n",
      "Epoch [02/15] Step [080/146]: acc=0.5000 g_loss=0.6608 d_loss=0.6849 kd_loss=0.3826\n",
      "Epoch [02/15] Step [085/146]: acc=0.5000 g_loss=0.6693 d_loss=0.6792 kd_loss=0.0796\n",
      "Epoch [02/15] Step [090/146]: acc=0.5000 g_loss=0.6633 d_loss=0.6848 kd_loss=0.1885\n",
      "Epoch [02/15] Step [095/146]: acc=0.5000 g_loss=0.6559 d_loss=0.6807 kd_loss=0.1043\n",
      "Epoch [02/15] Step [100/146]: acc=0.5000 g_loss=0.6587 d_loss=0.6718 kd_loss=0.3066\n",
      "Epoch [02/15] Step [105/146]: acc=0.5000 g_loss=0.6734 d_loss=0.6741 kd_loss=0.1722\n",
      "Epoch [02/15] Step [110/146]: acc=0.5000 g_loss=0.6716 d_loss=0.6782 kd_loss=0.1359\n",
      "Epoch [02/15] Step [115/146]: acc=0.5000 g_loss=0.6592 d_loss=0.6755 kd_loss=0.0487\n",
      "Epoch [02/15] Step [120/146]: acc=0.5000 g_loss=0.6541 d_loss=0.6784 kd_loss=0.1838\n",
      "Epoch [02/15] Step [125/146]: acc=0.5000 g_loss=0.6521 d_loss=0.6821 kd_loss=0.2241\n",
      "Epoch [02/15] Step [130/146]: acc=0.5000 g_loss=0.6650 d_loss=0.6772 kd_loss=0.2997\n",
      "Epoch [02/15] Step [135/146]: acc=0.5000 g_loss=0.6644 d_loss=0.6841 kd_loss=0.1579\n",
      "Epoch [02/15] Step [140/146]: acc=0.5000 g_loss=0.6681 d_loss=0.6650 kd_loss=0.1519\n",
      "Epoch [02/15] Step [145/146]: acc=0.2727 g_loss=0.6265 d_loss=0.6674 kd_loss=0.0436\n",
      "German Test:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  1.8916826248168945\n",
      "Accuracy: 0.6310679611650486\n",
      "F1 score (Macro): 0.52744611601506\n",
      "F1 score (Per class): [0.37162162 0.4604811  0.75023563]\n",
      "Precision score (Per class): [0.29255319 0.54918033 0.77431907]\n",
      "Recall score (Per class): [0.50925926 0.3964497  0.72760512]\n",
      "Epoch: 3/15\n",
      "Epoch [03/15] Step [000/146]: acc=0.5000 g_loss=0.5983 d_loss=0.7115 kd_loss=0.0591\n",
      "Epoch [03/15] Step [005/146]: acc=0.5000 g_loss=0.5879 d_loss=0.7148 kd_loss=0.0776\n",
      "Epoch [03/15] Step [010/146]: acc=0.5000 g_loss=0.5973 d_loss=0.7021 kd_loss=0.1140\n",
      "Epoch [03/15] Step [015/146]: acc=0.5000 g_loss=0.6240 d_loss=0.6935 kd_loss=0.0547\n",
      "Epoch [03/15] Step [020/146]: acc=0.5000 g_loss=0.6571 d_loss=0.6787 kd_loss=0.0311\n",
      "Epoch [03/15] Step [025/146]: acc=0.5000 g_loss=0.6575 d_loss=0.6998 kd_loss=0.0151\n",
      "Epoch [03/15] Step [030/146]: acc=0.5000 g_loss=0.6930 d_loss=0.6766 kd_loss=0.0577\n",
      "Epoch [03/15] Step [035/146]: acc=0.5000 g_loss=0.6136 d_loss=0.7287 kd_loss=0.1000\n",
      "Epoch [03/15] Step [040/146]: acc=0.5000 g_loss=0.6048 d_loss=0.7349 kd_loss=0.1501\n",
      "Epoch [03/15] Step [045/146]: acc=0.5000 g_loss=0.6424 d_loss=0.6994 kd_loss=0.0660\n",
      "Epoch [03/15] Step [050/146]: acc=0.5000 g_loss=0.6361 d_loss=0.6950 kd_loss=0.1132\n",
      "Epoch [03/15] Step [055/146]: acc=0.5000 g_loss=0.6669 d_loss=0.6792 kd_loss=0.1421\n",
      "Epoch [03/15] Step [060/146]: acc=0.5000 g_loss=0.6747 d_loss=0.6809 kd_loss=0.1954\n",
      "Epoch [03/15] Step [065/146]: acc=0.5000 g_loss=0.6507 d_loss=0.7025 kd_loss=0.1638\n",
      "Epoch [03/15] Step [070/146]: acc=0.5000 g_loss=0.6509 d_loss=0.6970 kd_loss=0.2614\n",
      "Epoch [03/15] Step [075/146]: acc=0.5000 g_loss=0.6447 d_loss=0.6899 kd_loss=0.1186\n",
      "Epoch [03/15] Step [080/146]: acc=0.5000 g_loss=0.6597 d_loss=0.6879 kd_loss=0.1051\n",
      "Epoch [03/15] Step [085/146]: acc=0.5000 g_loss=0.6670 d_loss=0.6826 kd_loss=0.0838\n",
      "Epoch [03/15] Step [090/146]: acc=0.5000 g_loss=0.6523 d_loss=0.6937 kd_loss=0.5569\n",
      "Epoch [03/15] Step [095/146]: acc=0.5000 g_loss=0.6500 d_loss=0.6849 kd_loss=0.1235\n",
      "Epoch [03/15] Step [100/146]: acc=0.5000 g_loss=0.6483 d_loss=0.6812 kd_loss=0.1312\n",
      "Epoch [03/15] Step [105/146]: acc=0.5000 g_loss=0.6708 d_loss=0.6763 kd_loss=0.1099\n",
      "Epoch [03/15] Step [110/146]: acc=0.5000 g_loss=0.6703 d_loss=0.6822 kd_loss=0.0406\n",
      "Epoch [03/15] Step [115/146]: acc=0.5000 g_loss=0.6520 d_loss=0.6851 kd_loss=0.0571\n",
      "Epoch [03/15] Step [120/146]: acc=0.5000 g_loss=0.6461 d_loss=0.6835 kd_loss=0.1102\n",
      "Epoch [03/15] Step [125/146]: acc=0.5000 g_loss=0.6559 d_loss=0.6850 kd_loss=0.0876\n",
      "Epoch [03/15] Step [130/146]: acc=0.5000 g_loss=0.6644 d_loss=0.6855 kd_loss=0.0621\n",
      "Epoch [03/15] Step [135/146]: acc=0.5000 g_loss=0.6629 d_loss=0.6897 kd_loss=0.0560\n",
      "Epoch [03/15] Step [140/146]: acc=0.5000 g_loss=0.6603 d_loss=0.6762 kd_loss=0.0450\n",
      "Epoch [03/15] Step [145/146]: acc=0.2727 g_loss=0.6226 d_loss=0.6760 kd_loss=0.0726\n",
      "German Test:\n",
      "Validation loss:  2.056084632873535\n",
      "Accuracy: 0.6201456310679612\n",
      "F1 score (Macro): 0.5192680437963596\n",
      "F1 score (Per class): [0.35409836 0.46366782 0.74003795]\n",
      "Precision score (Per class): [0.27411168 0.55833333 0.76923077]\n",
      "Recall score (Per class): [0.5        0.3964497  0.71297989]\n",
      "Epoch: 4/15\n",
      "Epoch [04/15] Step [000/146]: acc=0.5000 g_loss=0.5964 d_loss=0.7217 kd_loss=0.0749\n",
      "Epoch [04/15] Step [005/146]: acc=0.5000 g_loss=0.5895 d_loss=0.7241 kd_loss=0.0994\n",
      "Epoch [04/15] Step [010/146]: acc=0.5000 g_loss=0.5984 d_loss=0.7100 kd_loss=0.0496\n",
      "Epoch [04/15] Step [015/146]: acc=0.5000 g_loss=0.6217 d_loss=0.7083 kd_loss=0.0351\n",
      "Epoch [04/15] Step [020/146]: acc=0.5000 g_loss=0.6462 d_loss=0.6939 kd_loss=0.0429\n",
      "Epoch [04/15] Step [025/146]: acc=0.5000 g_loss=0.6512 d_loss=0.6992 kd_loss=0.0355\n",
      "Epoch [04/15] Step [030/146]: acc=0.5000 g_loss=0.6431 d_loss=0.7055 kd_loss=0.0604\n",
      "Epoch [04/15] Step [035/146]: acc=0.5000 g_loss=0.6340 d_loss=0.7149 kd_loss=0.0387\n",
      "Epoch [04/15] Step [040/146]: acc=0.5000 g_loss=0.6454 d_loss=0.7043 kd_loss=0.0906\n",
      "Epoch [04/15] Step [045/146]: acc=0.5000 g_loss=0.6558 d_loss=0.6958 kd_loss=0.0615\n",
      "Epoch [04/15] Step [050/146]: acc=0.5000 g_loss=0.6509 d_loss=0.6949 kd_loss=0.0840\n",
      "Epoch [04/15] Step [055/146]: acc=0.5000 g_loss=0.6673 d_loss=0.6854 kd_loss=0.2261\n",
      "Epoch [04/15] Step [060/146]: acc=0.5000 g_loss=0.6722 d_loss=0.6818 kd_loss=0.2510\n",
      "Epoch [04/15] Step [065/146]: acc=0.5000 g_loss=0.6542 d_loss=0.6990 kd_loss=0.0712\n",
      "Epoch [04/15] Step [070/146]: acc=0.5000 g_loss=0.6611 d_loss=0.6961 kd_loss=0.1363\n",
      "Epoch [04/15] Step [075/146]: acc=0.5000 g_loss=0.6498 d_loss=0.6871 kd_loss=0.2081\n",
      "Epoch [04/15] Step [080/146]: acc=0.5000 g_loss=0.6554 d_loss=0.6930 kd_loss=0.1296\n",
      "Epoch [04/15] Step [085/146]: acc=0.5000 g_loss=0.6676 d_loss=0.6843 kd_loss=0.0704\n",
      "Epoch [04/15] Step [090/146]: acc=0.5000 g_loss=0.6552 d_loss=0.6946 kd_loss=0.3106\n",
      "Epoch [04/15] Step [095/146]: acc=0.5000 g_loss=0.6539 d_loss=0.6865 kd_loss=0.0880\n",
      "Epoch [04/15] Step [100/146]: acc=0.5000 g_loss=0.6422 d_loss=0.6862 kd_loss=0.0984\n",
      "Epoch [04/15] Step [105/146]: acc=0.5000 g_loss=0.6566 d_loss=0.6869 kd_loss=0.1864\n",
      "Epoch [04/15] Step [110/146]: acc=0.5000 g_loss=0.6676 d_loss=0.6887 kd_loss=0.0839\n",
      "Epoch [04/15] Step [115/146]: acc=0.5000 g_loss=0.6541 d_loss=0.6862 kd_loss=0.0548\n",
      "Epoch [04/15] Step [120/146]: acc=0.5000 g_loss=0.6449 d_loss=0.6899 kd_loss=0.0470\n",
      "Epoch [04/15] Step [125/146]: acc=0.5000 g_loss=0.6499 d_loss=0.6886 kd_loss=0.1396\n",
      "Epoch [04/15] Step [130/146]: acc=0.5000 g_loss=0.6677 d_loss=0.6879 kd_loss=0.1480\n",
      "Epoch [04/15] Step [135/146]: acc=0.5000 g_loss=0.6656 d_loss=0.6926 kd_loss=0.2080\n",
      "Epoch [04/15] Step [140/146]: acc=0.5000 g_loss=0.6620 d_loss=0.6825 kd_loss=0.0830\n",
      "Epoch [04/15] Step [145/146]: acc=0.2727 g_loss=0.6305 d_loss=0.6780 kd_loss=0.0594\n",
      "German Test:\n",
      "Validation loss:  1.9418596029281616\n",
      "Accuracy: 0.6735436893203883\n",
      "F1 score (Macro): 0.5408701289529336\n",
      "F1 score (Per class): [0.36842105 0.46689895 0.78729038]\n",
      "Precision score (Per class): [0.35       0.56779661 0.76109215]\n",
      "Recall score (Per class): [0.38888889 0.3964497  0.81535649]\n",
      "Epoch: 5/15\n",
      "Epoch [05/15] Step [000/146]: acc=0.5000 g_loss=0.5998 d_loss=0.7235 kd_loss=0.0767\n",
      "Epoch [05/15] Step [005/146]: acc=0.5000 g_loss=0.6058 d_loss=0.7185 kd_loss=0.0395\n",
      "Epoch [05/15] Step [010/146]: acc=0.5000 g_loss=0.6170 d_loss=0.7109 kd_loss=0.0916\n",
      "Epoch [05/15] Step [015/146]: acc=0.5000 g_loss=0.6367 d_loss=0.7105 kd_loss=0.0901\n",
      "Epoch [05/15] Step [020/146]: acc=0.5000 g_loss=0.6679 d_loss=0.6925 kd_loss=0.0423\n",
      "Epoch [05/15] Step [025/146]: acc=0.5000 g_loss=0.6721 d_loss=0.6951 kd_loss=0.0173\n",
      "Epoch [05/15] Step [030/146]: acc=0.5000 g_loss=0.6682 d_loss=0.6973 kd_loss=0.0742\n",
      "Epoch [05/15] Step [035/146]: acc=0.5000 g_loss=0.6618 d_loss=0.7048 kd_loss=0.0587\n",
      "Epoch [05/15] Step [040/146]: acc=0.5000 g_loss=0.6699 d_loss=0.6969 kd_loss=0.1059\n",
      "Epoch [05/15] Step [045/146]: acc=0.5000 g_loss=0.6651 d_loss=0.6952 kd_loss=0.0677\n",
      "Epoch [05/15] Step [050/146]: acc=0.5000 g_loss=0.6588 d_loss=0.6954 kd_loss=0.0477\n",
      "Epoch [05/15] Step [055/146]: acc=0.5000 g_loss=0.6764 d_loss=0.6863 kd_loss=0.0860\n",
      "Epoch [05/15] Step [060/146]: acc=0.5000 g_loss=0.6731 d_loss=0.6871 kd_loss=0.0316\n",
      "Epoch [05/15] Step [065/146]: acc=0.5000 g_loss=0.6559 d_loss=0.7017 kd_loss=0.0638\n",
      "Epoch [05/15] Step [070/146]: acc=0.5000 g_loss=0.6601 d_loss=0.6972 kd_loss=0.0422\n",
      "Epoch [05/15] Step [075/146]: acc=0.5000 g_loss=0.6521 d_loss=0.6909 kd_loss=0.0806\n",
      "Epoch [05/15] Step [080/146]: acc=0.5000 g_loss=0.6585 d_loss=0.6940 kd_loss=0.1263\n",
      "Epoch [05/15] Step [085/146]: acc=0.5000 g_loss=0.6663 d_loss=0.6896 kd_loss=0.0484\n",
      "Epoch [05/15] Step [090/146]: acc=0.5000 g_loss=0.6618 d_loss=0.6944 kd_loss=0.1299\n",
      "Epoch [05/15] Step [095/146]: acc=0.5000 g_loss=0.6620 d_loss=0.6881 kd_loss=0.0976\n",
      "Epoch [05/15] Step [100/146]: acc=0.5000 g_loss=0.6515 d_loss=0.6881 kd_loss=0.1174\n",
      "Epoch [05/15] Step [105/146]: acc=0.5000 g_loss=0.6537 d_loss=0.6921 kd_loss=0.1718\n",
      "Epoch [05/15] Step [110/146]: acc=0.5000 g_loss=0.6701 d_loss=0.6904 kd_loss=0.0929\n",
      "Epoch [05/15] Step [115/146]: acc=0.5000 g_loss=0.6575 d_loss=0.6897 kd_loss=0.0396\n",
      "Epoch [05/15] Step [120/146]: acc=0.5000 g_loss=0.6537 d_loss=0.6917 kd_loss=0.1794\n",
      "Epoch [05/15] Step [125/146]: acc=0.5000 g_loss=0.6586 d_loss=0.6914 kd_loss=0.1153\n",
      "Epoch [05/15] Step [130/146]: acc=0.5000 g_loss=0.6682 d_loss=0.6911 kd_loss=0.1178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [05/15] Step [135/146]: acc=0.5000 g_loss=0.6655 d_loss=0.6942 kd_loss=0.0619\n",
      "Epoch [05/15] Step [140/146]: acc=0.5000 g_loss=0.6683 d_loss=0.6842 kd_loss=0.0359\n",
      "Epoch [05/15] Step [145/146]: acc=0.2727 g_loss=0.6429 d_loss=0.6841 kd_loss=0.0538\n",
      "German Test:\n",
      "Validation loss:  1.9258086681365967\n",
      "Accuracy: 0.6735436893203883\n",
      "F1 score (Macro): 0.5320129484051602\n",
      "F1 score (Per class): [0.35193133 0.4535316  0.79057592]\n",
      "Precision score (Per class): [0.328      0.61       0.75626043]\n",
      "Recall score (Per class): [0.37962963 0.36094675 0.82815356]\n",
      "Epoch: 6/15\n",
      "Epoch [06/15] Step [000/146]: acc=0.5000 g_loss=0.6185 d_loss=0.7186 kd_loss=0.0487\n",
      "Epoch [06/15] Step [005/146]: acc=0.5000 g_loss=0.6240 d_loss=0.7142 kd_loss=0.0382\n",
      "Epoch [06/15] Step [010/146]: acc=0.5000 g_loss=0.6346 d_loss=0.7038 kd_loss=0.0237\n",
      "Epoch [06/15] Step [015/146]: acc=0.5000 g_loss=0.6483 d_loss=0.7053 kd_loss=0.0454\n",
      "Epoch [06/15] Step [020/146]: acc=0.5000 g_loss=0.6662 d_loss=0.6961 kd_loss=0.0931\n",
      "Epoch [06/15] Step [025/146]: acc=0.5000 g_loss=0.6703 d_loss=0.6967 kd_loss=0.0202\n",
      "Epoch [06/15] Step [030/146]: acc=0.5000 g_loss=0.6715 d_loss=0.6954 kd_loss=0.0313\n",
      "Epoch [06/15] Step [035/146]: acc=0.5000 g_loss=0.6683 d_loss=0.6996 kd_loss=0.0595\n",
      "Epoch [06/15] Step [040/146]: acc=0.5000 g_loss=0.6767 d_loss=0.6919 kd_loss=0.0589\n",
      "Epoch [06/15] Step [045/146]: acc=0.5000 g_loss=0.6668 d_loss=0.6966 kd_loss=0.0696\n",
      "Epoch [06/15] Step [050/146]: acc=0.5000 g_loss=0.6598 d_loss=0.6974 kd_loss=0.0198\n",
      "Epoch [06/15] Step [055/146]: acc=0.5000 g_loss=0.6741 d_loss=0.6901 kd_loss=0.0342\n",
      "Epoch [06/15] Step [060/146]: acc=0.5000 g_loss=0.6745 d_loss=0.6890 kd_loss=0.1205\n",
      "Epoch [06/15] Step [065/146]: acc=0.5000 g_loss=0.6616 d_loss=0.6989 kd_loss=0.1106\n",
      "Epoch [06/15] Step [070/146]: acc=0.5000 g_loss=0.6636 d_loss=0.6971 kd_loss=0.0795\n",
      "Epoch [06/15] Step [075/146]: acc=0.5000 g_loss=0.6587 d_loss=0.6916 kd_loss=0.0889\n",
      "Epoch [06/15] Step [080/146]: acc=0.5000 g_loss=0.6629 d_loss=0.6951 kd_loss=0.1183\n",
      "Epoch [06/15] Step [085/146]: acc=0.5000 g_loss=0.6705 d_loss=0.6900 kd_loss=0.0897\n",
      "Epoch [06/15] Step [090/146]: acc=0.5000 g_loss=0.6663 d_loss=0.6962 kd_loss=0.1351\n",
      "Epoch [06/15] Step [095/146]: acc=0.5000 g_loss=0.6677 d_loss=0.6886 kd_loss=0.0652\n",
      "Epoch [06/15] Step [100/146]: acc=0.5000 g_loss=0.6591 d_loss=0.6904 kd_loss=0.0517\n",
      "Epoch [06/15] Step [105/146]: acc=0.5000 g_loss=0.6648 d_loss=0.6924 kd_loss=0.1311\n",
      "Epoch [06/15] Step [110/146]: acc=0.5000 g_loss=0.6770 d_loss=0.6906 kd_loss=0.0410\n",
      "Epoch [06/15] Step [115/146]: acc=0.5000 g_loss=0.6639 d_loss=0.6914 kd_loss=0.0292\n",
      "Epoch [06/15] Step [120/146]: acc=0.5000 g_loss=0.6622 d_loss=0.6903 kd_loss=0.0907\n",
      "Epoch [06/15] Step [125/146]: acc=0.5000 g_loss=0.6680 d_loss=0.6901 kd_loss=0.0561\n",
      "Epoch [06/15] Step [130/146]: acc=0.5000 g_loss=0.6755 d_loss=0.6919 kd_loss=0.1146\n",
      "Epoch [06/15] Step [135/146]: acc=0.5000 g_loss=0.6718 d_loss=0.6945 kd_loss=0.0295\n",
      "Epoch [06/15] Step [140/146]: acc=0.5000 g_loss=0.6712 d_loss=0.6874 kd_loss=0.0581\n",
      "Epoch [06/15] Step [145/146]: acc=0.2727 g_loss=0.6480 d_loss=0.6859 kd_loss=0.0520\n",
      "German Test:\n",
      "Validation loss:  1.962018370628357\n",
      "Accuracy: 0.6796116504854369\n",
      "F1 score (Macro): 0.5322823618997541\n",
      "F1 score (Per class): [0.3652968  0.43773585 0.79381443]\n",
      "Precision score (Per class): [0.36036036 0.60416667 0.74878444]\n",
      "Recall score (Per class): [0.37037037 0.34319527 0.84460695]\n",
      "Epoch: 7/15\n",
      "Epoch [07/15] Step [000/146]: acc=0.5000 g_loss=0.6305 d_loss=0.7149 kd_loss=0.0312\n",
      "Epoch [07/15] Step [005/146]: acc=0.5000 g_loss=0.6363 d_loss=0.7096 kd_loss=0.0369\n",
      "Epoch [07/15] Step [010/146]: acc=0.5000 g_loss=0.6456 d_loss=0.7029 kd_loss=0.0688\n",
      "Epoch [07/15] Step [015/146]: acc=0.5000 g_loss=0.6565 d_loss=0.7043 kd_loss=0.0451\n",
      "Epoch [07/15] Step [020/146]: acc=0.5000 g_loss=0.6735 d_loss=0.6944 kd_loss=0.0241\n",
      "Epoch [07/15] Step [025/146]: acc=0.5000 g_loss=0.6778 d_loss=0.6944 kd_loss=0.0157\n",
      "Epoch [07/15] Step [030/146]: acc=0.5000 g_loss=0.6789 d_loss=0.6956 kd_loss=0.0830\n",
      "Epoch [07/15] Step [035/146]: acc=0.5000 g_loss=0.6786 d_loss=0.6971 kd_loss=0.0290\n",
      "Epoch [07/15] Step [040/146]: acc=0.5000 g_loss=0.6866 d_loss=0.6894 kd_loss=0.0200\n",
      "Epoch [07/15] Step [045/146]: acc=0.5000 g_loss=0.6741 d_loss=0.6948 kd_loss=0.0334\n",
      "Epoch [07/15] Step [050/146]: acc=0.5000 g_loss=0.6661 d_loss=0.6976 kd_loss=0.0546\n",
      "Epoch [07/15] Step [055/146]: acc=0.5000 g_loss=0.6773 d_loss=0.6915 kd_loss=0.0708\n",
      "Epoch [07/15] Step [060/146]: acc=0.5000 g_loss=0.6829 d_loss=0.6883 kd_loss=0.0721\n",
      "Epoch [07/15] Step [065/146]: acc=0.5000 g_loss=0.6698 d_loss=0.6980 kd_loss=0.1033\n",
      "Epoch [07/15] Step [070/146]: acc=0.5000 g_loss=0.6713 d_loss=0.6965 kd_loss=0.0367\n",
      "Epoch [07/15] Step [075/146]: acc=0.5000 g_loss=0.6670 d_loss=0.6922 kd_loss=0.0405\n",
      "Epoch [07/15] Step [080/146]: acc=0.5000 g_loss=0.6711 d_loss=0.6956 kd_loss=0.2601\n",
      "Epoch [07/15] Step [085/146]: acc=0.5000 g_loss=0.6759 d_loss=0.6911 kd_loss=0.0420\n",
      "Epoch [07/15] Step [090/146]: acc=0.5000 g_loss=0.6701 d_loss=0.6959 kd_loss=0.1399\n",
      "Epoch [07/15] Step [095/146]: acc=0.5000 g_loss=0.6705 d_loss=0.6915 kd_loss=0.0630\n",
      "Epoch [07/15] Step [100/146]: acc=0.5000 g_loss=0.6661 d_loss=0.6911 kd_loss=0.0613\n",
      "Epoch [07/15] Step [105/146]: acc=0.5000 g_loss=0.6727 d_loss=0.6909 kd_loss=0.1043\n",
      "Epoch [07/15] Step [110/146]: acc=0.5000 g_loss=0.6806 d_loss=0.6914 kd_loss=0.0536\n",
      "Epoch [07/15] Step [115/146]: acc=0.5000 g_loss=0.6694 d_loss=0.6921 kd_loss=0.0507\n",
      "Epoch [07/15] Step [120/146]: acc=0.5000 g_loss=0.6680 d_loss=0.6916 kd_loss=0.0351\n",
      "Epoch [07/15] Step [125/146]: acc=0.5000 g_loss=0.6711 d_loss=0.6914 kd_loss=0.0619\n",
      "Epoch [07/15] Step [130/146]: acc=0.5000 g_loss=0.6804 d_loss=0.6925 kd_loss=0.0539\n",
      "Epoch [07/15] Step [135/146]: acc=0.5000 g_loss=0.6769 d_loss=0.6945 kd_loss=0.0279\n",
      "Epoch [07/15] Step [140/146]: acc=0.5000 g_loss=0.6759 d_loss=0.6890 kd_loss=0.0579\n",
      "Epoch [07/15] Step [145/146]: acc=0.2727 g_loss=0.6580 d_loss=0.6889 kd_loss=0.0469\n",
      "German Test:\n",
      "Validation loss:  2.0531668663024902\n",
      "Accuracy: 0.6808252427184466\n",
      "F1 score (Macro): 0.5179594854908472\n",
      "F1 score (Per class): [0.367713   0.38818565 0.7979798 ]\n",
      "Precision score (Per class): [0.35652174 0.67647059 0.73946958]\n",
      "Recall score (Per class): [0.37962963 0.27218935 0.86654479]\n",
      "Epoch: 8/15\n",
      "Epoch [08/15] Step [000/146]: acc=0.5000 g_loss=0.6438 d_loss=0.7104 kd_loss=0.0611\n",
      "Epoch [08/15] Step [005/146]: acc=0.5000 g_loss=0.6490 d_loss=0.7066 kd_loss=0.0730\n",
      "Epoch [08/15] Step [010/146]: acc=0.5000 g_loss=0.6560 d_loss=0.7003 kd_loss=0.0397\n",
      "Epoch [08/15] Step [015/146]: acc=0.5000 g_loss=0.6649 d_loss=0.7022 kd_loss=0.0253\n",
      "Epoch [08/15] Step [020/146]: acc=0.5000 g_loss=0.6807 d_loss=0.6936 kd_loss=0.0289\n",
      "Epoch [08/15] Step [025/146]: acc=0.5000 g_loss=0.6814 d_loss=0.6943 kd_loss=0.0136\n",
      "Epoch [08/15] Step [030/146]: acc=0.5000 g_loss=0.6837 d_loss=0.6932 kd_loss=0.0225\n",
      "Epoch [08/15] Step [035/146]: acc=0.5000 g_loss=0.6856 d_loss=0.6947 kd_loss=0.0428\n",
      "Epoch [08/15] Step [040/146]: acc=0.5000 g_loss=0.6926 d_loss=0.6884 kd_loss=0.1240\n",
      "Epoch [08/15] Step [045/146]: acc=0.5000 g_loss=0.6762 d_loss=0.6961 kd_loss=0.0507\n",
      "Epoch [08/15] Step [050/146]: acc=0.5000 g_loss=0.6672 d_loss=0.6980 kd_loss=0.0339\n",
      "Epoch [08/15] Step [055/146]: acc=0.5000 g_loss=0.6793 d_loss=0.6921 kd_loss=0.0697\n",
      "Epoch [08/15] Step [060/146]: acc=0.5000 g_loss=0.6822 d_loss=0.6902 kd_loss=0.0817\n",
      "Epoch [08/15] Step [065/146]: acc=0.5000 g_loss=0.6728 d_loss=0.6986 kd_loss=0.0504\n",
      "Epoch [08/15] Step [070/146]: acc=0.5000 g_loss=0.6767 d_loss=0.6960 kd_loss=0.0466\n",
      "Epoch [08/15] Step [075/146]: acc=0.5000 g_loss=0.6705 d_loss=0.6924 kd_loss=0.0857\n",
      "Epoch [08/15] Step [080/146]: acc=0.5000 g_loss=0.6756 d_loss=0.6950 kd_loss=0.1005\n",
      "Epoch [08/15] Step [085/146]: acc=0.5000 g_loss=0.6798 d_loss=0.6925 kd_loss=0.0776\n",
      "Epoch [08/15] Step [090/146]: acc=0.5000 g_loss=0.6753 d_loss=0.6961 kd_loss=0.0599\n",
      "Epoch [08/15] Step [095/146]: acc=0.5000 g_loss=0.6755 d_loss=0.6917 kd_loss=0.0288\n",
      "Epoch [08/15] Step [100/146]: acc=0.5000 g_loss=0.6697 d_loss=0.6919 kd_loss=0.1084\n",
      "Epoch [08/15] Step [105/146]: acc=0.5000 g_loss=0.6774 d_loss=0.6918 kd_loss=0.0610\n",
      "Epoch [08/15] Step [110/146]: acc=0.5000 g_loss=0.6839 d_loss=0.6912 kd_loss=0.0661\n",
      "Epoch [08/15] Step [115/146]: acc=0.5000 g_loss=0.6736 d_loss=0.6929 kd_loss=0.0201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [08/15] Step [120/146]: acc=0.5000 g_loss=0.6714 d_loss=0.6923 kd_loss=0.0713\n",
      "Epoch [08/15] Step [125/146]: acc=0.5000 g_loss=0.6723 d_loss=0.6935 kd_loss=0.0977\n",
      "Epoch [08/15] Step [130/146]: acc=0.5000 g_loss=0.6821 d_loss=0.6920 kd_loss=0.0696\n",
      "Epoch [08/15] Step [135/146]: acc=0.5000 g_loss=0.6815 d_loss=0.6937 kd_loss=0.1262\n",
      "Epoch [08/15] Step [140/146]: acc=0.5000 g_loss=0.6811 d_loss=0.6895 kd_loss=0.0697\n",
      "Epoch [08/15] Step [145/146]: acc=0.2727 g_loss=0.6647 d_loss=0.6893 kd_loss=0.0424\n",
      "German Test:\n",
      "Validation loss:  2.0403969287872314\n",
      "Accuracy: 0.6881067961165048\n",
      "F1 score (Macro): 0.5413818818267494\n",
      "F1 score (Per class): [0.37614679 0.4469697  0.80102916]\n",
      "Precision score (Per class): [0.37272727 0.62105263 0.75444265]\n",
      "Recall score (Per class): [0.37962963 0.34911243 0.85374771]\n",
      "Epoch: 9/15\n",
      "Epoch [09/15] Step [000/146]: acc=0.5000 g_loss=0.6525 d_loss=0.7073 kd_loss=0.0415\n",
      "Epoch [09/15] Step [005/146]: acc=0.5000 g_loss=0.6566 d_loss=0.7038 kd_loss=0.0736\n",
      "Epoch [09/15] Step [010/146]: acc=0.5000 g_loss=0.6636 d_loss=0.6988 kd_loss=0.0187\n",
      "Epoch [09/15] Step [015/146]: acc=0.5000 g_loss=0.6718 d_loss=0.7001 kd_loss=0.0736\n",
      "Epoch [09/15] Step [020/146]: acc=0.5000 g_loss=0.6832 d_loss=0.6940 kd_loss=0.0329\n",
      "Epoch [09/15] Step [025/146]: acc=0.5000 g_loss=0.6838 d_loss=0.6956 kd_loss=0.0118\n",
      "Epoch [09/15] Step [030/146]: acc=0.5000 g_loss=0.6857 d_loss=0.6945 kd_loss=0.0328\n",
      "Epoch [09/15] Step [035/146]: acc=0.5000 g_loss=0.6858 d_loss=0.6961 kd_loss=0.0297\n",
      "Epoch [09/15] Step [040/146]: acc=0.5000 g_loss=0.6943 d_loss=0.6894 kd_loss=0.0447\n",
      "Epoch [09/15] Step [045/146]: acc=0.5000 g_loss=0.6803 d_loss=0.6956 kd_loss=0.0327\n",
      "Epoch [09/15] Step [050/146]: acc=0.5000 g_loss=0.6726 d_loss=0.6976 kd_loss=0.0369\n",
      "Epoch [09/15] Step [055/146]: acc=0.5000 g_loss=0.6825 d_loss=0.6926 kd_loss=0.0431\n",
      "Epoch [09/15] Step [060/146]: acc=0.5000 g_loss=0.6847 d_loss=0.6906 kd_loss=0.1117\n",
      "Epoch [09/15] Step [065/146]: acc=0.5000 g_loss=0.6777 d_loss=0.6974 kd_loss=0.0930\n",
      "Epoch [09/15] Step [070/146]: acc=0.5000 g_loss=0.6816 d_loss=0.6946 kd_loss=0.0568\n",
      "Epoch [09/15] Step [075/146]: acc=0.5000 g_loss=0.6764 d_loss=0.6927 kd_loss=0.0264\n",
      "Epoch [09/15] Step [080/146]: acc=0.5000 g_loss=0.6791 d_loss=0.6945 kd_loss=0.0760\n",
      "Epoch [09/15] Step [085/146]: acc=0.5000 g_loss=0.6842 d_loss=0.6916 kd_loss=0.0453\n",
      "Epoch [09/15] Step [090/146]: acc=0.5000 g_loss=0.6813 d_loss=0.6952 kd_loss=0.1902\n",
      "Epoch [09/15] Step [095/146]: acc=0.5000 g_loss=0.6799 d_loss=0.6922 kd_loss=0.0346\n",
      "Epoch [09/15] Step [100/146]: acc=0.5000 g_loss=0.6751 d_loss=0.6914 kd_loss=0.0785\n",
      "Epoch [09/15] Step [105/146]: acc=0.5000 g_loss=0.6774 d_loss=0.6939 kd_loss=0.0721\n",
      "Epoch [09/15] Step [110/146]: acc=0.5000 g_loss=0.6875 d_loss=0.6912 kd_loss=0.0429\n",
      "Epoch [09/15] Step [115/146]: acc=0.5000 g_loss=0.6789 d_loss=0.6922 kd_loss=0.0478\n",
      "Epoch [09/15] Step [120/146]: acc=0.5000 g_loss=0.6772 d_loss=0.6928 kd_loss=0.1215\n",
      "Epoch [09/15] Step [125/146]: acc=0.5000 g_loss=0.6759 d_loss=0.6939 kd_loss=0.0468\n",
      "Epoch [09/15] Step [130/146]: acc=0.5000 g_loss=0.6838 d_loss=0.6933 kd_loss=0.1111\n",
      "Epoch [09/15] Step [135/146]: acc=0.5000 g_loss=0.6827 d_loss=0.6939 kd_loss=0.0670\n",
      "Epoch [09/15] Step [140/146]: acc=0.5000 g_loss=0.6842 d_loss=0.6892 kd_loss=0.0314\n",
      "Epoch [09/15] Step [145/146]: acc=0.2727 g_loss=0.6709 d_loss=0.6918 kd_loss=0.0696\n",
      "German Test:\n",
      "Validation loss:  2.034214735031128\n",
      "Accuracy: 0.662621359223301\n",
      "F1 score (Macro): 0.534022911471571\n",
      "F1 score (Per class): [0.35245902 0.46621622 0.7833935 ]\n",
      "Precision score (Per class): [0.31617647 0.54330709 0.77361854]\n",
      "Recall score (Per class): [0.39814815 0.40828402 0.79341865]\n",
      "Epoch: 10/15\n",
      "Epoch [10/15] Step [000/146]: acc=0.5000 g_loss=0.6583 d_loss=0.7064 kd_loss=0.0631\n",
      "Epoch [10/15] Step [005/146]: acc=0.5000 g_loss=0.6625 d_loss=0.7031 kd_loss=0.0402\n",
      "Epoch [10/15] Step [010/146]: acc=0.5000 g_loss=0.6689 d_loss=0.6979 kd_loss=0.0345\n",
      "Epoch [10/15] Step [015/146]: acc=0.5000 g_loss=0.6757 d_loss=0.6992 kd_loss=0.0366\n",
      "Epoch [10/15] Step [020/146]: acc=0.5000 g_loss=0.6852 d_loss=0.6937 kd_loss=0.0220\n",
      "Epoch [10/15] Step [025/146]: acc=0.5000 g_loss=0.6860 d_loss=0.6952 kd_loss=0.0264\n",
      "Epoch [10/15] Step [030/146]: acc=0.5000 g_loss=0.6874 d_loss=0.6951 kd_loss=0.0268\n",
      "Epoch [10/15] Step [035/146]: acc=0.5000 g_loss=0.6878 d_loss=0.6962 kd_loss=0.0657\n",
      "Epoch [10/15] Step [040/146]: acc=0.5000 g_loss=0.6945 d_loss=0.6908 kd_loss=0.0423\n",
      "Epoch [10/15] Step [045/146]: acc=0.5000 g_loss=0.6838 d_loss=0.6956 kd_loss=0.0415\n",
      "Epoch [10/15] Step [050/146]: acc=0.5000 g_loss=0.6777 d_loss=0.6967 kd_loss=0.0597\n",
      "Epoch [10/15] Step [055/146]: acc=0.5000 g_loss=0.6857 d_loss=0.6923 kd_loss=0.0677\n",
      "Epoch [10/15] Step [060/146]: acc=0.5000 g_loss=0.6862 d_loss=0.6913 kd_loss=0.0998\n",
      "Epoch [10/15] Step [065/146]: acc=0.5000 g_loss=0.6805 d_loss=0.6970 kd_loss=0.0471\n",
      "Epoch [10/15] Step [070/146]: acc=0.5000 g_loss=0.6849 d_loss=0.6943 kd_loss=0.0386\n",
      "Epoch [10/15] Step [075/146]: acc=0.5000 g_loss=0.6812 d_loss=0.6923 kd_loss=0.0681\n",
      "Epoch [10/15] Step [080/146]: acc=0.5000 g_loss=0.6820 d_loss=0.6949 kd_loss=0.1429\n",
      "Epoch [10/15] Step [085/146]: acc=0.5000 g_loss=0.6863 d_loss=0.6918 kd_loss=0.0479\n",
      "Epoch [10/15] Step [090/146]: acc=0.5000 g_loss=0.6833 d_loss=0.6949 kd_loss=0.0709\n",
      "Epoch [10/15] Step [095/146]: acc=0.5000 g_loss=0.6833 d_loss=0.6922 kd_loss=0.0505\n",
      "Epoch [10/15] Step [100/146]: acc=0.5000 g_loss=0.6807 d_loss=0.6923 kd_loss=0.0775\n",
      "Epoch [10/15] Step [105/146]: acc=0.5000 g_loss=0.6813 d_loss=0.6935 kd_loss=0.1298\n",
      "Epoch [10/15] Step [110/146]: acc=0.5000 g_loss=0.6901 d_loss=0.6913 kd_loss=0.0542\n",
      "Epoch [10/15] Step [115/146]: acc=0.5000 g_loss=0.6815 d_loss=0.6929 kd_loss=0.0173\n",
      "Epoch [10/15] Step [120/146]: acc=0.5000 g_loss=0.6812 d_loss=0.6927 kd_loss=0.0589\n",
      "Epoch [10/15] Step [125/146]: acc=0.5000 g_loss=0.6819 d_loss=0.6932 kd_loss=0.0960\n",
      "Epoch [10/15] Step [130/146]: acc=0.5000 g_loss=0.6870 d_loss=0.6935 kd_loss=0.0516\n",
      "Epoch [10/15] Step [135/146]: acc=0.5000 g_loss=0.6860 d_loss=0.6944 kd_loss=0.0482\n",
      "Epoch [10/15] Step [140/146]: acc=0.5000 g_loss=0.6873 d_loss=0.6900 kd_loss=0.0173\n",
      "Epoch [10/15] Step [145/146]: acc=0.2727 g_loss=0.6771 d_loss=0.6915 kd_loss=0.0429\n",
      "German Test:\n",
      "Validation loss:  1.9449933767318726\n",
      "Accuracy: 0.6711165048543689\n",
      "F1 score (Macro): 0.5432939005565248\n",
      "F1 score (Per class): [0.36752137 0.47491639 0.78744395]\n",
      "Precision score (Per class): [0.34126984 0.54615385 0.77288732]\n",
      "Recall score (Per class): [0.39814815 0.42011834 0.80255941]\n",
      "Epoch: 11/15\n",
      "Epoch [11/15] Step [000/146]: acc=0.5000 g_loss=0.6686 d_loss=0.7026 kd_loss=0.0442\n",
      "Epoch [11/15] Step [005/146]: acc=0.5000 g_loss=0.6723 d_loss=0.6999 kd_loss=0.0432\n",
      "Epoch [11/15] Step [010/146]: acc=0.5000 g_loss=0.6775 d_loss=0.6966 kd_loss=0.0671\n",
      "Epoch [11/15] Step [015/146]: acc=0.5000 g_loss=0.6839 d_loss=0.6965 kd_loss=0.0481\n",
      "Epoch [11/15] Step [020/146]: acc=0.5000 g_loss=0.6915 d_loss=0.6924 kd_loss=0.0499\n",
      "Epoch [11/15] Step [025/146]: acc=0.5000 g_loss=0.6871 d_loss=0.6956 kd_loss=0.0293\n",
      "Epoch [11/15] Step [030/146]: acc=0.5000 g_loss=0.6883 d_loss=0.6945 kd_loss=0.0590\n",
      "Epoch [11/15] Step [035/146]: acc=0.5000 g_loss=0.6893 d_loss=0.6954 kd_loss=0.0456\n",
      "Epoch [11/15] Step [040/146]: acc=0.5000 g_loss=0.6954 d_loss=0.6909 kd_loss=0.0822\n",
      "Epoch [11/15] Step [045/146]: acc=0.5000 g_loss=0.6840 d_loss=0.6960 kd_loss=0.0763\n",
      "Epoch [11/15] Step [050/146]: acc=0.5000 g_loss=0.6786 d_loss=0.6970 kd_loss=0.0558\n",
      "Epoch [11/15] Step [055/146]: acc=0.5000 g_loss=0.6875 d_loss=0.6922 kd_loss=0.0679\n",
      "Epoch [11/15] Step [060/146]: acc=0.5000 g_loss=0.6872 d_loss=0.6922 kd_loss=0.0384\n",
      "Epoch [11/15] Step [065/146]: acc=0.5000 g_loss=0.6821 d_loss=0.6967 kd_loss=0.0825\n",
      "Epoch [11/15] Step [070/146]: acc=0.5000 g_loss=0.6849 d_loss=0.6948 kd_loss=0.0667\n",
      "Epoch [11/15] Step [075/146]: acc=0.5000 g_loss=0.6829 d_loss=0.6926 kd_loss=0.0544\n",
      "Epoch [11/15] Step [080/146]: acc=0.5000 g_loss=0.6865 d_loss=0.6941 kd_loss=0.0638\n",
      "Epoch [11/15] Step [085/146]: acc=0.5000 g_loss=0.6883 d_loss=0.6920 kd_loss=0.0394\n",
      "Epoch [11/15] Step [090/146]: acc=0.5000 g_loss=0.6843 d_loss=0.6947 kd_loss=0.0889\n",
      "Epoch [11/15] Step [095/146]: acc=0.5000 g_loss=0.6848 d_loss=0.6928 kd_loss=0.0516\n",
      "Epoch [11/15] Step [100/146]: acc=0.5000 g_loss=0.6840 d_loss=0.6927 kd_loss=0.0844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15] Step [105/146]: acc=0.5000 g_loss=0.6858 d_loss=0.6933 kd_loss=0.0766\n",
      "Epoch [11/15] Step [110/146]: acc=0.5000 g_loss=0.6911 d_loss=0.6915 kd_loss=0.0172\n",
      "Epoch [11/15] Step [115/146]: acc=0.5000 g_loss=0.6838 d_loss=0.6933 kd_loss=0.0368\n",
      "Epoch [11/15] Step [120/146]: acc=0.5000 g_loss=0.6823 d_loss=0.6933 kd_loss=0.0604\n",
      "Epoch [11/15] Step [125/146]: acc=0.5000 g_loss=0.6852 d_loss=0.6928 kd_loss=0.0232\n",
      "Epoch [11/15] Step [130/146]: acc=0.5000 g_loss=0.6896 d_loss=0.6935 kd_loss=0.1214\n",
      "Epoch [11/15] Step [135/146]: acc=0.5000 g_loss=0.6879 d_loss=0.6942 kd_loss=0.0302\n",
      "Epoch [11/15] Step [140/146]: acc=0.5000 g_loss=0.6891 d_loss=0.6910 kd_loss=0.0436\n",
      "Epoch [11/15] Step [145/146]: acc=0.2727 g_loss=0.6805 d_loss=0.6924 kd_loss=0.0425\n",
      "German Test:\n",
      "Validation loss:  2.0727107524871826\n",
      "Accuracy: 0.6856796116504854\n",
      "F1 score (Macro): 0.5282706646750389\n",
      "F1 score (Per class): [0.34123223 0.44357977 0.8       ]\n",
      "Precision score (Per class): [0.34951456 0.64772727 0.74565561]\n",
      "Recall score (Per class): [0.33333333 0.33727811 0.86288848]\n",
      "Epoch: 12/15\n",
      "Epoch [12/15] Step [000/146]: acc=0.5000 g_loss=0.6743 d_loss=0.7005 kd_loss=0.0526\n",
      "Epoch [12/15] Step [005/146]: acc=0.5000 g_loss=0.6779 d_loss=0.6982 kd_loss=0.0194\n",
      "Epoch [12/15] Step [010/146]: acc=0.5000 g_loss=0.6827 d_loss=0.6956 kd_loss=0.0806\n",
      "Epoch [12/15] Step [015/146]: acc=0.5000 g_loss=0.6890 d_loss=0.6948 kd_loss=0.0365\n",
      "Epoch [12/15] Step [020/146]: acc=0.5000 g_loss=0.6961 d_loss=0.6917 kd_loss=0.0421\n",
      "Epoch [12/15] Step [025/146]: acc=0.5000 g_loss=0.6901 d_loss=0.6965 kd_loss=0.0317\n",
      "Epoch [12/15] Step [030/146]: acc=0.5000 g_loss=0.6909 d_loss=0.6950 kd_loss=0.0630\n",
      "Epoch [12/15] Step [035/146]: acc=0.5000 g_loss=0.6891 d_loss=0.6974 kd_loss=0.0497\n",
      "Epoch [12/15] Step [040/146]: acc=0.5000 g_loss=0.6954 d_loss=0.6924 kd_loss=0.0502\n",
      "Epoch [12/15] Step [045/146]: acc=0.5000 g_loss=0.6860 d_loss=0.6963 kd_loss=0.0534\n",
      "Epoch [12/15] Step [050/146]: acc=0.5000 g_loss=0.6810 d_loss=0.6966 kd_loss=0.0991\n",
      "Epoch [12/15] Step [055/146]: acc=0.5000 g_loss=0.6893 d_loss=0.6923 kd_loss=0.0474\n",
      "Epoch [12/15] Step [060/146]: acc=0.5000 g_loss=0.6890 d_loss=0.6927 kd_loss=0.1107\n",
      "Epoch [12/15] Step [065/146]: acc=0.5000 g_loss=0.6842 d_loss=0.6969 kd_loss=0.0646\n",
      "Epoch [12/15] Step [070/146]: acc=0.5000 g_loss=0.6867 d_loss=0.6948 kd_loss=0.0727\n",
      "Epoch [12/15] Step [075/146]: acc=0.5000 g_loss=0.6847 d_loss=0.6930 kd_loss=0.0767\n",
      "Epoch [12/15] Step [080/146]: acc=0.5000 g_loss=0.6871 d_loss=0.6944 kd_loss=0.0808\n",
      "Epoch [12/15] Step [085/146]: acc=0.5000 g_loss=0.6901 d_loss=0.6926 kd_loss=0.0297\n",
      "Epoch [12/15] Step [090/146]: acc=0.5000 g_loss=0.6871 d_loss=0.6952 kd_loss=0.0911\n",
      "Epoch [12/15] Step [095/146]: acc=0.5000 g_loss=0.6872 d_loss=0.6932 kd_loss=0.0856\n",
      "Epoch [12/15] Step [100/146]: acc=0.5000 g_loss=0.6868 d_loss=0.6925 kd_loss=0.0442\n",
      "Epoch [12/15] Step [105/146]: acc=0.5000 g_loss=0.6900 d_loss=0.6925 kd_loss=0.0929\n",
      "Epoch [12/15] Step [110/146]: acc=0.5000 g_loss=0.6929 d_loss=0.6919 kd_loss=0.0798\n",
      "Epoch [12/15] Step [115/146]: acc=0.5000 g_loss=0.6875 d_loss=0.6932 kd_loss=0.0423\n",
      "Epoch [12/15] Step [120/146]: acc=0.5000 g_loss=0.6860 d_loss=0.6933 kd_loss=0.0402\n",
      "Epoch [12/15] Step [125/146]: acc=0.5000 g_loss=0.6874 d_loss=0.6931 kd_loss=0.0817\n",
      "Epoch [12/15] Step [130/146]: acc=0.5000 g_loss=0.6918 d_loss=0.6929 kd_loss=0.0601\n",
      "Epoch [12/15] Step [135/146]: acc=0.5000 g_loss=0.6907 d_loss=0.6939 kd_loss=0.0422\n",
      "Epoch [12/15] Step [140/146]: acc=0.5000 g_loss=0.6914 d_loss=0.6911 kd_loss=0.0525\n",
      "Epoch [12/15] Step [145/146]: acc=0.2727 g_loss=0.6840 d_loss=0.6935 kd_loss=0.0496\n",
      "German Test:\n",
      "Validation loss:  2.033942937850952\n",
      "Accuracy: 0.691747572815534\n",
      "F1 score (Macro): 0.5389589176382965\n",
      "F1 score (Per class): [0.34517766 0.46931408 0.80238501]\n",
      "Precision score (Per class): [0.38202247 0.60185185 0.75119617]\n",
      "Recall score (Per class): [0.31481481 0.38461538 0.86106033]\n",
      "Epoch: 13/15\n",
      "Epoch [13/15] Step [000/146]: acc=0.5000 g_loss=0.6784 d_loss=0.6992 kd_loss=0.0450\n",
      "Epoch [13/15] Step [005/146]: acc=0.5000 g_loss=0.6810 d_loss=0.6978 kd_loss=0.0770\n",
      "Epoch [13/15] Step [010/146]: acc=0.5000 g_loss=0.6849 d_loss=0.6951 kd_loss=0.0245\n",
      "Epoch [13/15] Step [015/146]: acc=0.5000 g_loss=0.6905 d_loss=0.6943 kd_loss=0.0468\n",
      "Epoch [13/15] Step [020/146]: acc=0.5000 g_loss=0.6949 d_loss=0.6924 kd_loss=0.0217\n",
      "Epoch [13/15] Step [025/146]: acc=0.5000 g_loss=0.6908 d_loss=0.6959 kd_loss=0.0206\n",
      "Epoch [13/15] Step [030/146]: acc=0.5000 g_loss=0.6916 d_loss=0.6947 kd_loss=0.0298\n",
      "Epoch [13/15] Step [035/146]: acc=0.5000 g_loss=0.6906 d_loss=0.6963 kd_loss=0.0386\n",
      "Epoch [13/15] Step [040/146]: acc=0.5000 g_loss=0.6959 d_loss=0.6926 kd_loss=0.0501\n",
      "Epoch [13/15] Step [045/146]: acc=0.5000 g_loss=0.6871 d_loss=0.6964 kd_loss=0.0244\n",
      "Epoch [13/15] Step [050/146]: acc=0.5000 g_loss=0.6833 d_loss=0.6961 kd_loss=0.0377\n",
      "Epoch [13/15] Step [055/146]: acc=0.5000 g_loss=0.6905 d_loss=0.6924 kd_loss=0.0490\n",
      "Epoch [13/15] Step [060/146]: acc=0.5000 g_loss=0.6911 d_loss=0.6920 kd_loss=0.0642\n",
      "Epoch [13/15] Step [065/146]: acc=0.5000 g_loss=0.6865 d_loss=0.6963 kd_loss=0.0315\n",
      "Epoch [13/15] Step [070/146]: acc=0.5000 g_loss=0.6882 d_loss=0.6948 kd_loss=0.0488\n",
      "Epoch [13/15] Step [075/146]: acc=0.5000 g_loss=0.6863 d_loss=0.6933 kd_loss=0.0223\n",
      "Epoch [13/15] Step [080/146]: acc=0.5000 g_loss=0.6880 d_loss=0.6947 kd_loss=0.0570\n",
      "Epoch [13/15] Step [085/146]: acc=0.5000 g_loss=0.6915 d_loss=0.6926 kd_loss=0.0407\n",
      "Epoch [13/15] Step [090/146]: acc=0.5000 g_loss=0.6886 d_loss=0.6947 kd_loss=0.0629\n",
      "Epoch [13/15] Step [095/146]: acc=0.5000 g_loss=0.6893 d_loss=0.6930 kd_loss=0.0190\n",
      "Epoch [13/15] Step [100/146]: acc=0.5000 g_loss=0.6881 d_loss=0.6927 kd_loss=0.1075\n",
      "Epoch [13/15] Step [105/146]: acc=0.5000 g_loss=0.6890 d_loss=0.6934 kd_loss=0.0882\n",
      "Epoch [13/15] Step [110/146]: acc=0.5000 g_loss=0.6940 d_loss=0.6919 kd_loss=0.0511\n",
      "Epoch [13/15] Step [115/146]: acc=0.5000 g_loss=0.6893 d_loss=0.6930 kd_loss=0.0284\n",
      "Epoch [13/15] Step [120/146]: acc=0.5000 g_loss=0.6880 d_loss=0.6939 kd_loss=0.0797\n",
      "Epoch [13/15] Step [125/146]: acc=0.5000 g_loss=0.6886 d_loss=0.6932 kd_loss=0.0620\n",
      "Epoch [13/15] Step [130/146]: acc=0.5000 g_loss=0.6922 d_loss=0.6936 kd_loss=0.0310\n",
      "Epoch [13/15] Step [135/146]: acc=0.5000 g_loss=0.6915 d_loss=0.6936 kd_loss=0.0483\n",
      "Epoch [13/15] Step [140/146]: acc=0.5000 g_loss=0.6922 d_loss=0.6913 kd_loss=0.0203\n",
      "Epoch [13/15] Step [145/146]: acc=0.2727 g_loss=0.6858 d_loss=0.6935 kd_loss=0.0194\n",
      "German Test:\n",
      "Validation loss:  2.080662250518799\n",
      "Accuracy: 0.6723300970873787\n",
      "F1 score (Macro): 0.5384568978054036\n",
      "F1 score (Per class): [0.37446809 0.45323741 0.7876652 ]\n",
      "Precision score (Per class): [0.34645669 0.57798165 0.76020408]\n",
      "Recall score (Per class): [0.40740741 0.37278107 0.81718464]\n",
      "Epoch: 14/15\n",
      "Epoch [14/15] Step [000/146]: acc=0.5000 g_loss=0.6805 d_loss=0.6987 kd_loss=0.0130\n",
      "Epoch [14/15] Step [005/146]: acc=0.5000 g_loss=0.6832 d_loss=0.6973 kd_loss=0.0292\n",
      "Epoch [14/15] Step [010/146]: acc=0.5000 g_loss=0.6870 d_loss=0.6950 kd_loss=0.0539\n",
      "Epoch [14/15] Step [015/146]: acc=0.5000 g_loss=0.6917 d_loss=0.6944 kd_loss=0.0254\n",
      "Epoch [14/15] Step [020/146]: acc=0.5000 g_loss=0.6957 d_loss=0.6922 kd_loss=0.0229\n",
      "Epoch [14/15] Step [025/146]: acc=0.5000 g_loss=0.6909 d_loss=0.6961 kd_loss=0.0109\n",
      "Epoch [14/15] Step [030/146]: acc=0.5000 g_loss=0.6918 d_loss=0.6952 kd_loss=0.0278\n",
      "Epoch [14/15] Step [035/146]: acc=0.5000 g_loss=0.6916 d_loss=0.6961 kd_loss=0.0450\n",
      "Epoch [14/15] Step [040/146]: acc=0.5000 g_loss=0.6964 d_loss=0.6931 kd_loss=0.0361\n",
      "Epoch [14/15] Step [045/146]: acc=0.5000 g_loss=0.6898 d_loss=0.6960 kd_loss=0.0627\n",
      "Epoch [14/15] Step [050/146]: acc=0.5000 g_loss=0.6862 d_loss=0.6959 kd_loss=0.0142\n",
      "Epoch [14/15] Step [055/146]: acc=0.5000 g_loss=0.6921 d_loss=0.6927 kd_loss=0.0279\n",
      "Epoch [14/15] Step [060/146]: acc=0.5000 g_loss=0.6920 d_loss=0.6926 kd_loss=0.0546\n",
      "Epoch [14/15] Step [065/146]: acc=0.5000 g_loss=0.6890 d_loss=0.6960 kd_loss=0.1002\n",
      "Epoch [14/15] Step [070/146]: acc=0.5000 g_loss=0.6904 d_loss=0.6947 kd_loss=0.0273\n",
      "Epoch [14/15] Step [075/146]: acc=0.5000 g_loss=0.6886 d_loss=0.6937 kd_loss=0.0620\n",
      "Epoch [14/15] Step [080/146]: acc=0.5000 g_loss=0.6901 d_loss=0.6947 kd_loss=0.1395\n",
      "Epoch [14/15] Step [085/146]: acc=0.5000 g_loss=0.6929 d_loss=0.6930 kd_loss=0.0267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15] Step [090/146]: acc=0.5000 g_loss=0.6908 d_loss=0.6944 kd_loss=0.1178\n",
      "Epoch [14/15] Step [095/146]: acc=0.5000 g_loss=0.6914 d_loss=0.6931 kd_loss=0.0371\n",
      "Epoch [14/15] Step [100/146]: acc=0.5000 g_loss=0.6903 d_loss=0.6928 kd_loss=0.0322\n",
      "Epoch [14/15] Step [105/146]: acc=0.5000 g_loss=0.6915 d_loss=0.6932 kd_loss=0.0686\n",
      "Epoch [14/15] Step [110/146]: acc=0.5000 g_loss=0.6951 d_loss=0.6922 kd_loss=0.0094\n",
      "Epoch [14/15] Step [115/146]: acc=0.5000 g_loss=0.6913 d_loss=0.6931 kd_loss=0.0103\n",
      "Epoch [14/15] Step [120/146]: acc=0.5000 g_loss=0.6902 d_loss=0.6932 kd_loss=0.0299\n",
      "Epoch [14/15] Step [125/146]: acc=0.5000 g_loss=0.6911 d_loss=0.6933 kd_loss=0.0925\n",
      "Epoch [14/15] Step [130/146]: acc=0.5000 g_loss=0.6937 d_loss=0.6935 kd_loss=0.0575\n",
      "Epoch [14/15] Step [135/146]: acc=0.5000 g_loss=0.6932 d_loss=0.6937 kd_loss=0.0445\n",
      "Epoch [14/15] Step [140/146]: acc=0.5000 g_loss=0.6935 d_loss=0.6918 kd_loss=0.0240\n",
      "Epoch [14/15] Step [145/146]: acc=0.2727 g_loss=0.6889 d_loss=0.6943 kd_loss=0.0212\n",
      "German Test:\n",
      "Validation loss:  2.060258388519287\n",
      "Accuracy: 0.6783980582524272\n",
      "F1 score (Macro): 0.5477580339309542\n",
      "F1 score (Per class): [0.38626609 0.46478873 0.79221927]\n",
      "Precision score (Per class): [0.36       0.57391304 0.76712329]\n",
      "Recall score (Per class): [0.41666667 0.39053254 0.8190128 ]\n"
     ]
    }
   ],
   "source": [
    "tgt_encoder = adapt(src_encoder, discriminator,\n",
    "                    bi_lstm_classifier, train_dataloader, train_translated_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c28277fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tgt_encoder.state_dict(), 'model/french_adapted_encoder_12_12_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea96ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Test: \n",
      "\n",
      "Validation loss:  1.8512077331542969\n",
      "Accuracy: 0.5330132052821128\n",
      "F1 score (Macro): 0.4866059870873501\n",
      "F1 score (Per class): [0.33918129 0.46613546 0.65450122]\n",
      "Precision score (Per class): [0.28019324 0.52466368 0.6674938 ]\n",
      "Recall score (Per class): [0.42962963 0.41935484 0.64200477]\n",
      "\n",
      "\n",
      "\n",
      "German Test:\n",
      "Validation loss:  1.3572468757629395\n",
      "Accuracy: 0.6298543689320388\n",
      "F1 score (Macro): 0.5376748284908066\n",
      "F1 score (Per class): [0.36781609 0.49867374 0.74653465]\n",
      "Precision score (Per class): [0.31372549 0.45192308 0.81425486]\n",
      "Recall score (Per class): [0.44444444 0.55621302 0.68921389]\n",
      "\n",
      "\n",
      "\n",
      "Italian Test: \n",
      "\n",
      "Validation loss:  2.2205429077148438\n",
      "Accuracy: 0.4626218851570964\n",
      "F1 score (Macro): 0.44481746429594815\n",
      "F1 score (Per class): [0.37220844 0.55120101 0.41104294]\n",
      "Precision score (Per class): [0.29761905 0.65465465 0.3964497 ]\n",
      "Recall score (Per class): [0.49668874 0.47598253 0.42675159]\n"
     ]
    }
   ],
   "source": [
    "print(\"French Test: \\n\")\n",
    "evaluate_validation(tgt_encoder, bi_lstm_classifier, french_dataloader)\n",
    "print(\"\\n\\n\")\n",
    "print(\"German Test:\")\n",
    "evaluate_validation(tgt_encoder, bi_lstm_classifier, german_dataloader)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Italian Test: \\n\")\n",
    "evaluate_validation(tgt_encoder, bi_lstm_classifier, italian_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a49d3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Test:\n",
      "Validation loss:  2.7417564392089844\n",
      "Accuracy: 0.36771844660194175\n",
      "F1 score (Macro): 0.37011527679462075\n",
      "F1 score (Per class): [0.27287854 0.41025641 0.42721088]\n",
      "Precision score (Per class): [0.1663286  0.44755245 0.83510638]\n",
      "Recall score (Per class): [0.75925926 0.37869822 0.28702011]\n"
     ]
    }
   ],
   "source": [
    "print(\"German Test:\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, german_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7927741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian Test: \n",
      "\n",
      "Validation loss:  3.206312417984009\n",
      "Accuracy: 0.3488624052004334\n",
      "F1 score (Macro): 0.34640687429795575\n",
      "F1 score (Per class): [0.37837838 0.35233161 0.30851064]\n",
      "Precision score (Per class): [0.24094203 0.84297521 0.348     ]\n",
      "Recall score (Per class): [0.8807947  0.22270742 0.27707006]\n"
     ]
    }
   ],
   "source": [
    "print(\"Italian Test: \\n\")\n",
    "evaluate_validation(tgt_encoder, src_classifier, italian_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a132641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
